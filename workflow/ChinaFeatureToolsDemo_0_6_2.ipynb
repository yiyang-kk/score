{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple FeatureTools demo\n",
    "\n",
    "**Copyright:**\n",
    "```\n",
    "© 2019, Jan Hynek, Vaclav Svoboda, Martin Kotek and HomeCredit International a.s.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License \n",
    "- http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "```\n",
    "\n",
    "**Disclaimer #1:**\n",
    "\n",
    "**This notebook serves more as the simplest demonstration of FeatureTools use for HomeCredit scoring purposes.**\n",
    "\n",
    "For advanced feature tools use, I strongly recommend official [FeatureTools notebook done on HomeCredit Kaggle competition data](https://github.com/Featuretools/predict-loan-repayment/blob/master/Automated%20Loan%20Repayment.ipynb)\n",
    "\n",
    "This notebook is not intended to be run once, from beginning to end, but rather à la carte - when you need something, just pick that up (i.e. creation of custom primitives).\n",
    "\n",
    "Other than that, there are also links to other interesting resources in the docs, etc.\n",
    "\n",
    "**Disclaimer #2**:\n",
    "\n",
    "This is jupyter notebook, and markdown (and especially links:/) does not work correctly on gitlab.\n",
    "You are advised to download this notebook locally.\n",
    "\n",
    "**Disclaimer #3**:\n",
    "\n",
    "Do not hesitate to contact me at jan.hynek@homecredit.eu in case you would have any questions.\n",
    "\n",
    "\n",
    "# Setup\n",
    "In this part, we create basic setup.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import featuretools as ft\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "\n",
    "import xgboost\n",
    "\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versions used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "f\"\"\"\n",
    "featuretools: {ft.__version__}\n",
    "numpy: {np.__version__}\n",
    "pandas: {pd.__version__}\n",
    "shap: {shap.__version__}\n",
    "xgboost: {xgboost.__version__}\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "featuretools: 0.8.0\n",
    "numpy: 1.16.3\n",
    "pandas: 0.24.2\n",
    "shap: 0.29.1\n",
    "xgboost: 0.82\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We create following variables for several reasons:**\n",
    "- if we would like to rename the columns afterwards, it will be very easy - we can change that on one place only (good coding practice)\n",
    "- Jupyter Notebook now gives us variable suggestions! We do not have to write the name over and over and over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_APPLICATION_COLUMN = \"ID_APPLICATION\"\n",
    "ID_TRANSACTIONS_COLUMN = \"ID_TRANSACTION\"\n",
    "TIME_TRANSACTION_COLUMN = \"TIME\"\n",
    "TIME_APPLICATION_COLUMN = \"TIME_APPLICATION\"\n",
    "ENTITY_SET_NAME = \"clients\"\n",
    "TRANSACTIONS_ENTITY_NAME = \"transactions\"\n",
    "APPLICATIONS_ENTITY_NAME = \"applications\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select parts, we want to execute:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose whether you would like to calculate monthly aggregations - see part 3\n",
    "CALCULATE_MONTHLY_AGGREGATIONS = False\n",
    "\n",
    "# Choose whether to do automatic segmentation - see part 4\n",
    "AUTOMATIC_INTERESTING_VALUES = False\n",
    "\n",
    "# maximal DFS depth - see part 6\n",
    "MAX_DFS_DEPTH = 1\n",
    "if CALCULATE_MONTHLY_AGGREGATIONS:\n",
    "    MAX_DFS_DEPTH = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data loading:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "RAWDATA_PATH = \"demo_data/rawsim2.csv\"\n",
    "transactions = pd.read_csv(RAWDATA_PATH, sep=\",\", decimal=\".\", encoding=\"ANSI\")\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _FeatureTools_ internal structure\n",
    "\n",
    "_FT_ work with _entities_. We can imagine the entities as an internal database structure, <br>\n",
    "which just captures the relationships between individual datasets.\n",
    "\n",
    "__Glossary__\n",
    "- One dataset = one entity\n",
    "- One resulting dataset = one entity - set\n",
    "- One entity set = multiple entities\n",
    "- Entity ID = dataset identifier in the entity\n",
    "- Relationship - mapping between entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single dataset\n",
    "\n",
    "When we have a single dataset, such dataset can already have multiple joined data.\n",
    "Feature tools allow to easily capture the relationships between such data.\n",
    "\n",
    "To capture that, we just need to tell the feature tools names of the individual indices, in this case `TRANSACTION_ID` and `APPLICATION_ID`.\n",
    "\n",
    "\n",
    "For example of good use of `normalize_entity` function [see this notebook](https://github.com/Featuretools/predict-appointment-noshow/blob/master/Tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty entity set\n",
    "entity_set = ft.EntitySet(id=ENTITY_SET_NAME)\n",
    "\n",
    "\n",
    "# we fill the entity_set with the dataframes, and say, which IDs are relevant for given DF\n",
    "entity_set.entity_from_dataframe(\n",
    "    entity_id=TRANSACTIONS_ENTITY_NAME,\n",
    "    dataframe=transactions,\n",
    "    index=ID_TRANSACTIONS_COLUMN,\n",
    "    time_index=TIME_TRANSACTION_COLUMN,\n",
    ")\n",
    "\n",
    "entity_set.normalize_entity(\n",
    "    base_entity_id=TRANSACTIONS_ENTITY_NAME,\n",
    "    new_entity_id=APPLICATIONS_ENTITY_NAME,\n",
    "    index=ID_APPLICATION_COLUMN,\n",
    "    make_time_index=False,\n",
    ")\n",
    "\n",
    "# entity_set.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple datasets\n",
    "\n",
    "However, feature tools are stronger when we have multiple datasets.\n",
    "\n",
    "If we create one huge dataset, this takes a lot of memory - same value (i.e. application times) is multiplied several times.\n",
    "\n",
    "We can create the same  final dataset with relationship definition only.\n",
    "\n",
    "To see how complicated these relationships can be, I recommend [this notebook](https://github.com/Featuretools/predict-olympic-medals/blob/master/PredictOlympicMedals.ipynb) where this [entity set loading function is utilized](https://github.com/Featuretools/predict-olympic-medals/blob/901c4dcbd0ae0aa82994dc4ab80dba44c9e2a35b/utils.py#L115)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for demonstration purposes only - we divide the dataset in application ids and transactions\n",
    "# aggregated on the application level\n",
    "applications = transactions[\n",
    "    [ID_APPLICATION_COLUMN]\n",
    "].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume now, that we have two independent datasets. \n",
    "\n",
    "With feature tools, we can easly capture the relationships in the data, and process the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty entity set\n",
    "entity_set = ft.EntitySet(id=ENTITY_SET_NAME)\n",
    "\n",
    "\n",
    "# we fill the entity_set with the dataframes, and say, which IDs are relevant for given DF\n",
    "entity_set.entity_from_dataframe(\n",
    "    entity_id=TRANSACTIONS_ENTITY_NAME,\n",
    "    dataframe=transactions,\n",
    "    index=ID_TRANSACTIONS_COLUMN,\n",
    "    time_index=TIME_TRANSACTION_COLUMN,\n",
    ")\n",
    "\n",
    "\n",
    "#\n",
    "entity_set.entity_from_dataframe(\n",
    "    entity_id=APPLICATIONS_ENTITY_NAME,\n",
    "    dataframe=applications,\n",
    "    index=ID_APPLICATION_COLUMN,\n",
    ")\n",
    "\n",
    "# Specification of the relationship between entities\n",
    "r_transactions_applications = ft.Relationship(\n",
    "    child_variable=entity_set[TRANSACTIONS_ENTITY_NAME][ID_APPLICATION_COLUMN],\n",
    "    parent_variable=entity_set[APPLICATIONS_ENTITY_NAME][ID_APPLICATION_COLUMN],\n",
    ")\n",
    "entity_set.add_relationship(r_transactions_applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether FeatureTools identified all columns correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_set['transactions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling time dimension\n",
    "\n",
    "Feature tools have several ways, how to handle time. The most important terms are _time indices_ and _cutoff times_.\n",
    "\n",
    "To explain:\n",
    "\n",
    "- time indices - this is the time point, when our data became known. \n",
    "- cutoff times - when we should cut the times, as everything else is future information\n",
    "\n",
    "In this case, we set time indices as the `TIME_TRANSACTION_COLUMN` and we will create cutoff times from `TIME_APPLICATION_COLUMN`.\n",
    "\n",
    "This will ensure that:\n",
    "\n",
    "- when creating features with time dimension - these will take into account `TIME_TRANSACTION_COLUMN` only\n",
    "- if correctly paired, all features with time dimension will take into account only variables before observations in `TIME_APPLICATION_COLUMN`\n",
    "\n",
    "More about this (strongly recommended!) is in the [feature tools documentation.](https://docs.featuretools.com/automated_feature_engineering/handling_time.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutoff times\n",
    "\n",
    "Cutoff times need only two pieces of information \n",
    "\n",
    "- `ID_APPLICATION` - so they can be correctly paired\n",
    "- `TIME_APPLICATION` - so they can cut the future information off correctly, for given application id.\n",
    "\n",
    "_NOTE: Interesting trick with cutoff times is that any column added to column times is just pinned to the final dataset.\n",
    "If we have some variable, which is already correctly calculated for the final dataset - we can add them here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_times = transactions[\n",
    "    [\n",
    "        TIME_APPLICATION_COLUMN,\n",
    "        ID_APPLICATION_COLUMN,\n",
    "        # variable_1_which_i_would_like_to_add_to_final_data,\n",
    "        # variable_2_which_i_would_like_to_add_to_final_data, ...\n",
    "    ]\n",
    "].drop_duplicates()\n",
    "\n",
    "# transactions = transactions.drop([\n",
    "# variable_1_which_i_would_like_to_add_to_final_data,\n",
    "# variable_2_which_i_would_like_to_add_to_final_data,\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermezzo: Monthly aggregations\n",
    "\n",
    "What if we would like to **aggregate data on monthly level first**, and then **afterwards aggregate these?** (i.e. Slicer functionality).\n",
    "\n",
    "This part also shows how to **apply cutoff times manually** - we just omit all observations, where `MONTH_DIFFERENCE` will be negative.\n",
    "\n",
    "Proposed solution:\n",
    "\n",
    "- create column, which indicates the number of months between `TIME_TRANSACTION_COLUMN` and `TIME_APPLICATION_COLUMN`. Call this column `MONTH_DIFFERENCE`.\n",
    "- if you want to remove future - remove negative months from `MONTH_DIFFERENCE`.\n",
    "- create combined `ID_APPLICATION_MONTH_DIFFERENCE` column by string concatenation.\n",
    "- normalize entity on `ID_APPLICATION_MONTH_DIFFERENCE`.\n",
    "- normalize entity on `ID_APPLICATION` as well.\n",
    "\n",
    "__NOTE:__ Calculation of monthly aggregations need DFS depth of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CALCULATE_MONTHLY_AGGREGATIONS:\n",
    "\n",
    "    # EXAMPLE CODE: NOT TESTED IN THIS WORKFLOW\n",
    "    MONTH_DIFFERENCE_COLUMN = \"MONTH_DIFFERENCE\"\n",
    "    ID_APPLICATION_MONTH_COLUMN = \"ID_APPLICATION_MONTH\"\n",
    "    MONTH_ENTITY_NAME = 'monthly'\n",
    "\n",
    "\n",
    "    def months_between(d1, d2):\n",
    "        # same day in month = months_between = 0 (d1.dt.day<=d2.dt.day); count from 1 (the +1)\n",
    "        return (\n",
    "            (d1.dt.year - d2.dt.year) * 12\n",
    "            + d1.dt.month\n",
    "            - d2.dt.month\n",
    "            - (d1.dt.day <= d2.dt.day) * 1\n",
    "            + 1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we **show how to get rid of the future**.\n",
    "We have calculated number of months between, and now we omit the rows, where this difference is negative - transactions are in the future.\n",
    "\n",
    "**IMPORTANT NOTE:** We still have to use cutoff times, even if we get rid of the future. This is because cutoff time is fed into our custom primitives so we know, how far away back we should look in the time.\n",
    "\n",
    "If we want to calculate all custom primitives from single timepoint, we can do it with\n",
    "```\n",
    "...\n",
    "    cutoff_time=pd.Timestamp(\"2014-1-1 04:00\")\n",
    "...\n",
    "```\n",
    "when calculating dfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CALCULATE_MONTHLY_AGGREGATIONS:\n",
    "\n",
    "\n",
    "    transactions[MONTH_DIFFERENCE_COLUMN] = np.ceil(months_between(\n",
    "        transactions[TIME_APPLICATION_COLUMN], transactions[TIME_TRANSACTION_COLUMN]\n",
    "    ))\n",
    "\n",
    "    transactions = transactions.query(f\"{MONTH_DIFFERENCE_COLUMN} >= 0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we create new entity set**, with two normalised subentities. One for applications, and one for months+application ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CALCULATE_MONTHLY_AGGREGATIONS:\n",
    "    transactions[ID_APPLICATION_MONTH_COLUMN] = (\n",
    "        transactions[ID_APPLICATION_COLUMN].astype(str)\n",
    "        + \"_\"\n",
    "        + transactions[MONTH_DIFFERENCE_COLUMN].astype(str)\n",
    "    )\n",
    "    \n",
    "    # empty entity set\n",
    "    entity_set = ft.EntitySet(id=ENTITY_SET_NAME)\n",
    "\n",
    "\n",
    "    # we fill the entity_set with the dataframes, and say, which IDs are relevant for given DF\n",
    "    entity_set.entity_from_dataframe(\n",
    "        entity_id=TRANSACTIONS_ENTITY_NAME,\n",
    "        dataframe=transactions,\n",
    "        index=ID_TRANSACTIONS_COLUMN,\n",
    "        time_index=TIME_TRANSACTION_COLUMN,\n",
    "    )\n",
    "\n",
    "    entity_set.normalize_entity(\n",
    "        base_entity_id=TRANSACTIONS_ENTITY_NAME,\n",
    "        new_entity_id=APPLICATIONS_ENTITY_NAME,\n",
    "        index=ID_APPLICATION_COLUMN,\n",
    "        make_time_index=False,\n",
    "    )\n",
    "\n",
    "    entity_set.normalize_entity(\n",
    "        base_entity_id=TRANSACTIONS_ENTITY_NAME,\n",
    "        new_entity_id=MONTH_ENTITY_NAME,\n",
    "        index=ID_APPLICATION_MONTH_COLUMN,\n",
    "        make_time_index=False,\n",
    "    )\n",
    "\n",
    "entity_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Values - segmentation\n",
    "\n",
    "This line is one of the most interesting ones - we show featuretools, __by which variable we would like to segment the features.__ <br>\n",
    "\n",
    "Let's say that we are interested in segmentation by POS/ATM. Apart from that, we would like to know whether there are some differences in Hradec, Praha and Ostrava."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' CITY:        {transactions[\"CITY\"].unique().tolist()}')\n",
    "print(f' TRANS_PLACE: {transactions[\"TRANS_PLACE\"].unique().tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the segmenting, we need to set the `interesting_values` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_set[TRANSACTIONS_ENTITY_NAME][\"TRANS_PLACE\"].interesting_values = (\n",
    "    transactions[\"TRANS_PLACE\"].unique().tolist()\n",
    ")\n",
    "entity_set[TRANSACTIONS_ENTITY_NAME][\"CITY\"].interesting_values = [\"Hradec\", \"Praha\", \"Ostrava\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if we are lazy to define only the values which interest us, we can leave everything for the feature tools. Following command will create segmentations for every variable. We can set `max_values` argument, to limit the number of segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AUTOMATIC_INTERESTING_VALUES:\n",
    "    entity_set.add_interesting_values(max_values=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primitives\n",
    "\n",
    "The basic FT building blocks are primitives. These are the lego pieces, using which the individual features are being built.\n",
    "\n",
    "In the basic tools, we are provided with more than 70 different basic feature primitives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", 100):\n",
    "    display(ft.list_primitives())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom primitives\n",
    "\n",
    "Feature Tools allow for easy creation of new primitives.\n",
    "\n",
    "Let's see how some simple primitive is done.\n",
    "\n",
    "Common use case in HCI is to create aggregation over some time window.\n",
    "We will show another way how to do these time windows, but one of the solutions is creation of custom primitives.\n",
    "\n",
    "__First, we need to define the function itself.__ This can be typical pandas aggregation function. <br>\n",
    "\n",
    "__But we decided to use numpy structures and functions only__ - to get performance gain, as numpy functions are __often 400x__ faster than adequate pandas equivalents. <br>\n",
    "These functions are applied very often, and every optimalisation counts in the final performance.\n",
    "\n",
    "Interesting is the reserved argument `time=None`. This is reserved by feature tools itself and is applied when `cutoff_time` is specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_FUNCTIONS = dict(\n",
    "    max=np.nanmax,\n",
    "    min=np.nanmin,\n",
    "    mean=np.nanmean,\n",
    "    sum=np.nansum,\n",
    "    count=len,\n",
    "    mean_monthly=lambda x: np.nanmean(x) * 30,\n",
    ")\n",
    "\n",
    "\n",
    "def agg_last_x_days(values, time_col, days, func, time=None):\n",
    "    \"\"\"\n",
    "    Aggregate given data using prespecified aggregation functions.\n",
    "    Possibilities are set in CURRENT_FUNCTIONS\n",
    "    \n",
    "    \"\"\"\n",
    "    data = values[\n",
    "        time_col.values >= np.datetime64(time - np.timedelta64(days, \"D\"))\n",
    "    ].values\n",
    "    try:\n",
    "        result = CURRENT_FUNCTIONS[func](data)\n",
    "    except ValueError:\n",
    "        result = None\n",
    "    except KeyError:\n",
    "        print(\"Unidentified aggregation function\")\n",
    "        raise\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it is often helpful to provide custom name generating function.\n",
    "\n",
    "If the functions is intended to be used in interesting values, then it needs to have pre-specified where_stirng inside its name. Otherwise it is ignored (or to be more precise - it is rewritten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from featuretools.version import __version__\n",
    "if __version__ == '0.9.0':\n",
    "\n",
    "    def agg_last_x_days_generate_name(self, base_feature_names, **kwargs):\n",
    "    #     print(self.kwargs)\n",
    "    #     breakpoint()\n",
    "    #     print(kwargs)\n",
    "        name = \"{func}_{days}D({child_entity_id}.{feature}{where_string})\".format(\n",
    "            func=self.kwargs[\"func\"].upper(),\n",
    "            days=str(self.kwargs[\"days\"]),\n",
    "            child_entity_id=kwargs['relationship_path_name'],\n",
    "            feature=base_feature_names[0],\n",
    "            where_string=kwargs[\"where_str\"],\n",
    "        )\n",
    "        return name\n",
    "else:\n",
    "    def agg_last_x_days_generate_name(self, child_entity_id, base_feature_names, **kwargs):\n",
    "#         print(kwargs)\n",
    "        name = \"{func}_{days}D({child_entity_id}.{feature}{where_string})\".format(\n",
    "            func=self.kwargs[\"func\"].upper(),\n",
    "            days=str(self.kwargs[\"days\"]),\n",
    "            child_entity_id=child_entity_id,\n",
    "            feature=base_feature_names[0],\n",
    "            where_string=kwargs[\"where_str\"],\n",
    "        )\n",
    "        return name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, feature tools provide us with function to create custom primitives. It is needed to specify input and output types for the individual function, so the deep feature synthesis can stack the features together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGG_LAST_X_DAYS = ft.primitives.make_agg_primitive(\n",
    "    function=agg_last_x_days,  # function to be used\n",
    "    input_types=[\n",
    "        ft.variable_types.Numeric,\n",
    "        ft.variable_types.DatetimeTimeIndex,\n",
    "    ],  # input data types\n",
    "    return_type=ft.variable_types.Numeric,  # data types to be returned\n",
    "    uses_calc_time=True,  # whether function can utilize cutoff time information\n",
    "    stack_on_self=False,  # whether the primitive could be stacked on itself\n",
    "    cls_attributes={\n",
    "        \"generate_name\": agg_last_x_days_generate_name\n",
    "    },  # passing of name generating function\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already some custom primitives prepared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from scoring.feature_tools.custom_primitives import (\n",
    "    TIME_SINCE_LAST,\n",
    "    TIME_SINCE_FIRST,\n",
    "    AVG_TIME_BETWEEN,\n",
    "    COUNT_X_DAYS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Feature Synthesis\n",
    "\n",
    "The most important cell - here we create the features itself. __We use the building blocks - aggregations and transformations.__ Just like lego. And from these building blocks, we create the final features.\n",
    "\n",
    "We can also control the complexity of the features using `max_depth`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DFS Definition\n",
    "\n",
    "Now we are defining the lego blocks with which the DFS is going to play with.\n",
    "\n",
    "\n",
    "- **target entity**, on which level the data should be specified\n",
    "- **maximum depth of the features** - how many primitives should the features use, at most.  Rule of thumb is to use 1 for easily explained features, 2 in case you want to get ratios.\n",
    "\n",
    "**See Section 8 - appendix for definition of more advanced config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primitives_dfs_definition = dict(\n",
    "    # entity, for which we would like to calculate individual features\n",
    "    target_entity=APPLICATIONS_ENTITY_NAME,\n",
    "    # depth definition\n",
    "    max_depth=MAX_DFS_DEPTH,\n",
    "\n",
    "    agg_primitives=[\n",
    "        \"max\",\n",
    "        \"mean\",\n",
    "    ],\n",
    "    # primitives to be used for transformations (1:1 mapping)\n",
    "    trans_primitives=[\n",
    "        \"days_since\",\n",
    "    ],\n",
    "    # primitives to be used to aggregate important values (n:1 mapping - same as aggregations)\n",
    "    where_primitives=[\n",
    "        \"max\",\n",
    "        \"median\",\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the computation, we should also specify several technical aspects of the `dfs`.\n",
    "We should specify:\n",
    "- **entity set**, which has links to the dataframes\n",
    "- **cutoff_time** - so our dataset is not spoiled with future times\n",
    "- whether there are **some variables, which we would like to ignore**\n",
    "- **number of jobs**, for parallelisation. Good amount of jobs is even number, preferably multiple of available cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technical_dfs_definition = dict(\n",
    "    # relationship specification\n",
    "    entityset=entity_set,\n",
    "    \n",
    "    # parallelisation\n",
    "    n_jobs=4,\n",
    "    \n",
    "    # cutoff times for future omission\n",
    "    cutoff_time=cutoff_times if not CALCULATE_MONTHLY_AGGREGATIONS else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the script itself\n",
    "\n",
    "**And now, this is where the magic happens.**\n",
    "\n",
    "We perform the deep feature synthesis. For detailed view, what is happening behind you can observe [the original paper](http://www.jmaxkanter.com/static/papers/DSAA_DSM_2015.pdf) or [the summary of the key points](https://blog.featurelabs.com/deep-feature-synthesis/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_run1 = ft.dfs(\n",
    "    # whether feature synthesis should be run or not - the next step\n",
    "    features_only=True,\n",
    "    **technical_dfs_definition,\n",
    "    **primitives_dfs_definition\n",
    ")\n",
    "\n",
    "print(f\"Number of created variables: {len(variables_run1)}\")\n",
    "pprint(variables_run1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can do **multiple runs** of `dfs`. Notice that in subsequent runs, we can **manipulate with the feature depth.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primitives_dfs_definition_depth3 = dict(\n",
    "    target_entity=APPLICATIONS_ENTITY_NAME,\n",
    "    max_depth=3,\n",
    "\n",
    "    agg_primitives=[\n",
    "        'min',\n",
    "        'trend'\n",
    "    ],\n",
    "    trans_primitives=[\n",
    "        \"diff\"\n",
    "    ],\n",
    "    where_primitives=[\n",
    "        \"sum\",\n",
    "        AGG_LAST_X_DAYS(days=30, func=\"max\"),\n",
    "    ],\n",
    ")\n",
    "variables_to_create_depth3 = ft.dfs(\n",
    "    features_only=True,\n",
    "    **technical_dfs_definition,\n",
    "    **primitives_dfs_definition_depth3\n",
    ")\n",
    "pprint(variables_to_create_depth3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose only some of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_run2 = [var for var in variables_to_create_depth3 if 'DIFF' in var.generate_name()]\n",
    "pprint(variables_run2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can either select meaningful variables beforehand, or calculate the features directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_create = variables_run1 + variables_run2\n",
    "\n",
    "final_dataset = ft.calculate_feature_matrix(\n",
    "        features=variables_to_create,\n",
    "        **technical_dfs_definition,\n",
    ")\n",
    "    \n",
    "display(final_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part, we can observe whether we found the correct features. So, we create arbitrary target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of arbitrary target\n",
    "In the following part I create arbitrary target. We create several arbitrary relevant features. Maximal fee plays a role, and so does the average spending. It also plays a role where the person comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target definition\n",
    "target_base_1 = transactions.groupby([ID_APPLICATION_COLUMN]).mean()\n",
    "target_base_2 = transactions.groupby([ID_APPLICATION_COLUMN]).max()\n",
    "target_base_3 = (\n",
    "    transactions.groupby([ID_APPLICATION_COLUMN])\n",
    "    .CITY\n",
    "    .apply(lambda x: x.mode())\n",
    "    .reset_index()\n",
    "    .query(\"level_1 == 0\")\n",
    "    .set_index(ID_APPLICATION_COLUMN)\n",
    ")\n",
    "\n",
    "\n",
    "# defining target relationship with the data. To have absolute control over the feature engineering\n",
    "def standardize(column):\n",
    "    return (column - column.mean()) / column.std()\n",
    "\n",
    "# True target = default\n",
    "target = (\n",
    "    # Amount is very important\n",
    "    (standardize(target_base_1[\"AMOUNT\"]) * -2)\n",
    "    # Opposite effect have maximum fee\n",
    "    + (standardize(target_base_2[\"FEE\"]) * 2)\n",
    "    # Living in the city has a negative effect on default\n",
    "    + (standardize(target_base_3[\"CITY\"] == \"Praha\") * -2)\n",
    "    # Living in Ostrava has positive effect.\n",
    "    + (standardize(target_base_3[\"CITY\"] == \"Ostrava\") * 2)\n",
    ")\n",
    "\n",
    "target = standardize(target) > 0.5  # Approx 30% will be True, otherwise False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(target) / len(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost feature importance (using SHAP values)\n",
    "\n",
    "In this part we evaluate the feature importance of individual features.\n",
    "\n",
    "In the next part, we do basic preprocessing. We identify categorical features (automatically), and recode them using one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "categorical_features = [\n",
    "    feat for feat in final_dataset.columns if final_dataset[feat].dtype.kind == \"O\"\n",
    "]\n",
    "\n",
    "for feat in categorical_features:\n",
    "    final_dataset = pd.concat(\n",
    "        [final_dataset, pd.get_dummies(final_dataset[feat], prefix=feat)], axis=1\n",
    "    ).drop(columns=[feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model evaluation, we split the data in 3 parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    final_dataset, target, test_size=0.2, random_state=43\n",
    ")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    final_dataset, target, test_size=0.3, random_state=43\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now, we train basic XGBoost classifier. This is very fast, in contrast with feature tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 5,\n",
    "\n",
    "    'n_estimators': 1000,\n",
    "    'early_stopping_rounds': 20,\n",
    "    \n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'n_jobs': 1,\n",
    "    'random_state': 12345\n",
    "}\n",
    "\n",
    "booster_xgb_sklearn = XGBClassifier(**params)\n",
    "\n",
    "booster_xgb_sklearn.fit(X_train, y_train,\n",
    "                        eval_set=[(X_train, y_train),\n",
    "                                  (X_valid, y_valid)],\n",
    "                        early_stopping_rounds = params['early_stopping_rounds']\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature evaluation\n",
    "\n",
    "When the model is trained, we can now find the best features. \n",
    "\n",
    "As we already know the underlying features, we can observe that even though we have not captured the features exactly (e.g. max(transactions.FEE)), a lot of found features are similar, but segmented. \n",
    "\n",
    "To evaluate individual features, SHAP values are used. The main idea comes from Shapley values - term coined in  game theory;  **It calculates how each feature, all other things given, would change the output of the model.**\n",
    "\n",
    "More about SHAP values can be [found in the original paper](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf), in this [descriptive article](https://towardsdatascience.com/how-to-avoid-the-machine-learning-blackbox-with-shap-da567fc64a8b), or here [on kaggle](https://www.kaggle.com/dansbecker/shap-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_xgb = booster_xgb_sklearn.get_booster().feature_names\n",
    "explainer_xgb = shap.TreeExplainer(booster_xgb_sklearn)\n",
    "shap_values_xgb = explainer_xgb.shap_values(X_train[feature_names_xgb])\n",
    "shap.summary_plot(shap_values_xgb, X_train[feature_names_xgb], max_display=30, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can select a cutoff and choose, which variables are the most important for the individual model.\n",
    "Now, using the cutoff we can select the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUTOFF = 0.05\n",
    "\n",
    "shap_importance_xgb = (\n",
    "    pd.DataFrame(shap_values_xgb, columns=feature_names_xgb)\n",
    "    .abs()\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "# chosen_features = shap_importance_xgb.index[shap_importance_xgb > CUTOFF].tolist()\n",
    "\n",
    "display(shap_importance_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to find some itneractions and visualize the dependence plots as well. See https://slundberg.github.io/shap/notebooks/NHANES%20I%20Survival%20Model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the possible primitive variable definitions, w/ commented functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "primitives_dfs_definition = dict(\n",
    "    # entity, for which we would like to calculate individual features\n",
    "    target_entity=APPLICATIONS_ENTITY_NAME,\n",
    "    # depth definition\n",
    "    max_depth=MAX_DFS_DEPTH,\n",
    "    # output cleaning part\n",
    "    # variables which will be dropped afterwards\n",
    "    # drop_contains = [f'({ID_APPLICATION_COLUMN})', f'1 / {ID_APPLICATION_COLUMN}'],\n",
    "    # variables not to be calculated - ignored in dfs\n",
    "    # ignore_variables={TRANSACTIONS_ENTITY_NAME: [ID_APPLICATION_COLUMN]},\n",
    "    # features to be used for aggregations (n:1 mapping)\n",
    "    agg_primitives=[\n",
    "#         AGG_LAST_X_DAYS(days=30, func=\"max\"),\n",
    "        # AGG_LAST_X_DAYS(days=10, func=\"min\"),\n",
    "#         AGG_LAST_X_DAYS(days=180, func=\"mean_monthly\"),\n",
    "        \"max\",\n",
    "#         \"mode\",\n",
    "#         \"min\",\n",
    "        \"mean\",\n",
    "#         \"trend\",\n",
    "    ],\n",
    "    # primitives to be used for transformations (1:1 mapping)\n",
    "    trans_primitives=[\n",
    "#         \"week\",\n",
    "#         \"month\",\n",
    "#         \"year\",\n",
    "        \"days_since\",\n",
    "#         \"diff\",\n",
    "#         \"absolute\",\n",
    "#         \"divide_by_feature\",\n",
    "#         \"cum_min\",\n",
    "#         \"cum_max\",\n",
    "    ],\n",
    "    # primitives to be used to aggregate important values (n:1 mapping - same as aggregations)\n",
    "    where_primitives=[\n",
    "#         \"min\",\n",
    "        \"max\",\n",
    "        \"median\",\n",
    "#         \"trend\",\n",
    "#         \"sum\",\n",
    "#         AGG_LAST_X_DAYS(days=30, func=\"max\"),\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
