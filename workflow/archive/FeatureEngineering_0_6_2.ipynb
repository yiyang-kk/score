{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 30\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import scoring\n",
    "import scoring.feature_engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Metadata\n",
    "\n",
    "- **rawdata** - data with transactions already paired to application. Before feature engineering, you will need to do this manually: you'll need a dataset with application ID (SKP_APPLICATION or SKP_CREDIT_CASE), application datetime left joined to dataset with transactions (which includes transaction datetime and all relevant dimensions and metrics related to the transactions). You should use condition application datetime > transaction datetime to ensure you don't \"see the future\"\n",
    "\n",
    "- **slicemeta** - metadata for Slicer\n",
    "\n",
    "- **metadata**, **agglist**, **varcomb** - metadata for instance of FeatureEngineeringFromSlice which runs after Slicer in this particular demo\n",
    "\n",
    "- **metadataR**, **agglistR**, **seglistR** - metadata for instance of FeatureEngineeringFromSlice which runs after OrderAssigner in this particular demo\n",
    "\n",
    "- **timesincemeta** - metadata for TimeSinceCalc\n",
    "\n",
    "- **issomemeta** - metadata for IsSomething\n",
    "\n",
    "- **intermeta** - metadata for Interactions\n",
    "\n",
    "- **catmeta** - metadata for instance of CategoricalFeatures which runs after OrderAssigner in this particular demo\n",
    "\n",
    "- **catmetab** - metadata for instance of CategoricalFeatures which runs after Slicer in this particular demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = pd.read_csv('demo_data/rawsim2.csv',sep=',',decimal='.',encoding='ANSI',keep_default_na = False, na_values = [''])\n",
    "\n",
    "slicemeta = pd.read_csv('demo_data/slicemeta2.csv',sep=',',decimal='.',encoding='ANSI')\n",
    "\n",
    "metadata = pd.read_csv('demo_data/aggregations_metadata4.csv',sep=',',decimal='.',encoding='ANSI')\n",
    "agglist = pd.read_csv('demo_data/aggregations_agglist5.csv',sep=',',decimal='.',encoding='ANSI')\n",
    "varcomb = pd.read_csv('demo_data/varcomb2.csv',sep=',',decimal='.',encoding='ANSI')\n",
    "\n",
    "metadataR = pd.read_csv('demo_data/aggregations_metadataR.csv',sep=',',decimal='.',encoding='ANSI')\n",
    "agglistR = pd.read_csv('demo_data/aggregations_agglistR.csv',sep=',',decimal='.',encoding='ANSI')\n",
    "seglistR = pd.read_csv('demo_data/aggregations_segmlist.csv',sep=',',decimal='.',encoding='ANSI')\n",
    "\n",
    "timesincemeta = pd.read_csv('demo_data/timesincemeta2.csv',sep=',',decimal='.',encoding='ANSI')\n",
    "\n",
    "issomemeta = pd.read_csv('demo_data/issomemeta.csv',sep=',',decimal='.',encoding='ANSI')\n",
    "\n",
    "intermeta = pd.read_csv('demo_data/intermeta.csv',sep=',',decimal='.',encoding='ANSI')\n",
    "\n",
    "catmeta = pd.read_csv('demo_data/catmeta.csv',sep=',',decimal='.',encoding='ANSI')\n",
    "catmetab = pd.read_csv('demo_data/catmetab.csv',sep=',',decimal='.',encoding='ANSI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RELOADING FE MODULE FOR TESTING PURPOSES\n",
    "#import importlib\n",
    "#importlib.reload(scoring)\n",
    "#importlib.reload(scoring.feature_engineering_files.feature_engineering_from_slice)\n",
    "#importlib.reload(scoring.feature_engineering_files.categorical_features)\n",
    "#importlib.reload(scoring.feature_engineering_files.interactions)\n",
    "#importlib.reload(scoring.feature_engineering_files.is_something)\n",
    "#importlib.reload(scoring.feature_engineering_files.slicer_order_assigner)\n",
    "#importlib.reload(scoring.feature_engineering_files.time_since_calc)\n",
    "#importlib.reload(scoring.feature_engineering_files.utils)\n",
    "#importlib.reload(scoring.feature_engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data include:\n",
    "- **ID_TRANSACTION**: primary key for each transaction.\n",
    "- **ID_APPLICATION**: primary key for each loan application.\n",
    "There can be multiple transactions per each application and also multiple applications for each transaction (as it might happen that two or more applications belong to the same client). This means that primary key of this raw data table is combination of ID_TRANSACTION and ID_APPLICATION!\n",
    "- **TIME**: datetime of transaction\n",
    "- **TIME_APPLICATION**: datetime of APPLICATION\n",
    "The time fields are the most important as the feature engineering module's primary purpose is to create time-based features. Most of the features utilize transactional history (i.e. some properties of transactions which occured before the application). That's why in your dataset, you should ensure that TIME < TIME_APPLICATION. Transactions which occured after the application should be dropped from such dataset.\n",
    "- **AMOUNT**, **FEE**: transaction metrics\n",
    "- **TRANS_TYPE**, **TRANS_PLACE**, **CITY**: transaction dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(rawdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features from transactional granularity\n",
    "\n",
    "## OrderAssigner\n",
    "OrderAssigner which adds new column – difference of application time and transaction in desired unit (monts, days etc.) as integer – so the transactions can be grouped by this time unit later. The output is meant to be processed further.\n",
    "\n",
    "New instance of this class is initiated with parameters which point to metadata of the dataset that will be transformed.\n",
    "\n",
    "Names of the following variables are given as parameters during initiation: application time, transaction time. \n",
    "\n",
    "New instance of this class is initiated with parameters which point to metadata of the dataset that will be transformed.\n",
    "After initiation, fit method is called. Argument of the fit method is the dataset to be transformed.\n",
    "\n",
    "After fit, transform method is called. Argument of the transform method is the dataset to be transform. Output of transform method is the transformed data. During the run of transform method, sql code which is doing the same transformations in Oracle database is generated as property strsql_.\n",
    "\n",
    "**Parameters**\n",
    "- time_name : string - TIME ID column name of the datasets that will be used in fit and transform procedures\n",
    "- time_max_name : string - TIME MAX column is the column of \"current time\" for each ID, i.e. time from which the history is calculated back from, Example: time_max_name = application date, time_name = transaction date, only rows with time_name <= time_max_name are taken into account\n",
    "- time_granularity : granularity of the time intervals, can be 'years', 'months', 'weeks', 'days', 'hours', 'minutes', 'seconds' or 'order' (in such case, just the order of the row in time (sorted descending from time_max_name) is returned)\n",
    "- history_length : int, optional - how long the history should be (examplle values: 12 or 24 for months, 30 for days etc.)\n",
    "- time_format : string - the format of time columns \n",
    "- partition_name : string - if time_granularity is 'order', put the name of the column with the ID of the client (or other partitioning entity) here. the order is the calculated in each partition (e.g. for each client) separately\n",
    "- uppercase_suffix : boolean, optional - boolean if the suffix of the aggregation type should be uppercase\n",
    "\n",
    "**Attributes**\n",
    "- X_out : DataFrame - output \n",
    "- strsql_ : string - SQL query which makes the same transformation on Oracle database\n",
    "Methods\n",
    "- fit(X, y = None) : checks if the columns specified in parameters are present in the dataset\n",
    "    - X : dataframe - the dataframe you want the aggregations to be executed on\n",
    "- transform(X) : execute the time since aggregations   \n",
    "    - X : dataframe - the dataframe you want the aggregations to be executed on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.feature_engineering import OrderAssigner\n",
    "\n",
    "oa = OrderAssigner(time_name = 'TIME', \n",
    "                   time_max_name = 'TIME_APPLICATION',\n",
    "                   time_granularity = 'months',\n",
    "                   history_length = 6,\n",
    "                   time_format = '%Y-%m-%d %H:%M:%S',\n",
    "                   partition_name = None)\n",
    "oa.fit(rawdata)\n",
    "orderdata = oa.transform(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(oa.strsql_)\n",
    "display(orderdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeatureEngineeringFromSlice\n",
    "\n",
    "Using the time unit index, this class can calculate:\n",
    "- First level aggregations and their ratios from the TimeSinceCalc output, e.g. average of transaction amounts of last 3 months, average of transaction amounts of last 3 months / maximum of transaction amounts of last 1 month\n",
    "- Second level aggregations and their ratios from Slicer output, e.g. maximum of monthly averages of transaction amounts of last 3 months, maximum of monthly averages of transaction amounts of last 3 months / maximum of monthly averages of transaction amounts of months with index from 4 to 6\n",
    "\n",
    "New instance of this class is initiated with parameters which point to metadata of the dataset that will be transformed. \n",
    "\n",
    "Names of the following variables are given as parameters during initiation: application ID, time key (index of time interval, which can be generated using Slicer or OrderAssigner).\n",
    "There are three metadata matrices which specify which aggregations should be created:\n",
    "\n",
    "*Meta matrix*: Includes names of the variables from input data set which should be aggregated. In second column of this matrix, you should also define a “native” aggregating function for each of the variable (this function will be used in the Aggregation list matrix which is described below). All the variables specified here in Meta matrix are then aggregated into aggregations which are specified in Aggregation list matrix (see below).\n",
    "\n",
    "*Aggregation list matrix*: Includes instructions what variables should be created from each of the variable mentioned in meta matrix. It has 12 columns. In first column you specify how long history is needed for the aggregation to be calculated. In 2nd and 3rd column, you specify type of aggregation. The aggregation can be “basic”/“parametric”/”varcomb”/”segmented” (specified in 2nd column) and “simple”/”ratio” (specified in 3rd column).\n",
    "\n",
    "Basic aggregations are exactly specified in columns 4-12. You need to specify what time intervals are aggregated using which function. You can also specify whether there will be some kind of special condition applied before the aggregation to filter the rows. For this kind of “special condition” aggregations, you should specify also suffix which will differentiate the resulting variable from the common one.\n",
    "\n",
    "Parametric aggregations are similar to basic aggregations but you can use the word “parametric” to specify aggregation function. In these aggregation, the function specified in meta matrix as “native” will be used for the corresponding variable\n",
    "\n",
    "Varcomb aggregations are ratios of two different variables. Variables for these aggregations are specified in variable combination matrix which is described below\n",
    "\n",
    "Segmented aggregations work the same way as basic aggregations, but they are performed on clusters of rows defined by segmentation variable (any categorical variable).\n",
    "\n",
    "Simple aggregations are aggregations like “(agg) of (variable) within (time units) with indexes (from)-(to)”. Ratio aggregations on the other hand are adding a denominator to the formula, so they are computed like “(agg1) of (variable) within (time units) with indexes (from1)-(to1) divided by (agg2) of (variable) within (time units) with indexes (from2)-(to2)”, e.g. maximal DPD in last 3 months (simple) or maximal DPD in last 3 months / maximal DPD in months 4-6.\n",
    "\n",
    "*Variable combination matrix*: Includes pairs of variables which are the used for ratios to be calculated. E.g. row including a pair: SUM_AMOUNT, COUNT_TRANSACTION can be used for creating varibles such as sum of monthly sum of amounts in last 3 months / sum of monthly count of transactions in last 3 months. The specific aggregations are specified in abovementioned Aggregation list matrix, as aggregations with type “varcomb” (the aggregations are the same for each pair of variables specified in variable combination matrix).\n",
    "\n",
    "*Segmentation variable matrix*: Includes variables which should be used for segmentations in “segmented” type of aggregations. These aggregations can be for example SUM of transaction amount of all transactions of type POS/ATM during last 6 months, where POS/ATM are the distinct values of the segmentation variable. For this example, two features would be created (one for POS, second for ATM).\n",
    "\n",
    "After initiation, fit method is called. Argument of the fit method is the dataset to be transformed.\n",
    "\n",
    "After fit, transform method is called. Argument of the transform method is the dataset to be transform. Output of transform method is the transformed data. During the run of transform method, sql code which is doing the same transformations in Oracle database is generated as property strsql_.\n",
    "\n",
    "**Parameters**\n",
    "- id_name : string - ID column name of the datasets that will be used in fit and transform procedures\n",
    "- time_name : string  - TIME ID column name of the datasets that will be used in fit and transform procedures\n",
    "- metadata : matrix - a matrix of metadata - telling which aggregation families should be performed for which column, columns:\n",
    "    1. variable name (variable to be used for FE),\n",
    "    2. aggregation func (e.g. sum)\n",
    "- agglist : matrix - a matrix of aggregation types - defining the aggregation families, columns: \n",
    "    1. minimal max month,\n",
    "    2. type of aggregation (basic/parametric/varcomb/segmented),\n",
    "    3. 2nd type of aggregation (simple/ratio),\n",
    "    4. from (...of basic aggregation or numerator of ratio),\n",
    "    5. to (...of basic aggregation or numerator of ratio),\n",
    "    6. func (...of basic aggregation or numerator of ratio),\n",
    "    7. from (...of denominator of ratio),\n",
    "    8. to (...of denominator of ratio),\n",
    "    9. func (...of denominator of ratio),\n",
    "    10. query (...additional condition for basic aggregation or numerator of ratio)\n",
    "    11. query (...additional condition for denominator of ratio)\n",
    "    12. suffix (...suffix for the columns using the queries)\n",
    "- varcomb : matrix, optional - a matrix of varible combinations, columns:\n",
    "    1. variable for numerator.\n",
    "    2. variable for denominator\n",
    "- segm: matrix, optional – a matrix of segmentation variables, for which aggregations of type “segmented” should be created, columns:\n",
    "    1. segmentation variable\n",
    "- max_time : int, optional - number of time units that the aggregations are based on\n",
    "- uppercase_suffix : boolean, optional - if the suffix of the aggregation type should be uppercase\n",
    "- min_fill_share : decimal, optional - share of filled (non-NaN) rows of feature for this feature to be added to the new dataset (if set to 0, all columns will be added. If set to 1, only fully filled columns will be added)\n",
    "\n",
    "**Attributes**\n",
    "- X_in : DataFrame - input\n",
    "- X_out : DataFrame - output \n",
    "- strsql_ : string - SQL query which makes the same transformation on Oracle database\n",
    "\n",
    "**Methods**\n",
    "- fit(X, y = None) : go through the aggregation metadata and put all valid aggregations into a special structure\n",
    "    - X : dataframe - the dataframe you want the aggregations to be executed on\n",
    "- transform(X) : execute the aggregations \n",
    "    - X : dataframe - the dataframe you want the aggregations to be executed on  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.feature_engineering import FeatureEngineeringFromSlice\n",
    "\n",
    "fe1 = FeatureEngineeringFromSlice(id_name = 'ID_APPLICATION',\n",
    "                                  time_name = 'TIME_ORDER',\n",
    "                                  metadata = metadataR,\n",
    "                                  agglist = agglistR, \n",
    "                                  varcomb = None,\n",
    "                                  segm = seglistR,\n",
    "                                  max_time = 6,\n",
    "                                  min_fill_share = 0)\n",
    "fe1.fit(orderdata)\n",
    "transrawdata = fe1.transform(orderdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fe1.strsql_)\n",
    "display(transrawdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CategoricalFeatures\n",
    "\n",
    "Creates aggregation which are specified in a matrix with metadata. The input data must be slice-indexed. Either in transactional granularity (from OrderAssigner) or in slice granularity (from Slicer)\n",
    "\n",
    "**Parameters**\n",
    "- id_name : string - ID column name of the datasets that will be used in fit and transform procedures\n",
    "- from type : string - can be either 'slice' or 'raw' determining whether the features are created from pre-aggregated slices (i.e. after Slicer) or from raw-granularity data (i.e. after OrderAssigner)\n",
    "- catmeta : matrix - matrix of slice metadata - defining how the raw data set should be sliced, columns: \n",
    "    1. variable: name of categorical variable the features will be based on\n",
    "    2. aggregation type: mode, nunique, last, first, argmax, argmaxsum, argmaxmean, argmin, argminsum, argminmean, nchanges, tschange\n",
    "    3. from: minimal time index that will be taken into account for the aggragation calculation\n",
    "    4. to: maxinal time index that will be taken into account for the aggragation calculation\n",
    "    5. nancategory: whether NaN is a separate category or whether such rows should not be used\n",
    "    6. metric: for argmax/argmin/argmaxsum/argminsum/argmaxmean/argminmean type of aggregations, this is the metric the max/sum/mean is calculated from\n",
    "    7. granularity: for tschange aggregation (when calculated from raw data), this is the time unit the time since is calculated in\n",
    "- slice_name: string - time index (from Slicer or Order Assigner)\n",
    "- time_name: string, optional - from_type = 'raw', this should be time of the transaction\n",
    "- time_max_name: string, optional - for from_type = 'raw', time dimension of the ID (e.g. in the final aggregations granularity)\n",
    "- time_format : string - the format of time columns\n",
    "- uppercase_suffix : boolean, optional - boolean if the suffix of the aggregation type should be uppercase \n",
    "\n",
    "**Attributes**\n",
    "- X_in : DataFrame - input\n",
    "- X_out : DataFrame - output \n",
    "- strsql_ : string - SQL query which makes the same transformation on Oracle database\n",
    "\n",
    "**Methods** \n",
    "- fit(X, y = None) : go through the categorical aggregation metadata and put all valid aggregations into a special structure\n",
    "    - X : dataframe - the dataframe you want the aggregations to be executed on\n",
    "- transform(X) : execute the aggregations \n",
    "    - X : dataframe - the dataframe you want the aggregations to be executed on  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.feature_engineering import CategoricalFeatures\n",
    "\n",
    "ct = CategoricalFeatures(id_name = 'ID_APPLICATION', \n",
    "                         from_type = 'raw', \n",
    "                         catmeta = catmeta,\n",
    "                         slice_name = 'TIME_ORDER',\n",
    "                         time_name = 'TIME',\n",
    "                         time_max_name = 'TIME_APPLICATION',\n",
    "                         time_format = '%Y-%m-%d %H:%M:%S')\n",
    "ct.fit(orderdata)\n",
    "catdata = ct.transform(orderdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ct.strsql_)\n",
    "display(catdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features from monthly slices\n",
    "\n",
    "## Slicer\n",
    "\n",
    "Slicer creates some basic aggregations in granularity of time unit (you can specify whether it should be day, month etc.). The output from slicer will be table in granularity of application ID x time unit index (where x means Cartesian product). For example with 6 months of transactional history, you will get up to 6 rows per each application ID, and the columns in this table can be maximums, minimums, means, sums and other aggregations of the transactions per each month. The output is meant to be processed further.\n",
    "\n",
    "New instance of this class is initiated with parameters which point to metadata of the dataset that will be transformed.\n",
    "\n",
    "Names of the following variables are given as parameters during initiation: application ID, application time, transaction time. \n",
    "\n",
    "The most important metadata is the “slice metadata matrix”. This matrix has 4 columns. The first two columns are mandatory – name of the variable and aggregation type. Example: for variable AMOUNT and type SUM, the slicer creates variable with sum of amount in each time slice.\n",
    "\n",
    "The other two columns of the matrix are optional. Third column is the name of segmentation variable (which must be categorical). If the segmentation variable is filled, the aggregations are calculated “group by” this variable. The last column is 0/1 flag, whether these segmented aggregations should be calculated relatively (as ratio to non-segmented version). Examples: sum of amount of transactions on credit cards, sum of amount of transactions on debit cards, sum of amount of transactions on credit cards relatively to sum of amount of all transactions.\n",
    "\n",
    "After initiation, fit method is called. Argument of the fit method is the dataset to be transformed.\n",
    "\n",
    "After fit, transform method is called. Argument of the transform method is the dataset to be transform. Output of transform method is the transformed data. During the run of transform method, sql code which is doing the same transformations in Oracle database is generated as property strsql_.\n",
    "\n",
    "**Parameters **\n",
    "- id_name : string - ID column name of the datasets that will be used in fit and transform procedures\n",
    "- time_name : string - TIME ID column name of the datasets that will be used in fit and transform procedures\n",
    "- time_max_name : string - TIME MAX column is the column of \"current time\" for each ID, i.e. time from which the history is calculated back from, Example: time_max_name = application date, time_name = transaction date, only rows with time_name <= time_max_name are taken into account\n",
    "- slicemeta : matrix - a matrix of slice metadata - defining how the raw data set should be sliced, columns: \n",
    "    1. variable name (variable to be aggregated),\n",
    "    2. aggregation func (e.g. sum),\n",
    "    3. segmentation variable (meaning the aggregation should be segmented by this variable) which can be also empty,\n",
    "    4. relative segmentation flag (int 0/1) (if the segmented aggr. should be calculated as ratio to the non-segm. aggr.)   \n",
    "    5. categorical variable flag (int 0/1) flag whether the variable is categorical (for such variables columns 3 and 4 are ignored)\n",
    "    6. 1 if np.nan is considered a category (applies only if column 5 indicates the variable is categorical) or 0 if removed\n",
    "    7. metric for some categorical variable aggregation: for argmax/argmin/argmaxsum/argminsum/argmaxmean/argminmean type of aggregations, this is the metric the max/sum/mean is calculated from\n",
    "- time_granularity : string, optional - granularity of the time slices, can be 'months', 'weeks', 'days', 'hours'\n",
    "- history_length : int, optional - how long the history should be (examplle values: 12 or 24 for months, 30 for days etc.)\n",
    "- time_format : string - the format of time columns\n",
    "- uppercase_suffix : boolean, optional - boolean if the suffix of the aggregation type should be uppercase\n",
    "\n",
    "**Attributes**\n",
    "- X_in : DataFrame - input\n",
    "- X_out : DataFrame - output \n",
    "- strsql_ : string - SQL query which makes the same transformation on Oracle database\n",
    "\n",
    "**Methods**\n",
    "- fit(X, y = None) : go through the aggregation metadata and put all valid aggregations into a special structure\n",
    "    - X : dataframe - the dataframe you want the aggregations to be executed on\n",
    "- transform(X) : execute the slice aggregations   \n",
    "    - X : dataframe - the dataframe you want the aggregations to be executed on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.feature_engineering import Slicer\n",
    "\n",
    "sl = Slicer(id_name = 'ID_APPLICATION',\n",
    "            time_name = 'TIME',\n",
    "            time_max_name = 'TIME_APPLICATION',\n",
    "            slicemeta = slicemeta,\n",
    "            time_granularity = 'months',\n",
    "            history_length = 6,\n",
    "            time_format = '%Y-%m-%d %H:%M:%S')\n",
    "sl.fit(rawdata)\n",
    "newdata = sl.transform(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sl.strsql_)\n",
    "display(newdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeatureEngineeringFromSlice\n",
    "\n",
    "Previously, we created instance of this Class to transform the data which were pre-transformed by OrderAssigner. Equivalently we can call it to transform data pre-transformed by Slicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.feature_engineering import FeatureEngineeringFromSlice\n",
    "\n",
    "fe2 = FeatureEngineeringFromSlice(id_name = 'ID_APPLICATION',\n",
    "                                  time_name = 'TIME_ORDER',\n",
    "                                  metadata = metadata,\n",
    "                                  agglist = agglist,\n",
    "                                  varcomb = varcomb,\n",
    "                                  segm = None,\n",
    "                                  max_time = 6,\n",
    "                                  min_fill_share = 0)\n",
    "fe2.fit(newdata)\n",
    "transdata = fe2.transform(newdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fe2.strsql_)\n",
    "display(transdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CategoricalFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.feature_engineering import CategoricalFeatures\n",
    "\n",
    "ctb = CategoricalFeatures(id_name = 'ID_APPLICATION', \n",
    "                          from_type = 'slice', \n",
    "                          catmeta = catmetab, \n",
    "                          slice_name = 'TIME_ORDER',\n",
    "                          time_name = None,\n",
    "                          time_max_name = None,\n",
    "                          time_format = '%Y-%m-%d %H:%M:%S')\n",
    "ctb.fit(newdata)\n",
    "catdatab = ctb.transform(newdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ctb.strsql_)\n",
    "display(catdatab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time since first/last occurence of...\n",
    "\n",
    "## TimeSinceCalc\n",
    "\n",
    "TimeSinceCalc calculates time since last/first transaction which holds given properties. It basically calculates difference of application time and transaction in desired unit (monts, days etc.) as float, and then takes maximum or minimum of the time differences of specified subset of transactions.\n",
    "\n",
    "New instance of this class is initiated with parameters which point to metadata of the dataset that will be transformed. \n",
    "\n",
    "Names of the following variables are given as parameters during initiation: application ID, application time, transaction time. \n",
    "\n",
    "The most important metadata is “time since metadata matrix”. This matrix has 14 columns. The first 2 columns are mandatory: in first one you specify time granularity (in which time units should be the specific variable calculated in) and in the second one you specify whether you want to know the time since last or first transaction. If the other columns are not filled, the aggregation will be basically time since last or first transaction in the table.\n",
    "\n",
    "In the other columns user can specify conditions which will be applied – see below how to fill them. If the conditions are specified, the aggregation will be: time since last or first transaction in the table which is fulfilling the conditions.\n",
    "\n",
    "After initiation, fit method is called. Argument of the fit method is the dataset to be transformed.\n",
    "\n",
    "After fit, transform method is called. Argument of the transform method is the dataset to be transform. Output of transform method is the transformed data. During the run of transform method, sql code which is doing the same transformations in Oracle database is generated as property strsql_.\n",
    "\n",
    "**Parameters**\n",
    "-\tid_name : string - ID column name of the datasets that will be used in fit and transform procedures\n",
    "- \ttime_name : string - TIME ID column name of the datasets that will be used in fit and transform procedures\n",
    "- \ttime_max_name : string - TIME MAX column is the column of \"current time\" for each ID, i.e. time from which the history is calculated back from, Example: time_max_name = application date, time_name = transaction date, only rows with time_name <= time_max_name are taken into account\n",
    "- timesincemeta : matrix - a matrix of slice metadata - defining how the raw data set should be sliced, columns: \n",
    "    1. granularity: time units in which the time since should be calculated in, can be 'years', 'months', 'weeks', 'days', 'hours', 'minutes', 'seconds'\n",
    "    2. type: \"last\" or \"first\" - whether to calculated time since first or last transaction which happened in the history and is fulfilling the given condition\n",
    "    The other columns can specify up to two conditions which transaction must fulfill to enter the aggregation:\n",
    "    3. condition1: name of the column which the condition is based on. if empty, the condition is considered to be always true.\n",
    "    4. from1: if condition1 column is numeric, specify the left boundary of interval of values where the condition is considered fulfilled. If empty, the left boundary is considered to be -infinity.\n",
    "    5. from1eq: 0 or 1, specifying whether the inequality is sharp (1 for sharp)\n",
    "    6. to1: if condition1 column is numeric, specify the right boundary of interval of values where the condition is considered fulfilled. If empty, the left boundary is considered to be +infinity.\n",
    "    7. to1eq: 0 or 1, specifying whether the inequality is sharp (1 for sharp)\n",
    "    8. category1: if condition1 column is categorical, specify the category which the column should be equal to for the condition to be true. If empty, the algorithm scans the dataset for all the possible categories and uses each one of them as a separate condition. NaN is not considered a category.\n",
    "    9. etc. the same for second condition. If empty, the condition is considered to be always true.\n",
    "- time_format : string - the format of time columns\n",
    "- uppercase_suffix : boolean, optional - boolean if the suffix of the aggregation type should be uppercase\n",
    "\n",
    "**Attributes**\n",
    "- X_in : DataFrame - input\n",
    "- X_out : DataFrame - output \n",
    "- strsql_ : string - SQL query which makes the same transformation on Oracle database\n",
    "\n",
    "**Methods**\n",
    "- fit(X, y = None) : go through the aggregation metadata and put all valid aggregations into a special structure.\n",
    "    - X : dataframe, the dataframe you want the aggregations to be executed on\n",
    "- transform(X) : execute the time since aggregations   \n",
    "    - X : dataframe, the dataframe you want the aggregations to be executed on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.feature_engineering import TimeSinceCalc\n",
    "\n",
    "tsc = TimeSinceCalc(id_name = 'ID_APPLICATION',\n",
    "                    time_name = 'TIME',\n",
    "                    time_max_name = 'TIME_APPLICATION',\n",
    "                    timesincemeta = timesincemeta,\n",
    "                    time_format='%Y-%m-%d %H:%M:%S',\n",
    "                    keyword='entry')\n",
    "tsc.fit(rawdata)\n",
    "ds = tsc.transform(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tsc.strsql_)\n",
    "display(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special \"US Credit Bureau\"-like features\n",
    "\n",
    "## IsSomething\n",
    "\n",
    "This class calculates aggregations telling how many time intervals in row a specific event occurred and when was last time it occurred. Examples of such events are: monthly max transcation – how many times in a row was it greater than 100 EUR? When was last time it happened at least 3 times in a row? How long is the current series of monthly sum of transaction ascending each month?\n",
    "\n",
    "New instance of this class is initiated with parameters which point to metadata of the dataset that will be transformed. \n",
    "The transformation itself work on this principle:\n",
    "-\tFirst, a binary mask is applied to a given column based on criteria from metadata. We take a specified period (from-to) and for each time unit in this period we check whether given column fulfills given criteria (is ascending/descending in comparison with previous time unit; or is lower/greater/equal to a given value)\n",
    "-\tBased on this binary mask we can calculate aggregations like: how many time such event happened, how many time it happened in a row, how long ago such row of given length occurred etc.\n",
    "-\tThese aggregations are in granularity of application and so they are the final outputs (final features)\n",
    "This way we end up with the following conceptual predictors (Is Something means that the value in vector = TRUE) that can based on the configuration and input vector/filter generate the same predictors:\n",
    "-\tSum Is Something \\{x, y\\} (Number of months with balance \\> \\$0 in last 12 months)\n",
    "-\tMax Consecutive Is Something \\{x, y\\} (Maximum number of consecutive months with balance \\> \\$0 in last 12 months)\n",
    "-\tMax Consecutive Is Not Something \\{x, y\\} (Maximum number of consecutive months with balance = \\$0 in last 12 months) – Note that the binary condition might be complicated, so it is easier to generate new feature by negating the condition rather than adding new configuration for the opposite\n",
    "-\tConsecutive Is Something from Current \\{x, y\\}   (Number of consecutive months with balance \\> \\$0 in last 12 months, starting with current month)\n",
    "-\tConsecutive Is Not Something from Current \\{x, y\\} (Number of consecutive months with balance = \\$0 in last 12 months, starting with current month)\n",
    "-\tAvg Length of Consecutive Is Something \\{x, y\\} (Average length of consecutive months with balance \\> \\$0 in last 12 months)\n",
    "-\tAvg Length of Consecutive Is Not Something \\{x, y\\} (Average length of consecutive months with balance = \\$0 in last 12 months)\n",
    "-\tNumber of Consecutive Is or Is Not Something instances \\{x, y\\} (Number of consecutive instances with Balance > \\$0 or Balance = \\$0 in last 12 months)\n",
    "-\tNumber of Instances of Consecutive Something \\{x, y\\} is greater than z (Number of cases where Balance \\> \\$0 for more than or equal 3 consecutive months in last 12 months)\n",
    "-\tNumber of Instances of Consecutive Not Something \\{x, y\\} is greater than z (Number of cases where Balance = \\$0 for more than or equal 3 consecutive months in last 12 months)\n",
    "-\tDistance from last Consecutive Is Something \\{x, y\\} greater than z (Distance in months from the last occurrence of Balance \\> \\$0 for more than 3 consecutive months in last 12 months)\n",
    "-\tDistance from last Consecutive Is Not Something \\{x, y\\} greater than z (...dtto)\n",
    "-\tRatio Sum Is Something \\{x, s\\} and Sum Is Something \\{s, y\\} (Percentage of months where Balance \\> \\$0 in last 12 months)\n",
    "-\tRatio of Is Something \\{x, y\\} and Number of Records\n",
    "\n",
    "There is a metadata matrix “isSomeMatrix” which specify which aggregations should be created: you specify column name, condition for the binary transformation and time period for these aggregations to be calculated from. See below.\n",
    "\n",
    "After initiation, fit method is called. Argument of the fit method is the dataset to be transformed.\n",
    "\n",
    "After fit, transform method is called. Argument of the transform method is the dataset to be transform. Output of transform method is the transformed data.\n",
    "\n",
    "**Parameters**\n",
    "- id_name : string - ID column name of the datasets that will be used in fit and transform procedures\n",
    "- time_name : string – order (slice) name of the datasets that will be used in fit and transform procedures\n",
    "- issomemeta : matrix - matrix of aggregation types - defining the aggregation families, columns: \n",
    "    - column - name of column the condition is based on\n",
    "    - min order - minimal time order index which the condition is calculated for\n",
    "    - max order - maximal time order index which the condition is calculated for\n",
    "    - mid order - if filled, ratios of the occurencies condition fulfilled in intervals [min order,mid order] and (mid order,max order] are calculated\n",
    "    - threshold - if filled, counts of how many time the condition was fulfilled at least threshold-times in a row are calculated\n",
    "    - condition type - can be: \n",
    "        - asc (column values were ascending in time)\n",
    "        - desc (column values were descending in time)\n",
    "        - notasc (column values were not ascending in time)\n",
    "        - notdesc (column values were not descending in time)(column values were greater than condition value)\n",
    "        - \\>= (column values were greater than or equal to condition value)\n",
    "        - < (column values were lower than condition value)\n",
    "        - <= (column values were lower than or equal to condition value)\n",
    "        - = (column values were equal to condition value)\n",
    "    - condition value - value for conditin types >,>=,<,<=,=\n",
    "- max_time : int, optional - number of time units that the aggregations are based on. if max_order in issomemeta is greater than this, this overrides it. infinity by default\n",
    "- uppercase_suffix : boolean, optional  - boolean if the suffix of the aggregation type should be uppercase\n",
    "\n",
    "**Attributes**\n",
    "- X_in : DataFrame - input\n",
    "- X_out : DataFrame - output \n",
    "\n",
    "**Methods**\n",
    "- fit(X, y = None) : go through the aggregation metadata and put all valid aggregations into a special structure\n",
    "    - X : dataframe - the dataframe you want the aggregations to be executed on\n",
    "- transform(X) : execute the aggregations \n",
    "    - X : dataframe - the dataframe you want the aggregations to be executed on  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.feature_engineering import IsSomething\n",
    "\n",
    "iss = IsSomething(id_name = 'ID_APPLICATION',\n",
    "                  time_name = 'TIME_ORDER',\n",
    "                  issomemeta = issomemeta,\n",
    "                  max_time = np.inf)\n",
    "iss.fit(newdata)\n",
    "issomedata = iss.transform(newdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iss.strsql_)\n",
    "display(issomedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic interaction creation and special value cleaning\n",
    "\n",
    "## Interactions\n",
    "\n",
    "Creates combination from the columns of a dataset. The available types of combinations are:\n",
    "- sum: sum of two variables, i.e. var1 + var2, makes sense for numbers only\n",
    "- product: product of two variables, i.e. var1 * var2, makes sense for numbers only\n",
    "- difference: difference of two variables, i.e. var1 - var2, makes sense for numbers only\n",
    "- ratio: ratio of two variables, i.e. var1/var2, makes sense for numbers only\n",
    "- cartesian: Cartesian product of two variables. If any of these variables is numerical, it is automatically binned to quantiles, the number of quantiles can be specified in parametrization table\n",
    "- equality: comparison of two variables, the result is one of following: “<”, “=”, ly “>”, can be applied either to strings or to numbers\n",
    "- quantiles: bins a variable into quantiles, the number of quantiles can be specified in parametrization tale – in this case, variable2 is not used\n",
    "- length: length of string variable – in this case, variable2 is not used\n",
    "- clean: applies “cleaning”, i.e. filling of NaNs and infinites, to one variable – in this case, variable2 is not used\n",
    "\n",
    "**Parameters**\n",
    "- id_name : string - ID column name of the datasets that will be used in fit and transform procedures\n",
    "- intermeta : matrix - matrix of slice metadata - defining how the raw data set should be sliced, columns: \n",
    "    1. variable1: name of variable that should be combined with variable2\n",
    "    2. variable2: name of variable that should be combined with variable1\n",
    "    3. type: type of combination (the available types are mentioned in the description above)\n",
    "    4. in_nan: how NaNs should be treated BEFORE the transformation (can be left empty)\n",
    "    5. in_inf: how Infinities should be treated BEFORE the transformation (can be left empty)\n",
    "    6. out_nan: how NaNs should be treated AFTER the transformation (can be left empty)\n",
    "    7. out_inf: how Infinities should be treated AFTER the transformation (can be left empty)\n",
    "    8. bins: for “quantiles” or “cartesian” type of transformation, how many quantiles should be created\n",
    "- uppercase_suffix : boolean, optional - boolean if the suffix of the aggregation type should be uppercase \n",
    "\n",
    "**Attributes**\n",
    "- X_in : DataFrame - input\n",
    "- X_out : DataFrame - output \n",
    "- strsql_ : string - SQL query which makes the same transformation on Oracle database\n",
    "\n",
    "**Methods **\n",
    "- fit(X, y = None) : go through the interaction metadata and put all valid aggregations into a special structure\n",
    "    - X : dataframe - the dataframe you want the aggregations to be executed on\n",
    "- transform(X) : execute the interactions \n",
    "    - X : dataframe - the dataframe you want the aggregations to be executed on  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.feature_engineering import Interactions\n",
    "\n",
    "itr = Interactions(id_name = 'ID_APPLICATION',\n",
    "                   intermeta = intermeta)\n",
    "itr.fit(rawdata)\n",
    "itrdata = itr.transform(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(itr.strsql_)\n",
    "display(itrdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "596px",
    "left": "1535px",
    "right": "20px",
    "top": "73px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
