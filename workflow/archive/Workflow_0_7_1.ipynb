{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:30pt;font-weight:bold\">Home Credit Python Scoring Workflow v.0.7.1</font>\n",
    "\n",
    "**Copyright:**\n",
    "\n",
    "© 2017-2018, Pavel Sůva, Marek Teller, Martin Kotek, Jan Zeller, Marek Mukenšnabl, Kirill Odintsov, Jan Hynek, Elena Kuchina and Home Credit & Finance Bank Limited Liability Company, Moscow, Russia – all rights reserved\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the [License](http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "For list of contributors see [Gitlab page](https://git.homecredit.net/risk/python-scoring-workflow) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "\n",
    "## List of packages\n",
    "- `time`, datetime - ability to get current time for logs\n",
    "- `math` - basic mathematical functions (as logarithm etc.))\n",
    "- `random` - generate random selection from probability distributions\n",
    "- `NumPy` - for scientific, mathematical, numerical calculations\n",
    "- `Scipy` - for clustering and correlation calculations\n",
    "- `Pandas` - for efficient work with large data structures (you need pandas **version 0.23 or higher**)\n",
    "- `cx_Oracle` and sqlalchemy - for loading data from Oracle database (DWH etc.)\n",
    "- `scikit`-learn - all important machine learning (and statistical) algorithms used for training the models\n",
    "- `matplotlib` - for plotting the charts\n",
    "- `seaborn` - for statistical visualisations\n",
    "- `os` - for setting output paths for generated image files\n",
    "- `pickle` - to save models to external files\n",
    "- `tqdm` - intelligent progress bar\n",
    "- `tkinter` - for interactive Interactions tool GUI\n",
    "- `xgboost` - gradient boosting used for feature selection before regression\n",
    "\n",
    "**If any of these packages is missing, you have to install it from the Anaconda prompt using command *conda install packagename* where *packagename* is the name of the installed package.**\n",
    "\n",
    "There is another package called *scoring*, which is distributed along with this workflow. **The folder *scoring* must be located in the same folder as this workflow for the package to be loaded correctly.** Alternatively, you can locate it somewhere else and then use *sys.path.insert()* to map this location.\n",
    "\n",
    "## Other important prerequisites\n",
    "\n",
    "For the grouping some **extensions for Jupyter must be installed and enabled before Jupyter is started and the notebook is loaded**. These extensions are Javascripts running in the browser, so it is necessary to have a compatibile browser. Generally, Chrome is OK, Internet Explorer 11 is NOT OK. To install the extensions, run this in your Anaconda prompt:\n",
    "\n",
    "- `conda install ipywidgets`\n",
    "- `jupyter nbextension enable --py --sys-prefix widgetsnbextension`\n",
    "- `conda config --add channels conda-forge`\n",
    "- `conda install qgrid` \n",
    "- `jupyter nbextension enable --py --sys-prefix qgrid`\n",
    "- `conda install tqdm`\n",
    "\n",
    "Please, make sure that qgrid library that you installed in this step is **verison 1.0.3 or higher**. \n",
    "\n",
    "To be able to connect to Oracle database (to get the data directly from your DWH) you need a compatibile Oracle driver to be installed on your computer. **With 64-bit Python, you need to have 64-bit Oracle driver installed.** Before you install the driver, you need to have Java 8 JDK (JRE is not enough) installed on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import operator\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import cx_Oracle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os.path\n",
    "import pickle\n",
    "import gc\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "#import tkinter\n",
    "#import xgboost as xgb\n",
    "\n",
    "import sys\n",
    "# sys.path.insert(0, \"C:/Python_src/python-scoring-workflow/\")\n",
    "import scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set general technical parameters and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.close_figures=True\n",
    "from IPython.display import display, Markdown\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 15\n",
    "output_folder = 'documentation'\n",
    "\n",
    "if not os.path.exists(output_folder): os.makedirs(output_folder)\n",
    "if not os.path.exists(output_folder+'/performance'): os.makedirs(output_folder+'/performance')\n",
    "if not os.path.exists(output_folder+'/predictors'): os.makedirs(output_folder+'/predictors')\n",
    "if not os.path.exists(output_folder+'/stability'): os.makedirs(output_folder+'/stability')\n",
    "if not os.path.exists(output_folder+'/stability_short'): os.makedirs(output_folder+'/stability_short')\n",
    "if not os.path.exists(output_folder+'/analysis'): os.makedirs(output_folder+'/analysis')\n",
    "if not os.path.exists(output_folder+'/model'): os.makedirs(output_folder+'/model')\n",
    "if not os.path.exists(output_folder+'/nan_share'): os.makedirs(output_folder+'/nan_share')\n",
    "scoring.check_version('0.7.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialize doctools object for documentation generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring import doctools\n",
    "\n",
    "documentation = doctools.ProjectParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data\n",
    "\n",
    "## Import data\n",
    "Importing data from a CSV file. It is important to set the following parameters:\n",
    "\n",
    "encoding: usually 'utf-8' or windows-xxxx on Windows machines, where xxxx is 1250 for Central Europe, 1251 for Cyrilic etc.\n",
    "sep: separator of columns in the file\n",
    "decimal: decimal dot or coma\n",
    "index_col: which columns is used as index - should be the unique credit case identifier\n",
    "\n",
    "**Defining NA values:** In different datasets, there can be different values to be considered *N/A*. By default, we set only blank fields to be considered *N/A*, however you might want to change it and add values like *'NA'*, *'NAN'*, *'null'* to be also considered *N/A*. User parameter `na_values` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring import db\n",
    "data = db.read_csv(r'demo_data\\ExampleData8.CSV', sep = ',', decimal = '.',\n",
    "                   optimize_types=True, encoding = 'utf-8', index_col = 'ID', low_memory = False,\n",
    "                   keep_default_na = False, na_values = [''])\n",
    "print('Data loaded on',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data need to have **index column which has unique value per each row**. If not, it can cause some problems later. Run this to deal with such rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 1: remove rows with duplicated index\n",
    "# data=data[~data.index.duplicated(keep='first')]\n",
    "\n",
    "# #Option 2: reset index\n",
    "# data['INDEX_ORIGINAL'] = data.index\n",
    "# data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally the data can be loaded also from a database. The function read_sql uses cache, so the data don't have to be downloaded from the database repeatedly. The cache will be located in a new folder called **db_cache**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# from sqlalchemy import create_engine\n",
    "#engine = create_engine('oracle://PAVELS[GP_HQ_RISK]:xxx@(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=DBDWHRU.HOMECREDIT.RU)(PORT=1521))(CONNECT_DATA=(SERVICE_NAME=DWHRU)))', echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scoring.db import read_sql\n",
    "#ru_data = read_sql('select * from owner_dwh.f_application_tt where rownum<11',engine, index_col = 'sk_application')\n",
    "#print('Data loaded on',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to download data from the database again (and not from cache), use the parameter refresh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scoring.db import read_sql\n",
    "#data = read_sql('select * from owner_dwh.f_application_base_tt where rownum=1',engine, index_col = 'skp_application',refresh=True)\n",
    "#print('Data loaded on',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows:',data.shape[0])\n",
    "print('Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata definitions\n",
    "Assigning ID column, target column, time column and month column. The month column don't have to exist in the dataset, it will be created later in this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THESE COLUMNS MUST BE INCLUDED IN THE DATA SET ###\n",
    "#name of the target column\n",
    "col_target = \"DEF\"\n",
    "#name of the time column\n",
    "col_time = \"TIME\"\n",
    "\n",
    "### THESE COLUMNS DON'T HAVE TO BE INCLUDED IN THE DATA SET AND ARE CREATED AUTOMATICALLY LATER ###\n",
    "#name of the base column\n",
    "col_base = \"BASE\"\n",
    "#name of the month column\n",
    "col_month = \"MONTH\"\n",
    "#name of the day column\n",
    "col_day = \"DAY\"\n",
    "#name of the weight column - CURRENTLY COMMENTED OUT BECAUSE OF REASONS MENTIONED LATER\n",
    "col_weight = 'WEIGHT'\n",
    "#name of the reject column - only use if exists in your data, used for reject inference analysis\n",
    "col_reject = 'REJECTED'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a list of targets with their respective bases, time variable and rowid variable for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation.targets = [(col_target, col_base)]\n",
    "documentation.time_variable = col_month\n",
    "documentation.rowid_variable = \"ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records([['col_time',col_time],['col_month',col_month],['col_day',col_day],['col_target',col_target],['col_base',col_base]]) \\\n",
    ".to_csv(output_folder+'/model/metadata.csv',index=0,header=None)\n",
    "\n",
    "data[col_target] = data[col_target].astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have base column in your data set, the following code adds it (based on if target is filled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if col_base not in data:\n",
    "    data[col_base] = 0\n",
    "    data.loc[data[col_target]==0,col_base] = 1\n",
    "    data.loc[data[col_target]==1,col_base] = 1\n",
    "    print('Column',col_base,'added/modified. Number of columns:',data.shape[1])\n",
    "else:\n",
    "    print('Column',col_base,'already exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have weight column in your data set, the following code adds it, with value = 1 for each row. **The weights are currently supported by Data Exploration, Interactive Grouping and Model Selection (L1 regression and Stepwise) classes, but not by all functions in the workflow. This is why they are commented out now (can be uncommented by the user).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if col_weight not in data:\n",
    "#    data[col_weight] = 1\n",
    "#    print('Column',col_weight,'added/modified. Number of columns:',data.shape[1])\n",
    "# else:\n",
    "#    print('Column',col_weight,'already exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the month and day column from the time column is doing the following\n",
    "- take the time column and tell in which format the time is saved in - **you need to specify this in variable *dtime_input_format*** (see https://docs.python.org/3/library/time.html#time.strftime for reference)\n",
    "- strip the format just to year, month, day string\n",
    "- convert the string to number\n",
    "- the new column will be added to the dataset as day\n",
    "- truncate this column to just year and month and add it to dataset as month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtime_input_format = '%Y-%m-%d %H:%M:%S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[col_day] = pd.to_numeric(pd.to_datetime(data[col_time], format=dtime_input_format).dt.strftime('%Y%m%d'))\n",
    "data[col_month] = data[col_day].apply(lambda x: math.trunc(x/100))\n",
    "print('Columns',col_day,'and',col_month,'added/modified. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the predictors list from a csv file. The csv should have just one column, without any header, containing the name of the variables that should be used as predictors.\n",
    "\n",
    "Support for boolean predictors in **not** currently implemented. Convert boolean predictors to object to use them.\n",
    "\n",
    "`s = s.apply(lambda value: str(value) if not np.isnan(value) else value).astype('object')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.data_manipulation import split_predictors_bytype\n",
    "\n",
    "cols_pred = list(pd.read_csv(r'demo_data\\ExamplePredList8.CSV', sep = ',', decimal = '.', \n",
    "                   encoding = 'windows-1251', low_memory = False, header = None)[0])\n",
    "\n",
    "cols_pred, cols_pred_num, cols_pred_cat = split_predictors_bytype(data,\n",
    "                                                                  pred_list=cols_pred,\n",
    "                                                                  non_pred_list= [],\n",
    "                                                                  optimize_types=True,\n",
    "                                                                  convert_bool2int=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentation.predictors_continuous = cols_pred_num\n",
    "# documentation.predictors_grouped = cols_pred_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please check if all predictors were categorized correctly.** \n",
    "`Category` dtype is now used for categorical columns for memory efficiency. This means it will not be editable as *string*. If you need to edit values of a categorical column convert it to *string* using this syntax:\n",
    "\n",
    "`data['Column name'] = data['Column name'].astype(str)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descrip = data.describe(include='all').transpose()\n",
    "pd.options.display.max_rows = 1000\n",
    "display(descrip)\n",
    "pd.options.display.max_rows = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**explore_numerical** and **explore_categorical** functions give graphical data exploratory analyses. They can also output into html files. You just need to specify the folder for output.\n",
    "\n",
    "If you want the detailed legacy HTML output (v0.4.3) comment-out the cell below\n",
    "\n",
    "These functions analyze only the part of data where target is not null even if it is not explicitly specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.data_exploration import explore_categorical, explore_numerical, join_explorations\n",
    "\n",
    "explored_columns = list()\n",
    "for name, column in tqdm(data[cols_pred].iteritems(), total=len(cols_pred), leave=False):\n",
    "    if name in cols_pred_num[:]:\n",
    "        if (column.count() > 0) and (column.max() != column.min()):\n",
    "            explore_numerical(column, data[col_target], weightCol=None, htmlOut=True, ntbOut=False, outFolder='exp')\n",
    "            explored_columns.append(name)\n",
    "    if name in cols_pred_cat[:]:\n",
    "        if (column.count() > 0) and (len(set(column.unique()) - {np.nan}) > 1):\n",
    "            explore_categorical(column, data[col_target], weightCol=None, htmlOut=True, ntbOut=False, outFolder='exp')\n",
    "            explored_columns.append(name)\n",
    "            \n",
    "#comment out the line below if you didn't generate html files in this cell        \n",
    "join_explorations(explored_columns, filename = '_exploration.html', outFolder='exp', weighted=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**explore_df** function creates a simple text report about the important variable. The report can be then printed either to the screen or to a file.\n",
    "\n",
    "In the following code, only such part of data that has col_base = 1 is analyzed. You can remove the condition if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.data_exploration import explore_df\n",
    "st = explore_df(data[data[col_base]==1],col_month,col_target,cols_pred)\n",
    "print(st, file=open(\"data_exp.txt\", \"w\", encoding='utf-8'))\n",
    "# print(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default rate in time**: Simple visualisation of observation count and default rate in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import plot_dataset\n",
    "plot_dataset(data,\n",
    "             month_col=col_month,\n",
    "             def_col=col_target,\n",
    "             title='Count and bad rate',\n",
    "             base_col=col_base,\n",
    "             #weightCol=col_weight,\n",
    "             savepath=output_folder+'/analysis/',\n",
    "             zeroYlim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NaN share by month** for each variable in dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.data_exploration import nan_share_development\n",
    "\n",
    "nan_table = nan_share_development(data[cols_pred + [col_month]], col_month, \n",
    "                                  make_images=True, show_images=True, output_path=output_folder+'/nan_share/')\n",
    "display(nan_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "\n",
    "- Split data into five parts (in time training, in time validation, in time test, out of time, historical out of time)\n",
    "- Adds a new column indicating to which part the observations belong\n",
    "- The *splitting_points* (first date of train and first date of out of time sample) can be adjusted (there can be any number of such splitting points) - it should correspond to values of column specified by *time_column* parameter\n",
    "- For each time split, you can create multiple random splits (i.e. train/valid/test), the ratio of sizes of these splits is set by parameter *sample_sizes*\n",
    "- The random splits can be stratified by multiple variables, which are specified in a list - argument to *stratify_by_columns* parameter\n",
    "- Set the random seed so the results are replicable\n",
    "\n",
    "**Before you run data split, make sure that index in your dataset in unique!** If not, you need to create new unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #saving original index to a new column and resetting it\n",
    "# data['INDEX_ORIGINAL'] = data.index\n",
    "# data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from scoring.data_manipulation import data_sample_time_split\n",
    "\n",
    "data['data_type'] = data_sample_time_split(data, \n",
    "                           time_column = col_month,\n",
    "                           splitting_points = [201702, 201707],\n",
    "                           sample_sizes = [[ 1    ],[ 0.4   , 0.3   , 0.3  ],[ 1   ]],\n",
    "                           sample_names = [['hoot'],['train','valid','test'],['oot']],\n",
    "                           stratify_by_columns = [col_month,col_target],\n",
    "                           random_seed = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masks: boolean vectors corresponding to rows in the datasets. True if an row is observable and its data type belongs to given sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = (data['data_type'] == 'train') & (data[col_base] == 1) \n",
    "valid_mask = (data['data_type'] == 'valid') & (data[col_base] == 1) \n",
    "test_mask = (data['data_type'] == 'test') & (data[col_base] == 1) \n",
    "oot_mask = (data['data_type'] == 'oot') & (data[col_base] == 1) \n",
    "hoot_mask = (data['data_type'] == 'hoot') & (data[col_base] == 1)\n",
    "observable_mask = data[col_base] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add masks to _documentation_ object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation.sample_dict = {\n",
    "    \"HOOT\": hoot_mask,\n",
    "    \"Train\": train_mask,\n",
    "    \"Valid\": valid_mask,\n",
    "    \"Test\":test_mask,\n",
    "    \"OOT\": oot_mask,\n",
    "    \"Observable\": observable_mask\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data summary (number of defaults, number in base, number of observations, default rate) by month and by sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary = data.groupby([col_month,'data_type']).aggregate({\n",
    "    col_target:'sum',col_base:['sum','count']\n",
    "})\n",
    "data_summary.columns = [col_target,col_base,'Rows']\n",
    "data_summary[col_target+' rate'] = data_summary[col_target]/data_summary[col_base]\n",
    "display(data_summary)\n",
    "\n",
    "data_summary = data_summary.reset_index(level='data_type').pivot(columns='data_type')\n",
    "display(data_summary)\n",
    "data_summary.to_csv(output_folder+'/analysis/summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derived variables\n",
    "\n",
    "## Date differences\n",
    "\n",
    "Function *datetime_difference()* takes name of dataset and names of two variables with datetime strings (the format of those strings can be also specified in parameters) and caluclates their difference in specified time units. We then append names of these difference columns to the list of numerical predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_datediff = []\n",
    "\n",
    "from scoring.tools import datetime_difference\n",
    "\n",
    "cols_datediff.append(datetime_difference(data, \n",
    "                                         variable1 = col_time, \n",
    "                                         variable2 = \"DateVariable_1\",\n",
    "                                         format1 = \"%Y-%m-%d %H:%M:%S\",\n",
    "                                         format2 = \"%Y-%m-%d %H:%M:%S\",\n",
    "                                         unit = \"years\",\n",
    "                                         rounding = \"none\",\n",
    "                                         inplace = True,\n",
    "                                         new_variable_name = \"AGE\",\n",
    "                                         proxy_1970_fix_years = 0))\n",
    "cols_datediff.append(datetime_difference(data, \n",
    "                                         variable1 = col_time,\n",
    "                                         variable2 = \"DateVariable_2\",\n",
    "                                         format1 = \"%Y-%m-%d %H:%M:%S\",\n",
    "                                         format2 = \"%Y-%m-%d %H:%M:%S\",\n",
    "                                         unit = \"months\",\n",
    "                                         rounding = \"floor\",\n",
    "                                         inplace = True,\n",
    "                                         new_variable_name = None,\n",
    "                                         proxy_1970_fix_years = None))\n",
    "\n",
    "cols_pred_num = list(set(cols_pred_num) | set(cols_datediff))\n",
    "cols_pred = cols_pred_num + cols_pred_cat\n",
    "print('Columns',cols_datediff,'added to predictor list.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive interactions (alpha)\n",
    "**This is an alpha version meaning that we are aware there are some bugs and problems that need to be solved. If you decide to use this tool anyway, please inform us about any issues you find.**\n",
    "\n",
    "**KNOWN ISSUES:**\n",
    " - The interaction potential calculation works for numerical variables only. So you can't calculate interaction potential of two categorical variables or of a categorical and numerical variable.\n",
    " - The `tkinter` GUI does not work when you run it on a remote server. It always opens on the computer it is run on, so if you use only web-based Jupyter, GUI will not open. \n",
    "\n",
    "In this part we can observe two variables together and make their interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_INTERACTIONS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_INTERACTIONS:\n",
    "    from scoring.interactions import Interactions\n",
    "    INTERACTION_FOLDER = './interactions'\n",
    "    if not os.path.exists(INTERACTION_FOLDER):\n",
    "        os.makedirs(INTERACTION_FOLDER)\n",
    "\n",
    "    configuration = dict(\n",
    "        common=dict(\n",
    "            work_dir='',\n",
    "            log_filename='report',\n",
    "            log_level=40, # 10 DEBUG, 20 INFO, 30 WARNING, 40 ERROR, 50 CRITICAL\n",
    "            potential_output_filename='interaction_potential',\n",
    "        ),\n",
    "        data_specification=dict(\n",
    "            target_col=col_target,\n",
    "            numerical_cols=cols_pred_num,\n",
    "            categorical_cols=cols_pred_cat,        \n",
    "        ),\n",
    "        interactions=dict(\n",
    "            metadata_file='metadata',\n",
    "            sql_file_prefix='sql_'\n",
    "        ),\n",
    "        gui=dict(\n",
    "            font='sans-serif'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ints = Interactions(config_dict=configuration)\n",
    "    ints.interaction_potential(data[train_mask])\n",
    "    inter_pot = pd.read_table(os.path.join(configuration['common']['work_dir'], \n",
    "                                        configuration['common']['potential_output_filename'] + '.txt'), sep = ';')\n",
    "    inter_pot.sort_values('p_val_min', ascending = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interaction potential is calculated using t-test of interaction (product of two numerical predictors) in a model. This means that we can't calculate interaction potential for categorical variables although they might be useful in interactions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_INTERACTIONS:\n",
    "    inter_pot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can fit the interaction bins. We create metadata first using `.fit` method. We can also create multiple interactions first, which the will be applied to the dataset using `.transform` - all at the same time.\n",
    "\n",
    "This tool needs **tkinter** library to be installed.\n",
    "\n",
    "Check the background, whether there is open window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_INTERACTIONS:\n",
    "    ints.fit(row_var=\"Numerical_2\", column_var=\"Categorical_3\", data=data[train_mask], rewrite=True)\n",
    "    ints.fit(row_var=\"Numerical_2\", column_var=\"Numerical_3\", data=data[train_mask], interaction_variable_name='interaction_Numerical_2_Numerical_3', rewrite=True)\n",
    "    ints.fit(row_var=\"Categorical_2\", column_var=\"Categorical_5\", data=data[train_mask], rewrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_INTERACTIONS:\n",
    "    ints_metadata = ints._metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we fitted the interactions before and don't want to run the fit again, we can alternatively load them from output file, which can be done by the following code. Note that the name of the file should be specified above in the configuration when the instance of Interaction class is created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_INTERACTIONS:\n",
    "    import json\n",
    "    with open(os.path.join(configuration['common']['work_dir'], \n",
    "                           configuration['interactions']['metadata_file'] + '.json')) as f:\n",
    "        ints_metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform the original data and add the interaction column into list of numerical predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_INTERACTIONS:\n",
    "    data = ints.transform(data=data, metadata=ints_metadata)\n",
    "\n",
    "    cols_interactions = ints.list_interaction_names(metadata=ints_metadata)\n",
    "\n",
    "    cols_pred_num = list(set(cols_pred_num) | set(cols_interactions))\n",
    "    cols_pred = cols_pred_num + cols_pred_cat\n",
    "    print('Columns',cols_interactions,'added to data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate SQL code to replicate these interactions in Oracle DWH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_INTERACTIONS:\n",
    "    ints.create_sql_from_metadata(print_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping and WOE transformation of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't use such variables which have only 0 or 1 unique levels. Grouping doesn't work for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_del = list()\n",
    "for name, column in tqdm(data[train_mask][cols_pred].iteritems(), total=len(cols_pred)):\n",
    "    if name in cols_pred_num:\n",
    "        if (column.count() == 0) or (column.max() == column.min()):\n",
    "            cols_del.append(name)\n",
    "            cols_pred_num.remove(name)\n",
    "    if name in cols_pred_cat:\n",
    "        if (column.count() == 0) or (len(set(column.unique()) - {np.nan}) <= 1):\n",
    "            cols_del.append(name)\n",
    "            cols_pred_cat.remove(name)\n",
    "            \n",
    "cols_pred = cols_pred_num + cols_pred_cat\n",
    "\n",
    "if len(cols_del) > 0:\n",
    "    print('Variables', cols_del, 'will not be further used as they have only 1 unique level.')\n",
    "else:\n",
    "    print('All predictors have more than 1 unique level.')\n",
    "del cols_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, column in tqdm(data[cols_pred_num].iteritems(), total=len(cols_pred_num), leave=False):\n",
    "    if np.any(np.isinf(column.values)):\n",
    "            print('{} containes INF values. Please deal with them.'.format(name))\n",
    "\n",
    "# data['predictor'].replace(to_replace=np.inf, value=<good idea>, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two options how to group your variables. \n",
    "1. Automatic grouping groups the variables using a decision tree. User can't change the grouping in any interactive way. The grouping can be saved into external file using its method *save()*. \n",
    "2. Interactive grouping is suitable for smaller numbers of variables. User can control which values of each varible will enter which group. The grouping can be saved into external file using the interactive environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Grouping\n",
    "The grouping uses decision tree algorithm and the grouping is supervised based on the target variable. In the following code:\n",
    "\n",
    "A new instance of **Grouping** class is created. There are two important parameters:\n",
    " - *colums*: list of numerical columns to be grouped\n",
    " - *cat_columns*: list of categorical columns to be grouped\n",
    " - *group_count*: (maximal) number of final groups of each variable\n",
    " - *min_samples*: minimal number of observations in each group of each numerical variable\n",
    " - *min_samples_cat*: minimal number of observations in each group of each categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.grouping import Grouping\n",
    "\n",
    "grouping = Grouping(columns = sorted(cols_pred_num),\n",
    "                    cat_columns = sorted(cols_pred_cat),\n",
    "                    group_count=5, \n",
    "                    min_samples=100, \n",
    "                    min_samples_cat=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping.fit(data[train_mask][cols_pred],\n",
    "             data[train_mask][col_target],\n",
    "          #   w=data[train_mask][col_weight],\n",
    "            progress_bar=True, category_limit=10000);\n",
    "\n",
    "# if len(grouping.bins_data_) > 0:\n",
    "#     for v,g in grouping.bins_data_.items():\n",
    "#         print('Variable:',v)\n",
    "#         print('Bins:',g['bins'])\n",
    "#         print('WOEs:',g['woes'])\n",
    "#         if v in cols_pred_num:\n",
    "#             print('nan WOE:',g['nan_woe'])\n",
    "#         if v in cols_pred_cat:\n",
    "#             print('WOE for unknown values:',g['unknown_woe'])\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save grouping to an external file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'myGrouping'\n",
    "grouping.save(model_filename)\n",
    "# print('Grouping data saved to file',model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Grouping (beta)\n",
    "**This is a beta version which means it should work well in most cases but sometimes it can be unstable. Please inform us about any issues you find**\n",
    "\n",
    "**KNOWN ISSUES:**\n",
    "- If zoom level in the web browser is set to something else than 100%, the charts might get broken.\n",
    "\n",
    "A new instance of **InteractiveGrouping** class is created. There are two important parameters:\n",
    " - *colums*: list of numerical columns to be grouped\n",
    " - *cat_columns*: list of categorical columns to be grouped\n",
    " - *group_count*: (maximal) number of final groups of each variable\n",
    " - *min_samples*: minimal number of observations in each group of each numerical variable\n",
    " - *min_samples_cat*: minimal number of observations in each group of each categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_INTERATIVE_GROUPING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if USE_INTERATIVE_GROUPING:\n",
    "    from scoring.grouping import Grouping, InteractiveGrouping\n",
    "\n",
    "    grouping = InteractiveGrouping(columns = sorted(cols_pred_num),\n",
    "                                   cat_columns = sorted(cols_pred_cat),\n",
    "                                   group_count=5,\n",
    "                                   min_samples=100, \n",
    "                                   min_samples_cat=100,\n",
    "                                   woe_smooth_coef=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you open the interactive environment using **display** method. The important parameters are:\n",
    " - *train_t*: training dataset the grouping should be based on\n",
    " - *colums*: list of numerical columns to be grouped and displayed\n",
    " - *cat_columns*: list of categorical columns to be grouped and displayed\n",
    " - *target_column*: as the grouping is supervised and calculates WOE values, you need to specify the target column name\n",
    " - *w_column*: vector of weights of obervation (if not filled, grouping behaves as there are equal weights)\n",
    " - *filename*: use only if you want to load a grouping that you created and saved previously\n",
    " - *group_count*: (maximal) number of final groups of each variable\n",
    " - *min_samples*: minimal number of observations in each group of each numerical variable\n",
    " - *min_samples_cat*: minimal number of observations in each group of each categorical variable\n",
    "\n",
    "In the interactive environment, you can see four sections. From top to bottom:\n",
    "- **Chart section**: \n",
    " - For **numerical variables**, there is chart with equifrequncy fine classing (observations as bars, default rate as line), equidistant fine classing and the final groups.\n",
    " - For **categorical varibles** there is chart with each of the original categorical values and a chart with the final groups.\n",
    "- **Variable section**: here you can choose tab with varible which you want to edit. \n",
    " - For **numerical variables**, the tab contains of the borders of the final groups. You can edit these borders, add new with [+] button and remove them with [-] button. You can also manually set WOE for nulls. There is also a button to perform automatic grouping on the selected variable.\n",
    " - For **categorical variables**, the tab contains of two tables. In the top table, you can see some statistics for each of the categorical values. In the rightmost column, there is the number of group which is assigned to the category. You can edit this value (doubleclick on it) to change the grouping. In the bottom table you can see statistics for the groups. It is not editable. There is also a button to perform automatic grouping on the selected variable.\n",
    "- **Save section**: here you can save the grouping. Edit the file name and click the [Apply and Save] button.\n",
    "- **Settings section**: If you perform automatic grouping on some varible, the grouping algorithm uses some parameters. These parameters can be set here. You can set how many final groups do you want to have and what is their minimal size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_INTERATIVE_GROUPING:\n",
    "    sns.reset_orig()\n",
    "    %matplotlib notebook\n",
    "    %config InlineBackend.close_figures=False\n",
    "\n",
    "    grouping.display(train_t = data[train_mask][cols_pred_num+cols_pred_cat+[col_target]],\n",
    "                     #train_t = data[train_mask][cols_pred_num+cols_pred_cat+[col_target]+[col_weight]], #for call with weight\n",
    "                     columns = sorted(cols_pred_num),\n",
    "                     cat_columns = sorted(cols_pred_cat),\n",
    "                     target_column = col_target,\n",
    "                     #w_column = col_weight,\n",
    "    #                  filename = 'myIntGrouping',\n",
    "                     bin_count=20,\n",
    "                     woe_smooth_coef=0.001,\n",
    "                     group_count=5,\n",
    "                     min_samples=100,\n",
    "                     min_samples_cat=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to *Apply and Save* your changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the graphical environment to be used by the normal non-interactive charts\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.close_figures=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the grouping from a file (don't forget to set the right filename) and add the WOE columns to the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scoring.grouping import Grouping\n",
    "# grouping = Grouping(columns = sorted(cols_pred_num),\n",
    "#                     cat_columns = sorted(cols_pred_cat),\n",
    "#                     group_count=5, \n",
    "#                     min_samples=100, \n",
    "#                     min_samples_cat=100) \n",
    "# g_filename = 'myIntGrouping'\n",
    "# grouping.load(g_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the grouping to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WOE transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to apply the grouping to the data. *Grouping.transform()* method now automatically renames columns with proper suffix. If you need to transform just subset of columns use parameter *columns_to_transform=\\[...\\]*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_woe = grouping.transform(data, transform_to='woe', progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy transformation\n",
    "\n",
    "**Optional:** Transformation to dummy variables. Use if you want to use \"full\" regression instead of WOE regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummy = grouping.transform(data, transform_to='dummy', progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables transformation\n",
    "\n",
    "**Optional:** Transformation to categorical variables. Name of categories instead of WOE values - not useful for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_shortnames = grouping.transform(data, transform_to='shortnames', progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fitted WOEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation.GroupingPlots(\n",
    "    data,\n",
    "    predictors=cols_pred,\n",
    "    sample=\"Train\",\n",
    "    target=\"DEF\",\n",
    "    grouping=grouping,\n",
    "    output_folder=\"documentation\",\n",
    "    weight=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the transformed variables to the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WOE variables\n",
    "\n",
    "Add WOE variabes to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woe_columns_to_replace = list()\n",
    "for column in data_woe.columns:\n",
    "    if column in data:\n",
    "        woe_columns_to_replace.append(column)\n",
    "        print('Column', column ,'dropped as it already existed in the data set.')\n",
    "data = data.drop(woe_columns_to_replace, axis='columns')\n",
    "data = data.join(data_woe)\n",
    "\n",
    "del data_woe\n",
    "gc.collect()\n",
    "\n",
    "print('Added WOE variables. Number of columns:',data.shape[1])\n",
    "cols_woe = [s + '_WOE' for s in cols_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy variables\n",
    "\n",
    "**Optional.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_columns_to_replace = list()\n",
    "for column in data_dummy.columns:\n",
    "    if column in data:\n",
    "        dummy_columns_to_replace.append(column)\n",
    "        print('Column', column ,'dropped as it already existed in the data set.')\n",
    "data = data.drop(dummy_columns_to_replace, axis='columns')\n",
    "data = data.join(data_dummy)\n",
    "\n",
    "del data_dummy\n",
    "gc.collect()\n",
    "\n",
    "print('Added dummy variables. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Feature selection\n",
    "\n",
    "First remove WOE variables with one WOE value only - they will have no predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_del = list()\n",
    "for name, column in data[train_mask][cols_woe].iteritems():\n",
    "    if (column.count() == 0) or (column.max() == column.min()):\n",
    "        cols_del.append(name)\n",
    "        cols_woe.remove(name)\n",
    "\n",
    "if len(cols_del) > 0:\n",
    "    print('Variables',cols_del,'will not be further used as they have only 1 unique WOE level.')\n",
    "else:\n",
    "    print('All predictors have more than 1 unique WOE level.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor power analysis\n",
    "\n",
    "Calculates IV and Gini of each predictor, sorts the predictors by their power. The power is calculated for each of the samples (train, validate, test, OOT, H.OOT). **If one or more of the samples are empty, comment the according part of the code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.metrics import iv,gini,lift\n",
    "\n",
    "power_tab = []\n",
    "for j in range(0,len(cols_woe)):\n",
    "    power_tab.append({'Name':cols_woe[j]\n",
    "                    ,'IV Train':iv(data.loc[train_mask,col_target],data.loc[train_mask,cols_woe[j]])\n",
    "                    ,'Gini Train':gini(data.loc[train_mask,col_target],-data.loc[train_mask,cols_woe[j]])\n",
    "                    ,'IV Validate':iv(data.loc[valid_mask,col_target],data.loc[valid_mask,cols_woe[j]])\n",
    "                    ,'Gini Validate':gini(data.loc[valid_mask,col_target],-data.loc[valid_mask,cols_woe[j]])\n",
    "                    ,'IV Test':iv(data.loc[test_mask,col_target],data.loc[test_mask,cols_woe[j]])\n",
    "                    ,'Gini Test':gini(data.loc[test_mask,col_target],-data.loc[test_mask,cols_woe[j]])\n",
    "                    ,'IV OOT':iv(data.loc[oot_mask,col_target],data.loc[oot_mask,cols_woe[j]])\n",
    "                    ,'Gini OOT':gini(data.loc[oot_mask,col_target],-data.loc[oot_mask,cols_woe[j]])\n",
    "                    ,'IV HOOT':iv(data.loc[hoot_mask,col_target],data.loc[hoot_mask,cols_woe[j]])\n",
    "                    ,'Gini HOOT':gini(data.loc[hoot_mask,col_target],-data.loc[hoot_mask,cols_woe[j]])\n",
    "                         })\n",
    "power_out = pd.DataFrame.from_records(power_tab)\n",
    "power_out = power_out.set_index('Name')\n",
    "power_out = power_out.sort_values('Gini Train',ascending=False)\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "display(power_out)\n",
    "pd.options.display.max_rows = 15\n",
    "power_out.to_csv(output_folder+'/predictors/covariates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show correlation matrix of all the WOE variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation.Correlations(data,\n",
    "                           predictors=cols_woe,\n",
    "                           sample=\"All\",\n",
    "                           output_folder=output_folder+\"/analysis/\",\n",
    "                           filename=\"correlation_full.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical variable clustering.\n",
    "- Starts with each variable as a separate cluster\n",
    "- Creates clusters based on highest average correlations\n",
    "- The stopping criterion is either parameter *max_cluster_correlation* - once no correlation between clusters is larger than this parameter, the clustering is finished; or *max_clusters* - when this many clusters are created, the clustering is finished. If both specified, the one that makes less clusters is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring import variable_clustering\n",
    "\n",
    "clustering_correlation = variable_clustering.CorrVarClus(max_correlation=0.5,\n",
    "                                                         #max_clusters=9,\n",
    "                                                         standardize=True,\n",
    "                                                         sample_size=50000)\n",
    "\n",
    "clustering_correlation.fit(data[train_mask][cols_woe], data[train_mask][col_target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dendrogram (tree scheme of the hierarchical clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_correlation.draw(output_file=output_folder + '/analysis/clustering_dendrogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of predictors with number of cluster they are in and order in their respective clusters ranked by gini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_correlation.display(output_folder+'/predictors/predictor_clusters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset of WOE variables created that contains only the strongest (based on **training Gini**) variable of each cluster. It's up to the user to choose whether they want to used the full set (in this workflow by default) or such restricted set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best variables based on correlation clustering:')\n",
    "clustering_correlation.bestVariables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two other options for clustering in the scoring library:\n",
    "\n",
    " - Feature agglomeration (`variable_clustering.FeatureAggVarClus`) which is very similar to correlation. It uses cosine distance (which is closely related to correlation) and you can also specify usage of nearest neighbors connectivity constraints there.\n",
    " - k-means with PCA (`variable_clustering.KMeansVarClus`) which uses Euclidean distance between the observations to find the variable clusters. However, Euclidean distance suffers from the curse of dimensionality in high-dimensional spaces, so PCA is applied to the data first. Anyway, this method is not recommended as the dimensionality problem is not solved fully and the results of the previous two methods seems to be much more usable.\n",
    " \n",
    "For reference for these methods, see the classes in scoring library themself (there are docstrings there)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 regularized Logistic Regression\n",
    "\n",
    "Efficient way how to select subset of predictors from a very big set of covariate. Uses grid search through value of L1 regularization parameter. We start with no predictor in the model and try to add predictors from list called **cols_shortlist** which is defined below (by default, we put there all the WOE variables). The best model selected based on validation Gini.\n",
    "\n",
    "Interation process can be tuned using various parameters:\n",
    " - *steps*: number of steps of grid search\n",
    " - *grid_length*: length of the grid for grid search\n",
    " - *log_C_init*: initial value of log10 of C parameter for grid search\n",
    " - *max_predictors*: maximal number of predictors to enter the model. Ignored if set to 0.\n",
    " - *max_correlation*: maximal absolute value of correlation of predictors in the model (variable with larger correlation with existing predictors will not be added to the model)\n",
    " - *beta_sgn_criterion*: if this is set to True, all the betas in the model must have the same signature (all positive or all negative)\n",
    " - *stop_immediately*: the iteration process will be stopped immediately after a model which is not fulfilling the criteria (max_predictors, max_correlation or beta_sgn_criterion) is found. No further models are searched for.\n",
    " - *stop_when_decr*: the iteration process will be stopped immediately when the validation gini decreases in comparison with the validation gini of previous step. No further models are searched for.\n",
    " - *correlation_sample*: for better performance, correlation matrix is calculated just on a sample of data. The size of the sample is set in this parameter\n",
    " - *use_cv*: boolean if Cross Validation should be used instead of train/validation split. In this case, if both training and validation samples are presented to the fit method, they are concatenated together and are used for CV. Gini is evaluated as average of all CV's folds' validation Gini. **Please, be aware that using CV after automatic grouping which was trained using train/validate split might lead to overfitted model.**\n",
    " - *cv_folds*: parameter for Cross Validation - number of folds\n",
    " - *cv_seed*: parameter for Cross Validation - random seed used to split the folds\n",
    " \n",
    "The *fit* method can be called with two arguments *fit(X,y)* or with four agruments *fit(X_train,y_train,X_valid,y_valid)*. When called with four arguments, the Gini is measured on the validation sample (i.e. validation sample is used for decisions about what steps to be done in stepwise).\n",
    "\n",
    "There are another optional arguments, *sample_weight* and *sample_weight_valid* where you can put the vector (data set column) with weights of the observations for the train and validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a shortlist of predictors to enter the modelling in the next steps.\n",
    "cols_shortlist = cols_woe\n",
    "#cols_shortlist = list(set(cols_woe) - set(['unwanted1','unwanted2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.model_selection import L1GiniModelSelection\n",
    "\n",
    "modelL1 = L1GiniModelSelection(steps = 100,\n",
    "                               grid_length=5,\n",
    "                               log_C_init=None,\n",
    "                               max_predictors=200,\n",
    "                               max_correlation=1,\n",
    "                               beta_sgn_criterion=False,\n",
    "                               stop_immediately=False,\n",
    "                               stop_when_decr=False,\n",
    "                               correlation_sample = 10000,\n",
    "                               penalty='l1',\n",
    "                               use_cv=False,\n",
    "                               cv_folds=5,\n",
    "                               cv_seed=98765)\n",
    "\n",
    "modelL1.fit(data[train_mask][cols_shortlist],data[train_mask][col_target],\n",
    "            data[valid_mask][cols_shortlist],data[valid_mask][col_target],\n",
    "#             sample_weight = data[train_mask][col_weight], sample_weight_valid = data[valid_mask][col_weight],\n",
    "            progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelL1.draw_coeff_progression(cols_shortlist, output_folder+'/model/l1path.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelL1.draw_gini_progression(output_folder+'/model/l1gini.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predictors in the model:',list(modelL1.final_predictors_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename1 = 'myModelL1'\n",
    "pickle.dump(modelL1, open(model_filename1, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_filename1 = 'myModelL1'\n",
    "#modelL1 = pickle.load(open(model_filename1, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drawback of regularized model is that it is not calibrated, so it must be refitted afterwards. In this workflow, there is stepwise regression after this L1 regression which can serve this purpose (i.e. fitting model with the same set or subset of predictors, but without the regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection using xgBoost variable importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needs xgboost library to be installed. Otherwise, you might want to skip these xgboost related steps.\n",
    "\n",
    "First we train a gradient boosting model using a \"standard\" set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.metrics import gini\n",
    "\n",
    "USE_XGBOOST = True\n",
    "\n",
    "if USE_XGBOOST:\n",
    "    import xgboost as xgb\n",
    "\n",
    "    dt_xgb = data[cols_pred_num + [c + '_WOE' for c in cols_pred_cat]]\n",
    "\n",
    "    xgb_params = {'eta': 0.1,\n",
    "      'max_depth': 3,\n",
    "      'objective': 'binary:logistic',\n",
    "      'eval_metric': 'auc',\n",
    "      'min_child_weight': 30,\n",
    "      'subsample': 0.85}\n",
    "\n",
    "    evals_result = {}\n",
    "\n",
    "    booster = xgb.train(params = xgb_params,\n",
    "                            dtrain = xgb.DMatrix(dt_xgb[train_mask],data[train_mask][col_target]),\n",
    "                            num_boost_round = 500,\n",
    "                            early_stopping_rounds = 50,\n",
    "                            evals = ((xgb.DMatrix(dt_xgb[train_mask],data[train_mask][col_target]),'train'),\n",
    "                                     (xgb.DMatrix(dt_xgb[valid_mask],data[valid_mask][col_target]),'valid')\n",
    "                                    ), \n",
    "                            evals_result = evals_result,)\n",
    "\n",
    "\n",
    "    xgb_scored = booster.predict(xgb.DMatrix(dt_xgb), ntree_limit=booster.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_XGBOOST:\n",
    "    print('     Train gini:',gini(data[train_mask][col_target], xgb_scored[train_mask]))\n",
    "    print('Validation gini:',gini(data[valid_mask][col_target], xgb_scored[valid_mask]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight of each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_XGBOOST:\n",
    "    %matplotlib inline\n",
    "    from matplotlib import rc\n",
    "\n",
    "    fs = booster.get_score(importance_type = 'weight')\n",
    "    imp=sorted([(k, v) for k, v in fs.items()], key=lambda x:x[1], reverse = True)\n",
    "    imp.reverse()\n",
    "\n",
    "    fig=plt.figure(figsize=(25,20))\n",
    "    ax=fig.add_subplot(111)\n",
    "    ax.barh(range(len(imp)), [v for k, v in imp], color=\"blue\",  align='center')\n",
    "    plt.yticks(range(len(imp)), [k for k, v in imp], fontsize=15)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.xlabel('Importance',fontsize=15)\n",
    "    plt.ylim([-1, len(imp)])\n",
    "    plt.xlim([0, max([v for k, v in imp])*1.2])\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select *n_top* variables with highest weight (i.e. those which were in most trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_XGBOOST:\n",
    "    n_top = 30 #how many best variables I want to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_XGBOOST:\n",
    "    pred_xgb_wgh = [x[0] for x in sorted([(k, v) for k, v in booster.get_score(importance_type = 'weight').items()]\\\n",
    "                                         , key=lambda x:x[1], reverse = True)]\n",
    "    pred_xgb_wgh = [x if x[-4:]=='_WOE' else x+'_WOE' for x in pred_xgb_wgh]\n",
    "    if len(pred_xgb_wgh) > n_top:\n",
    "        pred_xgb_wgh = pred_xgb_wgh[:n_top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain of each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_XGBOOST:\n",
    "    fs = booster.get_score(importance_type = 'gain') # available importance types: 'gain', 'cover', 'weight'\n",
    "    imp = sorted([(k, v) for k, v in fs.items()], key=lambda x:x[1], reverse = True)\n",
    "    imp.reverse()\n",
    "\n",
    "    fig = plt.figure(figsize=(25,20))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.barh(range(len(imp)), [v for k, v in imp], color=\"blue\",  align='center')\n",
    "    plt.yticks(range(len(imp)), [k for k, v in imp], fontsize=15)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.xlabel('Importance',fontsize=15)\n",
    "    plt.ylim([-1, len(imp)])\n",
    "    plt.xlim([0, max([v for k, v in imp])*1.2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select *n_top* variables with highest gain (i.e. relative contribution of the corresponding feature to the model calculated by taking each feature’s contribution for each tree in the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_XGBOOST:\n",
    "    n_top = 30 #how many best variables I want to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_XGBOOST:\n",
    "    pred_xgb_gain = [x[0] for x in sorted([(k, v) for k, v in booster.get_score(importance_type = 'gain').items()]\\\n",
    "                                          , key=lambda x:x[1], reverse = True)]\n",
    "    pred_xgb_gain = [x if x[-4:]=='_WOE' else x+'_WOE' for x in pred_xgb_gain]\n",
    "    if len(pred_xgb_gain) > n_top:\n",
    "        pred_xgb_gain = pred_xgb_gain[:n_top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorecard estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise logistic Regression\n",
    "\n",
    "### Estimate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run stepwise logistic regression on training data set. We start with no predictor in the model and try to add predictors from list called **cols_shortlist2** which is defined below.\n",
    "\n",
    "We can put there all the WOE variables, but we can also used output of one of the feature selection methods above or some kind of their combination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_shortlist2 = cols_woe\n",
    "#cols_shortlist2 = clustering_correlation.bestVariables()\n",
    "#cols_shortlist2 = modelL1.final_predictors_\n",
    "#cols_shortlist2 = pred_xgb_wgh\n",
    "#cols_shortlist2 = pred_xgb_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stepwise process can be tuned using various parameters:\n",
    " - *initial_predictors*: set of starting predictors (useful for backward method)\n",
    " - *max_iter*: maximal number of iterations\n",
    " - *min_increase*: minimal marginal Gini contribution for predictor to be added\n",
    " - *max_decrease*: minimal marginal Gini diminution for predictor to be removed\n",
    " - *max_predictors*: maximal number of predictors to enter the model. Ignored if set to 0.\n",
    " - *max_correlation*: maximal absolute value of correlation of predictors in the model (variable with larger correlation with existing predictors will not be added to the model). **This parameter works for \"forward\" selection method only.**\n",
    " - *beta_sgn_criterion*: if this is set to True, all the betas in the model must have the same signature (all positive or all negative). **This parameter works for \"forward\" selection method only.**\n",
    " - *penalty, C*: regularization parameters for logitic regression (sklearn library)\n",
    " - *correlation_sample*: for better performance, correlation matrix is calculated just on a sample of data. The size of the sample is set in this parameter\n",
    " - *selection_method*: stepwise or forward or backward\n",
    " - *use_cv*: boolean if Cross Validation should be used instead of train/validation split. In this case, if both training and validation samples are presented to the fit method, they are concatenated together and are used for CV. Gini is evaluated as average of all CV's folds' validation Gini. **Please, be aware that using CV after automatic grouping which was trained using train/validate split might lead to overfitted model.**\n",
    " - *cv_folds*: parameter for Cross Validation - number of folds\n",
    " - *cv_seed*: parameter for Cross Validation - random seed used to split the folds\n",
    " \n",
    "The *fit* method can be called with two arguments *fit(X,y)* or with four agruments *fit(X_train,y_train,X_valid,y_valid)*. When called with four arguments, the Gini is measured on the validation sample (i.e. validation sample is used for decisions about what steps to be done in stepwise).\n",
    "\n",
    "There are another optional arguments, *sample_weight* and *sample_weight_valid* where you can put the vector (data set column) with weights of the observations for the train and validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.model_selection import GiniStepwiseLogit\n",
    "\n",
    "modelSW = GiniStepwiseLogit(initial_predictors = set(),\n",
    "                            max_iter=1000,\n",
    "                            min_increase=0.1,\n",
    "                            max_decrease=0.05,\n",
    "                            max_predictors=0, \n",
    "                            max_correlation=0.5, \n",
    "                            beta_sgn_criterion=False, \n",
    "                            penalty='l2', C=10e10, \n",
    "                            correlation_sample=10000,\n",
    "                            selection_method='forward',\n",
    "                            use_cv=True,\n",
    "                            cv_folds=5, \n",
    "                            cv_seed=98765)\n",
    "\n",
    "modelSW.fit(data[train_mask][cols_shortlist2],data[train_mask][col_target]\n",
    "        ,data[valid_mask][cols_shortlist2],data[valid_mask][col_target]\n",
    "       # ,sample_weight = data[train_mask][col_weight], sample_weight_valid = data[valid_mask][col_weight]\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSW.draw_gini_progression(output_folder+'/model/stepwisegini.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSW.model_progress_\n",
    "# to view specific iterations or rows use code below\n",
    "# modelSW.model_progress_[modelSW.model_progress_['iteration']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Regression\n",
    "\n",
    "**Optional:** Alternative to WOE logistic regression. Uses dummy variables instead of WOE variables - it is necessary to make Dummy transformation as the output of Grouping instead of WOE variables.\n",
    "\n",
    "As predictor set which is served to the class *GiniStepwiseFullLogit*, list of original variables (i.e. names of variables before dummy transformation) is used.\n",
    "\n",
    "Grouping, which was used to create dummy variables, is served to the class as parameter called *dummy_bindings*. The algorithm then automatically extracts bindings between original and dummy variables and uses it during steps of forward/backward/stepwise regression.\n",
    "\n",
    "The resulting model includes the both the list of final predictors (original variables) and the list of final dummy variables with their coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictors = cols_pred\n",
    "\n",
    "from scoring.model_selection_full_logit import GiniStepwiseFullLogit\n",
    "\n",
    "modelFull = GiniStepwiseFullLogit(initial_predictors = set(),\n",
    "                                  all_predictors = set(all_predictors),\n",
    "                                  dummy_bindings = grouping.get_dummies(),\n",
    "                                  max_iter=1000, min_increase=0.1, max_decrease=0.05, max_predictors=0,\n",
    "                                  selection_method='stepwise',\n",
    "                                  use_cv=False, cv_folds=5, cv_seed=98765)\n",
    "\n",
    "modelFull.fit(data[train_mask],data[train_mask][col_target]\n",
    "        ,data[valid_mask],data[valid_mask][col_target]\n",
    "       # ,sample_weight = data[train_mask][col_weight], sample_weight_valid = data[valid_mask][col_weight]\n",
    "             )\n",
    "\n",
    "clf = modelFull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oFull = list()\n",
    "oFull.append('| FINAL MODEL | COEFFICIENTS |\\n| --- | --- |')\n",
    "oFull.append('| Intercept | {} |'.format(modelFull.intercept_[0]))\n",
    "for p,b in zip(modelFull.final_predictors_,modelFull.coef_[0]):\n",
    "    oFull.append('| {} | {} |'.format(p,b))\n",
    "display(Markdown('\\n'.join(oFull)))\n",
    "\n",
    "for p in modelFull.final_predictors_orig_:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename2 = 'myModelSW'\n",
    "pickle.dump(modelSW, open(model_filename2, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_filename2 = 'myModelSW'\n",
    "#modelSW = pickle.load(open(model_filename2, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predictors in the model:',list(modelSW.final_predictors_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time stability of predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set metadata for the stability charts. Two types of charts will be drawn:\n",
    "- Stability of default rate, for which the variables with default and with base need to be set\n",
    "- Stability of population, for which the variable with observation count needs to be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"ID\"] = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = modelSW\n",
    "\n",
    "for col in list(clf.final_predictors_):\n",
    "    documentation.GroupedEvaluation(data,\n",
    "                                   predictor=col,\n",
    "                                   sample=\"Observable\",\n",
    "                                   target=col_target,\n",
    "                                   grouping=grouping,\n",
    "                                   output_folder=output_folder + \"/stability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also verify the population stability of the whole time period and stability of shorter target, e.g. FPD30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of the short target column\n",
    "target_for_default_short = \"FPD\"\n",
    "#name of the short target's base column\n",
    "base_for_default_short = \"FPD_BASE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation.targets.append((target_for_default_short, base_for_default_short))\n",
    "documentation.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code adds base for short target (its name is defined above) if it does not already exist in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if base_for_default_short not in data:\n",
    "    data[base_for_default_short] = 0\n",
    "    data.loc[data[target_for_default_short]==0,base_for_default_short] = 1\n",
    "    data.loc[data[target_for_default_short]==1,base_for_default_short] = 1\n",
    "    print('Column',base_for_default_short,'added/modified. Number of columns:',data.shape[1])\n",
    "else:\n",
    "    print('Column',base_for_default_short,'already exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation.sample_dict[\"Observable short\"] = data[base_for_default_short] == 1\n",
    "documentation.sample_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list(clf.final_predictors_):\n",
    "    documentation.GroupedEvaluation(data,\n",
    "                                   predictor=col,\n",
    "                                   sample=\"Observable short\",\n",
    "                                   target=target_for_default_short,\n",
    "                                   grouping=grouping,\n",
    "                                   output_folder=output_folder + \"/stability_short\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability Index\n",
    "\n",
    "The following code calculates two versions of Elena's Stability Index, which are defined as follows.\n",
    "\n",
    "**v1** \n",
    "1. Compute the bad rates in each category for each month\n",
    "2. Compute the rank of each category within each month based on the bad rate\n",
    "3. Compute the frequency of each position / rank within each category from step 2\n",
    "4. Compute the ratios of the most frequent position / rank within each category from step 3\n",
    "5. Compute average of the ratios from step 4\n",
    "\n",
    "**v2**\n",
    "1. Compute the bad rates in each category for each month\n",
    "2. Compute the rank of each category within each month based on the bad rate\n",
    "3. Compute the frequency of each position / rank through all categories from step 2 and the corresponding ratios\n",
    "4. Compute the product of the ratios within each position / rank from step 3\n",
    "5. Compute average of the products from step 4\n",
    "\n",
    "*Note: Both version can give \"false positives\" (indicating that variable is unstable) for U-shaped variables.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.stability_index import stability_index_value\n",
    "\n",
    "stability_tab = []\n",
    "\n",
    "for pred in clf.final_predictors_:\n",
    "    for mask in ['train_mask', 'valid_mask', 'test_mask', 'oot_mask', 'hoot_mask']:\n",
    "        for ver in ['v1', 'v2']:\n",
    "            stability_tab.append({'Name':pred, 'Sample':mask[:-5], 'Index version':ver, 'Index value':\n",
    "                                 stability_index_value(data[eval(mask)], pred, col_target, col_base, col_month)[ver]})\n",
    "\n",
    "stability_tab = pd.DataFrame(stability_tab)\n",
    "stability_tab = stability_tab.groupby(['Name','Index version','Sample'])[['Index value']].mean().unstack(level=[1,2])\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "display(stability_tab)\n",
    "stability_tab.to_csv(output_folder+'/stability/stability_index.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.stability_index import psi_calc_df\n",
    "monthly_psi, masked_psi = psi_calc_df(data, cols_pred_psi=clf.final_predictors_, col_month='MONTH')\n",
    "display(monthly_psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with exact set of predictors\n",
    "\n",
    "Use predictors from stepwise model for marginal contribution test\n",
    "\n",
    "### Estimate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = modelSW\n",
    "cols_mc_shortlist = list(clf.final_predictors_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a model with an exact set of predictors. Set *cols_exactlist* to list of your chosen predictors.\n",
    " - *cols_exactlist*: list of predictors to fit with a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "from scoring.model_selection import GiniStepwiseLogit\n",
    "\n",
    "cols_exactlist = cols_mc_shortlist\n",
    "\n",
    "modelSW1 = GiniStepwiseLogit(initial_predictors = set(cols_exactlist),max_iter=0)\n",
    "modelSW1.fit(data[train_mask][cols_exactlist],data[train_mask][col_target]\n",
    "        ,data[valid_mask][cols_exactlist],data[valid_mask][col_target])\n",
    "m1 = modelSW1.model_progress_.iloc[0]['Gini']\n",
    "display(Markdown('Predictors: {}'.format(',\\n '.join(cols_exactlist))))\n",
    "display(Markdown('Gini: **{}**'.format(m1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename3 = 'myModelSW1'\n",
    "pickle.dump(modelSW1, open(model_filename3, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_filename3 = 'myModelSW'\n",
    "#modelSW1 = pickle.load(open(model_filename3, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For adding predictors\n",
    "Calculate marginal contribution for adding predictors to a chosen set.\n",
    " - *cols_mc*: base list of predictors\n",
    " - *cols_to_add*: list of predictors to be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "from scoring.model_selection import GiniStepwiseLogit\n",
    "\n",
    "cols_mc = cols_mc_shortlist\n",
    "cols_mc_to_add = cols_woe\n",
    "\n",
    "\n",
    "modelSW2 = GiniStepwiseLogit(initial_predictors = set(cols_mc), max_iter=1, min_increase=0.01, max_decrease=0.005,\n",
    "                    max_predictors=0, max_correlation=1, beta_sgn_criterion=False, \n",
    "                    penalty='l2', C=10e10, correlation_sample=10000,\n",
    "                    selection_method='forward')\n",
    "modelSW2.fit(data[train_mask][cols_mc + cols_mc_to_add],\n",
    "             data[train_mask][col_target],\n",
    "             data[valid_mask][cols_mc + cols_mc_to_add],\n",
    "             data[valid_mask][col_target])\n",
    "mcadd = modelSW2.model_progress_[modelSW2.model_progress_['addrm']==1].copy()\n",
    "mcadd.loc[:,'added'] = mcadd['predictors'] - modelSW2.initial_predictors\n",
    "mcadd.loc[:,'added'] = mcadd.loc[:,'added'].apply(lambda x: next(iter(x)))\n",
    "mcadd = mcadd[['added','Gini','diff']].sort_values('diff',ascending=False)\n",
    "mcadd.to_csv('{}/model/marg_cont_add.csv'.format(output_folder))\n",
    "\n",
    "display('Predictors in base model:', cols_mc)\n",
    "display(mcadd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For removing predictors\n",
    "Calculate marginal contribution for removing predictors from a chosen set.\n",
    " - *cols_mc*: base list of predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "from scoring.model_selection import GiniStepwiseLogit\n",
    "\n",
    "cols_mc = cols_mc_shortlist\n",
    "\n",
    "modelSW3 = GiniStepwiseLogit(initial_predictors = set(cols_mc), max_iter=1, min_increase=0.01, max_decrease=0.005,\n",
    "                    max_predictors=0, max_correlation=1, beta_sgn_criterion=False, \n",
    "                    penalty='l2', C=10e10, correlation_sample=10000,\n",
    "                    selection_method='backward')\n",
    "modelSW3.fit(data[train_mask][cols_mc],\n",
    "             data[train_mask][col_target],\n",
    "             data[valid_mask][cols_mc],\n",
    "             data[valid_mask][col_target])\n",
    "mcrem = modelSW3.model_progress_[modelSW3.model_progress_['addrm']==-1].copy()\n",
    "mcrem.loc[:,'removed'] = modelSW3.initial_predictors - mcrem['predictors']\n",
    "mcrem.loc[:,'removed'] = mcrem.loc[:,'removed'].apply(lambda x: next(iter(x)))\n",
    "mcrem = mcrem[['removed','Gini','diff']].sort_values('diff',ascending=False)\n",
    "mcrem.to_csv('{}/model/marg_cont_rem.csv'.format(output_folder))\n",
    "\n",
    "display(mcrem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Surrogates\n",
    "\n",
    "Based on clustering which you ran in the previous parts of workflow (either *hierarchical* or *k-means*), clusters of original variables were established. In this part of workflow, you can check what would happen if you swapped any of the predictors with another variable from the same cluster.\n",
    "This is particulartly useful when you can’t use an originally selected predictor (e.g. there is a business reason why the predictor should not be used).\n",
    "For this object to work, you need list of predictors (from your model), list of all variables and corresponding list of numbers assigning clusters to these variables. In the data set, you need to have all the variables from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.model_selection import VarClusSurrogates\n",
    "\n",
    "surrogates = VarClusSurrogates(clustering_correlation.variables_,\n",
    "                               clustering_correlation.labels_,\n",
    "                               modelSW.final_predictors_)\n",
    "\n",
    "surrogates.fit(data[train_mask][cols_woe],\n",
    "               data[train_mask][col_target],\n",
    "               data[valid_mask][cols_woe],\n",
    "               data[valid_mask][col_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogates.displaySurrogates(output_folder+'/predictors/cluster_surrogates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score the dataset\n",
    "**First choose which model is your final model** (into variable *clf*)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = modelL1\n",
    "clf = modelSW1\n",
    "\n",
    "cols_final_predictors = list(clf.final_predictors_)\n",
    "pd.DataFrame(cols_final_predictors).to_csv(output_folder+'/predictors/predictors.csv',index=False,header=None)\n",
    "\n",
    "o = list()\n",
    "o.append('| FINAL MODEL | COEFFICIENTS |\\n| --- | --- |')\n",
    "o.append('| Intercept: | {} |'.format(clf.intercept_[0]))\n",
    "for p,b in zip(cols_final_predictors,list(clf.coef_[0])):\n",
    "    o.append('| {} | {} |'.format(p,b))\n",
    "display(Markdown('\\n'.join(o)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column with the prediction (probability of default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_score = 'SCORE'\n",
    "\n",
    "data[col_score] = clf.predict(data)\n",
    "print('Column',col_score,'with the prediction added/modified. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorecard table output\n",
    "Output the scorecard to a table. Stats are calculated on a subset of data given by the mask defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this mask is an union of masks for training, validation, testing and out of time data sets\n",
    "table_mask = train_mask|valid_mask|test_mask|oot_mask|hoot_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.final_predictors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.scorecard import ScoreCard\n",
    "scorecard_new = ScoreCard(grouping=grouping,\n",
    "                          predictors=clf.final_predictors_,\n",
    "                          coefficients=clf.coef_,\n",
    "                          intercept=clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 1000\n",
    "display(scorecard_new.scorecard_table_simple())\n",
    "pd.options.display.max_rows = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 1000\n",
    "scorecard_out = scorecard_new.scorecard_table_full(data, table_mask, col_target, weightcol=None)\n",
    "scorecard_out.to_csv(output_folder + '/model/scorecard.csv')\n",
    "display(scorecard_out)\n",
    "pd.options.display.max_rows = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reject inference\n",
    "In case there were many rejects in the approval process, it might happen that for some values of certain predictors, reject rate was much higher than for others. In this case, so called *„cherry picking“* migh have occured: only the best part of certain sub-population (corresponding to some predictor values) is approved resulting in default rate of this subpopulation being seemingly much lower than it would be normally. This might affect the future performance of the new model, as this sub-population would receive overly optimistic score based on training data.\n",
    "\n",
    "## Reject inference analysis\n",
    "\n",
    "In this analysis we see WoE of default rate (based on logarithm of odds „good vs bad“) compared to WoE of reject rate (based on logarithm odds „approve vs reject“). If these two WoEs have opposite trend for a certain predictor, it means that there is potential reject inference issue.\n",
    "\n",
    "**For this object to work properly, you need to include rejected applications in the data and add a new 0-1 variable containing the rejected flag.** This means that the original masks as *train_mask* can't be used as they consist of observations with measureable defaults only, i.e. there are not the rejected observations in them.\n",
    "\n",
    "The reject inference analysis can be peformed on two samples (e.g. train and validation) so you can also compare WoEs of these samples against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.reject_inference import RejectInferenceCharts\n",
    "\n",
    "rejectinf = RejectInferenceCharts(target=col_target, reject=col_reject, predictors=cols_final_predictors, weight=None)\n",
    "rejectinf.fit(data1 = data[data['data_type']=='train'],\n",
    "              data2 = data[data['data_type']=='valid'])\n",
    "rejectinf.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejected target imputation\n",
    "\n",
    "Sometimes, we have many rejected cases without observed target, however there exist some kind of proxy target, which can be assigned to these observations so the final model can be scored on them as well as on the usual observations.\n",
    "\n",
    "For example: there is a high reject rate, but for the rejected cases we have some external score which can tell us to some extent whether they woul default or not. To avoid reject inference we want to add these observations into our trainig sample.\n",
    "\n",
    "**TargetImputer** is a class for target imputation based on pre-calculated target probabilities. Typical usage is if we want to assign a proxy target to rows where target is unobserved.\n",
    "There are three imputation types that can be used:\n",
    "- *randomized*: for each observation where we want to impute the target, an random number between 0 and 1 is generated. Then this ranom number is compared with the pre-calculated probability and target is assigned based on this comparison.\n",
    "- *cutoff*: for each observation where we want to impute the target, the pre-caluclated probability is compared with this cutoff value and target is assigned based on this comparison.\n",
    "- *weighted*: for each observation where we want to impute the target, two observations are generated. Each of them has a different value of the target, one of the has weight p and the other has weight 1-p, where p is the pre-calculated target probability.\n",
    "\n",
    "When we initiate an instance of this class, we use the following arguments:\n",
    "- imputation_type - Imputation which we want to use, can be 'randomized', 'cutoff' or 'weighted'\n",
    "- cutoff - Cutoff value which is used if 'cutoff' imputation_type is selected (default: 0.5)\n",
    "- random_seed - Random seed which is used if 'randomized' imputation_type is selected (default: 987)\n",
    "\n",
    "Then, *fit* method is called, where the dataset where the target should be imputed is specified. This dataset must contain a column with pre-caluclated target probabilities (floats between 0 and 1) and a column with indicator whether target should be imputed for given row (integers with values 0 or 1). The method then calculates the imputed target using the parameter specified during initialization.\n",
    "\n",
    "And finally, *transform* method add the imputed values into the dataset. We can specify, whether they should rewrite the original values or be added as new columns. For *weighted* imputation method, it is recommended to use optional argument `reset_index=True`, because this method duplicates some rows, and having rows with duplicate index can cause error in other parts of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scoring.reject_inference import TargetImputer\n",
    "\n",
    "# targetImputer = TargetImputer(imputation_type='randomized', random_seed=987)\n",
    "\n",
    "# targetImputer.fit(data = data,\n",
    "#                   col_probs = col_score,\n",
    "#                   col_reject = col_reject,\n",
    "#                   #col_weight = col_weight,\n",
    "#                   prob_of = 1)\n",
    "\n",
    "# data = targetImputer.transform(data = data,\n",
    "#                                col_target = col_target,\n",
    "#                                #col_weight = col_weight,\n",
    "#                                as_new_columns = True,\n",
    "#                                reset_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance characteristics\n",
    "Performance characteristics of the model (Gini, Lift) and their visualisations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance per sample\n",
    "\n",
    "If some od these samples (train, valid, test, OOT, HOOT) are empty (i.e. you don't use them), **you need to comment their according rows in some of the following cells.** Such rows are marked by short comments at their ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.metrics import gini, lift, kolmogorov_smirnov, eval_performance_wrapper\n",
    "lift_perc = 10\n",
    "\n",
    "eval_masks = {\n",
    "    'train' : train_mask,\n",
    "    'valid' : valid_mask,\n",
    "    'test' : test_mask,\n",
    "    'oot' : oot_mask,\n",
    "    'hoot' : hoot_mask,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = eval_performance_wrapper(data=data,\n",
    "                                masks=eval_masks,\n",
    "                                col_target=col_target,\n",
    "                                col_score=col_score,\n",
    "                                #col_weight=col_weight,\n",
    "                                lift_perc=10)\n",
    "display(perf)\n",
    "perf.to_csv(output_folder+'/performance/performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.tools import curves_wrapper\n",
    "curves_wrapper(data=data,\n",
    "               masks=eval_masks,\n",
    "               col_target=col_target,\n",
    "               col_score=col_score,\n",
    "               #col_weight=col_weight,\n",
    "               lift_perc=10,\n",
    "               output_folder=output_folder+'/performance/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring.plot.plot_kolmogorov_smirnov(data[col_target], data[col_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def proc_gini(x,y,z):\n",
    "    fpr, tpr, _ = roc_curve(x[y], x[z], pos_label=0)\n",
    "    roc_gini = (auc(fpr, tpr)-0.5)*2\n",
    "    return roc_gini\n",
    "%matplotlib inline\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "len1 = 0\n",
    "\n",
    "grouped = data[hoot_mask].groupby(col_month, axis=0) #hoot\n",
    "res_hoot= grouped.apply(proc_gini, col_target ,col_score) #hoot\n",
    "plt.plot(range(len1,len1+len(res_hoot)),-res_hoot, linewidth=2.0,label='hist. OOT', color = 'm', marker='o') #hoot\n",
    "\n",
    "if res_hoot is not None: len1 = len1 + len(res_hoot)\n",
    "\n",
    "grouped = data[train_mask].groupby(col_month, axis=0) #train\n",
    "res_train= grouped.apply(proc_gini, col_target ,col_score) #train\n",
    "plt.plot(range(len1,len1+len(res_train)),-res_train, linewidth=2.0,label='Train', color = 'g', marker='o') #train\n",
    "grouped = data[valid_mask].groupby(col_month, axis=0) #valid\n",
    "res_valid= grouped.apply(proc_gini, col_target ,col_score) #valid\n",
    "plt.plot(range(len1,len1+len(res_valid)),-res_valid, linewidth=2.0,label='Validation', color = 'r', marker='o') #valid\n",
    "grouped = data[test_mask].groupby(col_month, axis=0) #test\n",
    "res_test= grouped.apply(proc_gini, col_target ,col_score) #test\n",
    "plt.plot(range(len1,len1+len(res_test)),-res_test, linewidth=2.0,label='Test', color = 'y', marker='o') #test\n",
    "\n",
    "if res_train is not None: len1 = len1 + len(res_train)\n",
    "\n",
    "grouped = data[oot_mask].groupby(col_month, axis=0) #oot\n",
    "res_oot= grouped.apply(proc_gini, col_target ,col_score) #oot\n",
    "plt.plot(range(len1,len1+len(res_oot)),-res_oot, linewidth=2.0,label='OOT', color = 'b', marker='o') #oot\n",
    "\n",
    "plt.xticks(range(len(res_train)+len(res_oot)), np.sort(data[col_month].unique()), rotation=45)\n",
    "plt.xticks(range(len(data[col_month].unique())), np.sort(data[col_month].unique()), rotation=45)\n",
    "\n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.title('Gini by months')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Gini')\n",
    "plt.savefig(output_folder+'/performance/ginistability.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance - bootstrap\n",
    "\n",
    "In this part, we make bootstrapped sample from the defined dataset `n_iter` times, measure Gini of the score each time and the calculate the (5%, 95%) confidence interval for Gini from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of iterations for bootstrapping\n",
    "n_iter = 100\n",
    "# confidence interval range in percent (the CI will be calculated as [ci_range, 100-ci_range])\n",
    "ci_range = 5\n",
    "\n",
    "from scoring.metrics import bootstrap_gini\n",
    "\n",
    "bootstrap_results = []\n",
    "\n",
    "for name, mask in tqdm(eval_masks.items(), leave = False):\n",
    "    mean, std, ci = bootstrap_gini(data=data[mask],\n",
    "                                   col_target=col_target,\n",
    "                                   col_score=col_score,\n",
    "                                   #col_weight=col_weight,\n",
    "                                   n_iter=n_iter,\n",
    "                                   ci_range=ci_range)\n",
    "    bootstrap_results.append([name, mean, std, ci[0], ci[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_results = pd.DataFrame(bootstrap_results).set_index(0)\n",
    "bootstrap_results.index.rename(\"Sample\", inplace=True)\n",
    "bootstrap_results.columns=[\"Gini Mean\", \"Gini std\", \"CI 5%\", \"CI 95%\"]\n",
    "display(bootstrap_results)\n",
    "perf.to_csv(output_folder+'/performance/bootstrap_performance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score distribution\n",
    "\n",
    "## Distribution charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import plot_score_dist\n",
    "\n",
    "data['score_lin'] = np.log(data[col_score] / (1 - data[col_score]))\n",
    "distr_linear = plot_score_dist(data[train_mask|valid_mask|test_mask|oot_mask|hoot_mask], score_name = 'score_lin',\n",
    "                      target_name = col_target, weight_name = None, n_bins = 25,\n",
    "                      savefile = output_folder+'/model/distr_linear.png')\n",
    "\n",
    "distr_pd = plot_score_dist(data[train_mask|valid_mask|test_mask|oot_mask|hoot_mask], score_name = col_score,\n",
    "                      target_name = col_target, weight_name = None, n_bins = 100, min_score = 0, max_score = 1,\n",
    "                      savefile = output_folder+'/model/distr_pd.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation.ScoreCalibration(data,\n",
    "                               score=col_score,\n",
    "                               sample=[\"Train\", \"Valid\", \"Test\", \"OOT\", \"HOOT\"],\n",
    "                               target=col_target,\n",
    "                               output_folder=output_folder+\"/model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations\n",
    "Calculate and visualise correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation.Correlations(data,\n",
    "                           predictors=cols_final_predictors,\n",
    "                           sample=\"All\",\n",
    "                           output_folder=output_folder+\"/analysis/\",\n",
    "                           filename=\"correlation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the list of the highest correlation (restricted to correlations that are, in absolute value, higher than *max_ok_correlation* parameter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cormat = data[cols_final_predictors].corr()\n",
    "max_ok_correlation = 0.0\n",
    "\n",
    "# find highest pairwise correlation (correlation greater than .. in absolute value)\n",
    "hicors = []\n",
    "for i in range(0,len(cormat)):\n",
    "    for j in range(0,len(cormat)):\n",
    "        if ((cormat.iloc[i][j] > max_ok_correlation or cormat.iloc[i][j] < -max_ok_correlation) and i < j):\n",
    "            hicors.append((i,j,cormat.index[i],cormat.index[j],cormat.iloc[i][j],abs(cormat.iloc[i][j])))\n",
    "hicors.sort(key= lambda tup: tup[5], reverse=True)\n",
    "\n",
    "hicors2 = pd.DataFrame(list(zip(*list(zip(*hicors))[2:5])))\n",
    "\n",
    "# print list of highest correlations\n",
    "hicors2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with another score\n",
    "\n",
    "## Performance comparison\n",
    "Similar charts to what were already done for the new scorecard are now drawn to compare the new scorecard to another scorecard. The value of the old score should be saved in a special column of original data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_oldscore = 'OLD_SCORE'\n",
    "\n",
    "#define subset dataOld where old score is not null\n",
    "old_mask = pd.notnull(data[col_oldscore])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation.sample_dict[\"Old comparison\"] = (test_mask|oot_mask|hoot_mask) & old_mask\n",
    "documentation.sample_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_old_masks = {\n",
    "    'valid' : valid_mask & old_mask,\n",
    "    'test' : test_mask & old_mask,\n",
    "    'oot' : oot_mask & old_mask,\n",
    "    'hoot' : hoot_mask & old_mask,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the score gives the complementary probability (of non-default), run the following. Otherwise, don't run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[col_oldscore]=1-data[col_oldscore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_perc = 10\n",
    "\n",
    "perf_oldscore = eval_performance_wrapper(data=data,\n",
    "                                masks=eval_old_masks,\n",
    "                                col_target=col_target,\n",
    "                                col_score=[col_score, col_oldscore],\n",
    "                                #col_weight=col_weight,\n",
    "                                lift_perc=10)\n",
    "\n",
    "display(perf_oldscore)\n",
    "perf_oldscore.to_csv(output_folder+'/performance/comparison_performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.tools import curves_wrapper\n",
    "curves_wrapper(data=data,\n",
    "               masks={'test & oot': (test_mask|oot_mask|hoot_mask) & old_mask},\n",
    "               col_target=col_target,\n",
    "               col_score=[col_score, col_oldscore],\n",
    "               #col_weight=col_weight,\n",
    "               lift_perc=10,\n",
    "               output_folder=output_folder+'/performance/comparison_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini in time comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation.ScoreComparison(data,\n",
    "                              scores=[col_score, col_oldscore],\n",
    "                              sample=\"Old comparison\",\n",
    "                              target=col_target,\n",
    "                              output_folder=output_folder+'/performance',\n",
    "                              filename= \"comparison_ginistability.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices for the observable population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import transmatrix\n",
    "    \n",
    "transmatrix(oldscore = data[(valid_mask|test_mask|oot_mask|hoot_mask) & old_mask][col_oldscore],\n",
    "            newscore = data[(valid_mask|test_mask|oot_mask|hoot_mask) & old_mask][col_score],\n",
    "            target = data[(valid_mask|test_mask|oot_mask|hoot_mask) & old_mask][col_target],\n",
    "            base = data[(valid_mask|test_mask|oot_mask|hoot_mask) & old_mask][col_base],\n",
    "            obs = data[(valid_mask|test_mask|oot_mask|hoot_mask) & old_mask][col_base],\n",
    "            draw_default_matrix=True,\n",
    "            draw_transition_matrix=True,\n",
    "            savepath=output_folder+'/analysis/devpop_',\n",
    "            quantiles_count = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition matrix for the whole population (put also the rejected etc. here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_mask = data.data_type != 'train'\n",
    "\n",
    "transmatrix(oldscore = data[pop_mask & old_mask][col_oldscore],\n",
    "            newscore = data[pop_mask & old_mask][col_score],\n",
    "            target = data[pop_mask & old_mask][col_target],\n",
    "            base = data[pop_mask & old_mask][col_base],\n",
    "            obs = data[pop_mask & old_mask][col_base],\n",
    "            draw_default_matrix=False,\n",
    "            draw_transition_matrix=True,\n",
    "            savepath=output_folder+'/analysis/allpop_',\n",
    "            quantiles_count = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on short target\n",
    "If there is also a shorter (e.g. FPD30) target in the original dataset, we draw also charts for performance on this target in this part of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of the short target column\n",
    "col_short = \"FPD\"\n",
    "#name of the short target's base column\n",
    "col_shortbase = \"FPD_BASE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have base column in your data set, the following code adds it. **Otherwise, don't run it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if col_shortbase not in data:\n",
    "    data[col_shortbase] = 0\n",
    "    data.loc[data[col_short]==0,col_shortbase] = 1\n",
    "    data.loc[data[col_short]==1,col_shortbase] = 1\n",
    "    print('Column',col_shortbase,'added/modified. Number of columns:',data.shape[1])\n",
    "else:\n",
    "    print('Column',col_base,'already exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortbase_mask = ((data.data_type == 'test')|(data.data_type == 'oot')|(data.data_type == 'hoot')) \\\n",
    "&(data[col_shortbase] == 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_perc = 10\n",
    "\n",
    "perf_shorttarget = eval_performance_wrapper(data=data,\n",
    "                                masks={'short base test & oot': shortbase_mask},\n",
    "                                col_target=col_target,\n",
    "                                col_score=col_score,\n",
    "                                #col_weight=col_weight,\n",
    "                                lift_perc=10)\n",
    "\n",
    "display(perf_shorttarget)\n",
    "perf_shorttarget.to_csv(output_folder+'/performance/performance_shorttarget.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def proc_gini(x,y,z):\n",
    "    fpr, tpr, _ = roc_curve(x[y], x[z], pos_label=0)\n",
    "    roc_gini = (auc(fpr, tpr)-0.5)*2\n",
    "    return roc_gini\n",
    "%matplotlib inline\n",
    "plt.figure(figsize = (10,7))\n",
    "grouped = data[valid_mask|test_mask|oot_mask|hoot_mask].groupby(col_month, axis=0)\n",
    "res_new= grouped.apply(proc_gini, col_target ,col_score)\n",
    "plt.plot(range(len(res_new)),-res_new, linewidth=2.0,label='target', color = 'g', marker='o')\n",
    "\n",
    "grouped = data[shortbase_mask].groupby(col_month, axis=0)\n",
    "res_short= grouped.apply(proc_gini, col_short ,col_score)\n",
    "plt.plot(range(len(res_short)),-res_short, linewidth=2.0,label='short target', color = 'r', marker='o')\n",
    "\n",
    "plt.xticks(range(len(data[col_month].unique())), np.sort(data[col_month].unique()), rotation=45)\n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.title('Gini by months')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Gini')\n",
    "plt.savefig(output_folder+'/performance/ginistability_shorttarget.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorecard code export\n",
    "This uses scorecard table generated above (before reject inference and performance analyses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL code\n",
    "\n",
    "To run the scorecard on Oracle DWH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scorecard_new.to_SQL(ntbOut=True, file='scorecard.sql', output_folder=output_folder))\n",
    "\n",
    "# generate special SQL query with grouping as requested by VN analytical team\n",
    "#scorecard_new.to_SQL_with_grouping(ntbOut=True, file='scorecard_with_grouping.sql', output_folder=output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blaze table\n",
    "\n",
    "To be imported using Blaze Remote Desktop *Blaze Tools*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard_new.to_blaze_rd(ntbOut=True, file='my_blaze_scorecard.csv', output_folder=output_folder)\n",
    "\n",
    "# generate slighly different format of Blaze Table compatible with Russian Blaze tools\n",
    "#scorecard_new.to_blaze(ntbOut=True, file='my_blaze_scorecard.csv', output_folder=output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python code\n",
    "\n",
    "To run the scorecard in any Python script independently on this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scorecard_new.to_python(ntbOut=True, file='my_python_scorecard.py', output_folder=output_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML documentation\n",
    "\n",
    "Create a basic HTML document with important scorecard characteristics. The results of some of the previous parts of the workflow have to be already created on disk, this part will just wrap them up.\n",
    "\n",
    "If some specific parts (short target analysis, old score comparison) were not done, use the parameters below and set them to *False*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_scorecard_name = 'My PSW model'\n",
    "txt_author_name = 'John Doe'\n",
    "short_target_analysis_done = True\n",
    "old_score_comparison_done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_folder+'/documentation.html', 'w', encoding='utf-8') as f:\n",
    "    f.write('<html>\\n<head>\\n<title>'+txt_scorecard_name+'</title>\\n')\n",
    "    f.write('<meta charset=\"utf-8\">\\n')    \n",
    "    f.write('<style>\\nbody{font: normal 10pt Helvetica, Arial, sans-serif;}\\n'+ \\\n",
    "            '.textbold{font-weight:bold;}\\n' + \\\n",
    "            '.divcode{font-family:Courier New,Courier,Lucida Sans Typewriter,Lucida Typewriter,monospace;}\\n' + \\\n",
    "            '.divpic{padding-bottom: 20pt;}\\n' + \\\n",
    "            '.textlabel{font-style:italic;font-size:8pt;}\\n' + \\\n",
    "            'table{border-collapse:collapse;}\\n' + \\\n",
    "            '</style>\\n')\n",
    "    f.write('</head>\\n<body>')\n",
    "    f.write('<h1>'+txt_scorecard_name+' - documentation</h1>\\n')\n",
    "    f.write('<h2>Document information</h2>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divtext\"><span class=\"textbold\">Author:</span> '+txt_author_name+'</div>\\n')\n",
    "    f.write(' <div class=\"divtext\"><span class=\"textbold\">Date:</span> '+ \\\n",
    "            datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")+'</div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h2>Data sample</h2>\\n')\n",
    "    f.write('<h3>Target</h3>')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divtext\"><span class=\"textbold\">Target variable:</span> '+\\\n",
    "            pd.read_csv(output_folder+'/model/metadata.csv',header=None,index_col=0).loc['col_target'][1]+'</div>\\n')\n",
    "    f.write(' <div class=\"divtext\"><span class=\"textbold\">Base variable:</span> '+\\\n",
    "            pd.read_csv(output_folder+'/model/metadata.csv',header=None,index_col=0).loc['col_base'][1]+'</div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h3>Sample characteristics</h3>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"analysis/data.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Observations and defaults in time</span></div>\\n')\n",
    "    f.write(' <div class=\"divtab\">\\n'+pd.read_csv(output_folder+'/analysis/summary.csv',header=[0,1],index_col=0) \\\n",
    "            .to_html(na_rep='')+'\\n </div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h3>Covariates</h3>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divtab\">\\n'+pd.read_csv(output_folder+'/predictors/covariates.csv',header=0,index_col=0) \\\n",
    "            .to_html(na_rep='')+'\\n </div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h2>Final scorecard</h2>\\n')\n",
    "    f.write('<h3>Scorecard</h3>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divtab\">\\n'+pd.read_csv(output_folder+'/model/scorecard.csv',header=0,index_col=0,keep_default_na = False,na_values = ['']) \\\n",
    "            .to_html(na_rep='')+'\\n </div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h3>Scoring SQL</h3>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divcode\">\\n'+open(output_folder+'/model/scorecard.sql', 'r').read() \\\n",
    "            .replace(' ','&nbsp;').replace('\\n','<br />')+'\\n </div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h2>Predictors</h2>\\n')\n",
    "    for pred in pd.read_csv(output_folder+'/predictors/predictors.csv',index_col=None,header=None)[0].tolist():\n",
    "        pred0 = ''.join(pred.split())[:-4]\n",
    "        f.write('<h3>'+pred0+'</h3>\\n')\n",
    "        f.write('<h4>Grouping</h4>')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"predictors/'+pred0+'_binning.png\" /></div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "        f.write('<h4>Stability</h4>')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"stability/'+pred+'.png\" /></div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "    f.write('<h2>Correlations</h2>\\n')\n",
    "    f.write('<h3>Correlation matrix between WOE variables</h3>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"analysis/correlation.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Correlation of WOE variables</span></div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h2>Model evaluation</h2>\\n')\n",
    "    f.write('<h3>Performance</h3>\\n')\n",
    "    f.write('<h4>General performance</h4>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divtab\">\\n'+pd.read_csv(output_folder+'/performance/performance.csv',header=0,index_col=0) \\\n",
    "            .to_html(na_rep='')+'\\n </div>\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"performance/roc.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">ROC curve</span></div>\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"performance/lift.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Lift curve</span></div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h4>Performance stability</h4>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"performance/ginistability.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Stability of Gini in time</span></div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h3>Marginal Contribution</h3>\\n')\n",
    "    f.write('<h4>Adding predictors</h4>\\n')\n",
    "    f.write(' <div class=\"divtab\">\\n'+pd.read_csv(output_folder+'/model/marg_cont_add.csv',header=0,index_col=0) \\\n",
    "            .to_html(na_rep='')+'\\n </div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h4>Removing predictors</h4>\\n')\n",
    "    f.write(' <div class=\"divtab\">\\n'+pd.read_csv(output_folder+'/model/marg_cont_rem.csv',header=0,index_col=0) \\\n",
    "            .to_html(na_rep='')+'\\n </div>\\n')\n",
    "    if short_target_analysis_done:\n",
    "        f.write('<h3>Performance on shorter target</h3>\\n')\n",
    "        f.write('<h4>General performance</h4>\\n')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divtab\">\\n'+ \\\n",
    "                pd.read_csv(output_folder+'/performance/performance_shorttarget.csv',header=0,index_col=0) \\\n",
    "                .to_html(na_rep='')+'\\n </div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "        f.write('<h4>Performance stability</h4>\\n')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"performance/ginistability_shorttarget.png\" />\\n' + \\\n",
    "                ' <br /><span class=\"textlabel\">Stability of Gini in time</span></div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "        f.write('<h3>Calibration</h3>\\n')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"model/calibration.png\" />\\n' + \\\n",
    "                ' <br /><span class=\"textlabel\">Model calibration chart</span></div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "    if old_score_comparison_done:\n",
    "        f.write('<h2>Comparison with current model</h2>\\n')\n",
    "        f.write('<h3>Performance comparison</h3>\\n')\n",
    "        f.write('<h4>General performance</h4>\\n')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divtab\">\\n'+ \\\n",
    "                pd.read_csv(output_folder+'/performance/comparison_performance.csv',header=[0,1],index_col=0) \\\n",
    "                .to_html(na_rep='')+'\\n </div>\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"performance/comparison_roc.png\" />\\n' + \\\n",
    "                ' <br /><span class=\"textlabel\">ROC curve</span></div>\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"performance/comparison_lift.png\" />\\n' + \\\n",
    "                ' <br /><span class=\"textlabel\">Lift curve</span></div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "        f.write('<h4>Performance stability</h4>\\n')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"performance/comparison_ginistability.png\" />\\n' + \\\n",
    "                ' <br /><span class=\"textlabel\">Stability of Gini in time</span></div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "        f.write('<h3>Transition matrices</h3>\\n')\n",
    "        f.write('<h4>Bad rate matrix</h4>\\n')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"analysis/devpop_matrix_default.png\" />\\n' + \\\n",
    "                ' <br /><span class=\"textlabel\">Default rate matrix</span></div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "        f.write('<h4>Transition matrix - development sample</h4>\\n')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"analysis/devpop_matrix_transition.png\" />\\n' + \\\n",
    "                ' <br /><span class=\"textlabel\">Transition matrix</span></div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "        f.write('<h4>Transition matrix - whole population</h4>\\n')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"analysis/allpop_matrix_transition.png\" />\\n' + \\\n",
    "                ' <br /><span class=\"textlabel\">Transition matrix</span></div>\\n')\n",
    "        f.write('</div>\\n')  \n",
    "    f.write('</body></html>')\n",
    "    print('Created documentation in file',f.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "638px",
    "left": "1569px",
    "right": "20px",
    "top": "72px",
    "width": "289px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
