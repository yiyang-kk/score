{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Credit Python Scoring Workflow v.0.4.1\n",
    "\n",
    "**Contributors:**\n",
    "- Pavel SÅ¯va (HCI Research & Development)\n",
    "- Sergey Gerasimov (HCRU Scoring & Big Data)\n",
    "- Valentina Kalenichenko (HCRU Scoring & Big Data)\n",
    "- Marek Teller (HCI Research & Development)\n",
    "- Martin Kotek (HCI Research & Development)\n",
    "- Jan Zeller (HCI Research & Development)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "- time, datetime - ability to get current time for logs\n",
    "- math - basic mathematical functions (as logarithm etc.))\n",
    "- random - generate random selection from probability distributions\n",
    "- NumPy - for scientific, mathematical, numerical calculations\n",
    "- Pandas - for efficient work with large data structures (you need pandas **version 0.21 or higher**)\n",
    "- cx_Oracle and sqlalchemy - for loading data from Oracle database (DWH etc.)\n",
    "- statsmodels - library with some statistical functions and models\n",
    "- scikit-learn - all important machine learning (and statistical) algorithms used for training the models\n",
    "- matplotlib - for plotting the charts\n",
    "- seaborn - for statistical visualisations\n",
    "- os - for setting output paths for generated image files\n",
    "- pickle - to save models to external files\n",
    "\n",
    "**If any of these packages is missing, you have to install it from the Anaconda prompt using command *conda install packagename* where *packagename* is the name of the installed package.**\n",
    "\n",
    "There is another package called *scoring*, which is distributed along with this workflow. **The folder *scoring* must be located in the same folder as this workflow for the package to be loaded correctly.** Alternatively, you can locate it somewhere else and then use *sys.path.insert()* to map this location.\n",
    "\n",
    "### Other important prerequisites:\n",
    "\n",
    "For the grouping some **extensions for Jupyter must be installed and enabled before Jupyter is started and the notebook is loaded**. These extensions are Javascripts running in the browser, so it is necessary to have a compatibile browser. Generally, Chrome is OK, Internet Explorer 11 is NOT OK. To install the extensions, run this in your Anaconda prompt:\n",
    "\n",
    "- *conda install ipywidgets*\n",
    "- *jupyter nbextension enable --py --sys-prefix widgetsnbextension*\n",
    "- *conda config --add channels conda-forge*\n",
    "- *conda install qgrid* \n",
    "- *jupyter nbextension enable --py --sys-prefix qgrid*\n",
    "\n",
    "Please, make sure that qgrid library that you installed in this step is **verison 1.0.2 or higher**. \n",
    "\n",
    "To be able to connect to Oracle database (to get the data directly from your DWH) you need a compatibile Oracle driver to be installed on your computer. **With 64-bit Python, you need to have 64-bit Oracle driver installed.** Before you install the driver, you need to have Java 8 JDK (JRE is not enough) installed on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import operator\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cx_Oracle\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils import as_float_array\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "#import sys\n",
    "#sys.path.insert(0, 'C:/py_src/scoring/hcfsc')\n",
    "import scoring\n",
    "#import importlib\n",
    "#importlib.reload(scoring)\n",
    "#importlib.reload(scoring.grouping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set general technical parameters and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.close_figures=True\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 15\n",
    "output_folder = 'documentation'\n",
    "\n",
    "if not os.path.exists(output_folder): os.makedirs(output_folder)\n",
    "if not os.path.exists(output_folder+'/performance'): os.makedirs(output_folder+'/performance')\n",
    "if not os.path.exists(output_folder+'/predictors'): os.makedirs(output_folder+'/predictors')\n",
    "if not os.path.exists(output_folder+'/stability'): os.makedirs(output_folder+'/stability')\n",
    "if not os.path.exists(output_folder+'/analysis'): os.makedirs(output_folder+'/analysis')\n",
    "if not os.path.exists(output_folder+'/model'): os.makedirs(output_folder+'/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "Importing data from a CSV file. It is important to set the following parameters:\n",
    "\n",
    "encoding: usually 'utf-8' or windows-xxxx on Windows machines, where xxxx is 1250 for Central Europe, 1251 for Cyrilic etc.\n",
    "sep: separator of columns in the file\n",
    "decimal: decimal dot or coma\n",
    "index_col: which columns is used as index - should be the unique credit case identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'ExampleData4.CSV', sep = ',', decimal = '.', \n",
    "                   encoding = 'utf-8', index_col = 'ID', low_memory = False)\n",
    "print('Data loaded on',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data need to have index column which has unique value per each row. If not, it can cause some problems later. Run this to deal with such rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 1: remove rows with duplicated index\n",
    "data=data[~data.index.duplicated(keep='first')]\n",
    "\n",
    "#Option 2: reset index\n",
    "#data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally the data can be loaded also from a database. The function read_sql uses cache, so the data don't have to be downloaded from the database repeatedly. The cache will be located in a new folder called **db_cache**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#engine = create_engine('oracle://PAVELS[GP_HQ_RISK]:xxx@(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=DBDWHRU.HOMECREDIT.RU)(PORT=1521))(CONNECT_DATA=(SERVICE_NAME=DWHRU)))', echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scoring.db import read_sql\n",
    "#ru_data = read_sql('select * from owner_dwh.f_application_tt where rownum<11',engine, index_col = 'sk_application')\n",
    "#print('Data loaded on',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to download data from the database again (and not from cache), use the parameter refresh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scoring.db import read_sql\n",
    "#data = read_sql('select * from owner_dwh.f_application_base_tt where rownum=1',engine, index_col = 'skp_application',refresh=True)\n",
    "#print('Data loaded on',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows:',data.shape[0])\n",
    "print('Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata definitions\n",
    "Assigning ID column, target column, time column and month column. The month column don't have to exist in the dataset, it will be created later in this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of the time column\n",
    "col_time = \"TIME\"\n",
    "#name of the month column\n",
    "col_month = \"MONTH\"\n",
    "#name of the day column\n",
    "col_day = \"DAY\"\n",
    "#name of the target column\n",
    "col_target = \"DEF\"\n",
    "#name of the base column\n",
    "col_base = \"BASE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records([['col_time',col_time],['col_month',col_month],['col_day',col_day],['col_target',col_target],['col_base',col_base]]) \\\n",
    ".to_csv(output_folder+'/model/metadata.csv',index=0,header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have base column in your data set, the following code adds it (based on if target is filled). **Otherwise, don't run it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if col_base not in data:\n",
    "    data[col_base] = 0\n",
    "    data.loc[data[col_target]==0,col_base] = 1\n",
    "    data.loc[data[col_target]==1,col_base] = 1\n",
    "    print('Column',col_base,'added/modified. Number of columns:',data.shape[1])\n",
    "else:\n",
    "    print('Column',col_base,'already exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the month and day column from the time column is doing the following\n",
    "- take the time column and tell in which format the time is saved in - **you need to specify this in variable *dtime_input_format*** (see https://docs.python.org/3/library/time.html#time.strftime for reference)\n",
    "- strip the format just to year, month, day string\n",
    "- convert the string to number\n",
    "- the new column will be added to the dataset as day\n",
    "- truncate this column to just year and month and add it to dataset as month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtime_input_format = '%Y-%m-%d %H:%M:%S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,col_day] = pd.to_numeric(pd.to_datetime(data[col_time], format=dtime_input_format).dt.strftime('%Y%m%d'))\n",
    "data[col_month] = data[col_day].apply(lambda x: math.trunc(x/100))\n",
    "print('Columns',col_day,'and',col_month,'added/modified. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the predictors list from a csv file. The csv should have just one column, without any header, containing the name of the variables that should be used as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_pred = list(pd.read_csv(r'ExamplePredList.CSV', sep = ',', decimal = '.', \n",
    "                   encoding = 'windows-1251', low_memory = False, header = None)[0])\n",
    "\n",
    "cols_pred_cat = list(set([c[0] for c in list(zip(data.columns, data.dtypes)) if c[1]=='O']) & set(cols_pred))\n",
    "cols_pred_num = list(set([c[0] for c in list(zip(data.columns, data.dtypes)) if c[1]!='O']) & set(cols_pred))\n",
    "\n",
    "# ALTERNATIVELY, DEFINE THE PREDICTOR NAMES MANUALLY\n",
    "\n",
    "#cols_pred_num = [\"Numerical_1\",\"Numerical_2\",\"Numerical_3\",\"Numerical_4\",\"Numerical_5\"]\n",
    "#cols_pred_cat = [\"Categorical_1\",\"Categorical_2\",\"Categorical_3\",\"Categorical_4\",\"Categorical_5\"]\n",
    "\n",
    "\n",
    "cols_pred = cols_pred_num + cols_pred_cat\n",
    "\n",
    "print(len(cols_pred_num),'numerical predictors:')\n",
    "for p in cols_pred_num: print(p)\n",
    "print('-'*100)\n",
    "print()\n",
    "print(len(cols_pred_cat),'categorical predictors:')\n",
    "for p in cols_pred_cat: print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descrip = data.describe(include='all').transpose()\n",
    "pd.options.display.max_rows = 1000\n",
    "display(descrip)\n",
    "pd.options.display.max_rows = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**exploreNominal** and **exploreInterval** functions give graphical data exploratory analyses. They can also output even more comprehensive analysis into html files. You just need to specify the folder for output.\n",
    "\n",
    "These functions analyze only the part of data where target is not null even if it is not explicitly specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.data_exploration import exploreNominal, exploreInterval\n",
    "\n",
    "for c in sorted(cols_pred_num):\n",
    "    if (data[c].count() > 0) and (data[c].max() != data[c].min()):\n",
    "        exploreInterval(data[c],data[col_target],htmlOut=True,ntbOut=True,OutFolder2='dexp',bin_count=10)\n",
    "\n",
    "for c in sorted(cols_pred_cat):\n",
    "    if (data[c].count() > 0) and (len(list(set(data[c].unique()) - {np.nan})) > 1):\n",
    "        exploreNominal(data[c],data[col_target],htmlOut=True,ntbOut=True,OutFolder2='dexp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**explore_df** function creates a simple text report about the important variable. The report can be then printed either to the screen or to a file.\n",
    "\n",
    "In the following code, only such part of data that has col_base = 1 is analyzed. You can remove the condition if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.data_exploration import explore_df\n",
    "st = explore_df(data[data[col_base]==1],col_month,col_target,cols_pred)\n",
    "print(st,file=open(\"data_exp.txt\", \"w\", encoding='utf-8'))\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default rate in time**: Simple visualisation of observation count and default rate in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import plot_dataset\n",
    "plot_dataset(data,col_month,col_target,'Count and bad rate',col_base,savepath=output_folder+'/analysis/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "\n",
    "- Split data into five parts (in time training, in time validation, in time test, out of time, historical out of time).\n",
    "- Adds a new column indicating to which part the observations belong.\n",
    "- The split parameters are set at the beginning of the code.\n",
    "- In the first line you can set the random seed so the results are replicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(12345)\n",
    "share_train = 0.6\n",
    "share_validation = 0.2\n",
    "first_train_day = 20170201 #first day of train, everything before it will be considered \"old\", historical out of time\n",
    "first_oot_day = 20170601 #first day of \"new\" out of time, i.e. out of time after train\n",
    "\n",
    "data['random_value'] = 1\n",
    "data['random_value'] = data['random_value'].apply(lambda x: random.uniform(0, 1)) \n",
    "\n",
    "data.loc[(data['random_value']<=share_train)&(data[col_day]<first_oot_day)&\n",
    "         (data[col_day]>=first_train_day),'data_type'] = 'train'\n",
    "data.loc[(data['random_value']>share_train)&(data['random_value']<=share_train+share_validation)&(data[col_day]<first_oot_day)&\n",
    "         (data[col_day]>=first_train_day),'data_type'] = 'valid'\n",
    "data.loc[(data['random_value']>share_train+share_validation)&(data[col_day]<first_oot_day)&\n",
    "         (data[col_day]>=first_train_day),'data_type'] = 'test'\n",
    "data.loc[(data[col_day]>=first_oot_day),'data_type'] = 'oot'\n",
    "data.loc[(data[col_day]<first_train_day),'data_type'] = 'hoot'\n",
    "\n",
    "data= data.drop(['random_value'],axis = 1)\n",
    "\n",
    "train_mask = (data.data_type == 'train')& (data[col_base] == 1) \n",
    "valid_mask = (data.data_type == 'valid')& (data[col_base] == 1) \n",
    "test_mask = (data.data_type == 'test')& (data[col_base] == 1) \n",
    "oot_mask = (data.data_type == 'oot')& (data[col_base] == 1) \n",
    "hoot_mask = (data.data_type == 'hoot')& (data[col_base] == 1) \n",
    "\n",
    "print('Train observations:',data[train_mask].shape[0])\n",
    "print('Validation observations:',data[valid_mask].shape[0])\n",
    "print('Test observations:',data[test_mask].shape[0])\n",
    "print('Out-of-time observations:',data[oot_mask].shape[0])\n",
    "print('Historical-out-of-time observations:',data[hoot_mask].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data summary (number of defaults, number in base, number of observations, default rate) by month and by sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary = data.groupby([col_month,'data_type']).aggregate({\n",
    "    col_target:'sum',col_base:['sum','count']\n",
    "})\n",
    "data_summary.columns = [col_target,col_base,'Rows']\n",
    "data_summary[col_target+' rate'] = data_summary[col_target]/data_summary[col_base]\n",
    "\n",
    "data_summary = data_summary.reset_index(level='data_type').pivot(columns='data_type')\n",
    "display(data_summary)\n",
    "data_summary.to_csv(output_folder+'/analysis/summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and WOE transformation of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't use such variables which have only 0 or 1 unique level. Grouping don't work for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_del = set()\n",
    "for c in cols_pred_num:\n",
    "    if (data[train_mask][c].count() == 0) or (data[train_mask][c].max() == data[train_mask][c].min()):\n",
    "        cols_del = cols_del | {c}\n",
    "        cols_pred_num = list(set(cols_pred_num) - {c})\n",
    "for c in cols_pred_cat:\n",
    "    if (data[train_mask][c].count() == 0) or (len(list(set(data[train_mask][c].unique()) - {np.nan})) <= 1):\n",
    "        cols_del = cols_del | {c}\n",
    "        cols_pred_cat = list(set(cols_pred_cat) - {c})\n",
    "            \n",
    "cols_pred = cols_pred_num + cols_pred_cat\n",
    "\n",
    "if len(list(cols_del)) > 0:\n",
    "    print('Variables',cols_del,'will not be further used as they have only 1 unique level.')\n",
    "else:\n",
    "    print('All predictors have more than 1 unique level.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two options how to group your variables. \n",
    "1. Automatic grouping groups the variables using a decision tree. User can't change the grouping in any interactive way. The grouping can be saved into external file using its method *save()*. \n",
    "2. Interactive grouping is suitable for smaller numbers of variables. User can control which values of each varible will enter which group. The grouping can be saved into external file using the interactive environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Automatic Grouping\n",
    "The grouping uses decision tree algorithm and the grouping is supervised based on the target variable. In the following code:\n",
    "\n",
    "A new instance of **Grouping** class is created. There are two important parameters:\n",
    " - *colums*: list of numerical columns to be grouped\n",
    " - *cat_columns*: list of categorical columns to be grouped\n",
    " - *group_count*: (maximal) number of final groups of each variable\n",
    " - *min_samples*: minimal number of observations in each group of each numerical variable\n",
    " - *min_samples_cat*: minimal number of observations in each group of each categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.grouping import Grouping\n",
    "\n",
    "grouping = Grouping(columns = sorted(cols_pred_num),\n",
    "                    cat_columns = sorted(cols_pred_cat),\n",
    "                    group_count=5, \n",
    "                    min_samples=100, \n",
    "                    min_samples_cat=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you fit the grouping using **fit** method. The parameters are array of the predictors and series of the target. Grouping is fitted on training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping.fit(data[train_mask][cols_pred],\n",
    "             data[train_mask][col_target])\n",
    "\n",
    "if len(grouping.bins_data_) > 0:\n",
    "    for v,g in grouping.bins_data_.items():\n",
    "        print('Variable:',v)\n",
    "        print('Bins:',g['bins'])\n",
    "        print('WOEs:',g['woes'])\n",
    "        if v in cols_pred_num:\n",
    "            print('nan WOE:',g['nan_woe'])\n",
    "        if v in cols_pred_cat:\n",
    "            print('WOE for unknown values:',g['unknown_woe'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you apply the grouping on your full data set using the **transform** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_woe = grouping.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save grouping to an external file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'myGrouping'\n",
    "grouping.save(model_filename)\n",
    "print('Grouping data saved to file',model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Interactive Grouping (beta)\n",
    "\n",
    "A new instance of **InteractiveGrouping** class is created. There are two important parameters:\n",
    " - *colums*: list of numerical columns to be grouped\n",
    " - *cat_columns*: list of categorical columns to be grouped\n",
    " - *group_count*: (maximal) number of final groups of each variable\n",
    " - *min_samples*: minimal number of observations in each group of each numerical variable\n",
    " - *min_samples_cat*: minimal number of observations in each group of each categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import importlib\n",
    "#importlib.reload(scoring)\n",
    "#importlib.reload(scoring.grouping)\n",
    "\n",
    "from scoring.grouping import Grouping, InteractiveGrouping\n",
    "\n",
    "grouping = InteractiveGrouping(columns = sorted(cols_pred_num),\n",
    "                               cat_columns = sorted(cols_pred_cat),\n",
    "                               group_count=5,\n",
    "                               min_samples=100, \n",
    "                               min_samples_cat=100,\n",
    "                               woe_smooth_coef=0.001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you open the interactive environment using **display** method. The important parameters are:\n",
    " - *train_t*: training dataset the grouping should be based on\n",
    " - *colums*: list of numerical columns to be grouped and displayed\n",
    " - *cat_columns*: list of categorical columns to be grouped and displayed\n",
    " - *target_column*: as the grouping is supervised and calculates WOE values, you need to specify the target column name\n",
    " - *filename*: use only if you want to load a grouping that you created and saved previously\n",
    " - *group_count*: (maximal) number of final groups of each variable\n",
    " - *min_samples*: minimal number of observations in each group of each numerical variable\n",
    " - *min_samples_cat*: minimal number of observations in each group of each categorical variable\n",
    "\n",
    "In the interactive environment, you can see four sections. From top to bottom:\n",
    "- **Chart section**: \n",
    " - For **numerical variables**, there is chart with equifrequncy fine classing (observations as bars, default rate as line), equidistant fine classing and the final groups.\n",
    " - For **categorical varibles** there is chart with each of the original categorical values and a chart with the final groups.\n",
    "- **Variable section**: here you can choose tab with varible which you want to edit. \n",
    " - For **numerical variables**, the tab contains of the borders of the final groups. You can edit these borders, add new with [+] button and remove them with [-] button. You can also manually set WOE for nulls. There is also a button to perform automatic grouping on the selected variable.\n",
    " - For **categorical variables**, the tab contains of two tables. In the top table, you can see some statistics for each of the categorical values. In the rightmost column, there is the number of group which is assigned to the category. You can edit this value (doubleclick on it) to change the grouping. In the bottom table you can see statistics for the groups. It is not editable. There is also a button to perform automatic grouping on the selected variable.\n",
    "- **Save section**: here you can save the grouping. Edit the file name and click the [Apply and Save] button.\n",
    "- **Settings section**: If you perform automatic grouping on some varible, the grouping algorithm uses some parameters. These parameters can be set here. You can set how many final groups do you want to have and what is their minimal size.\n",
    "\n",
    "Known bugs:\n",
    "- This bug may occur only if there are any categorical variables. In such case, user must open at least 1 tab with such variable before saving the grouping (otherwise there will be an error during the saving).\n",
    "- If zoom level in the web browser is set to something else than 100%, the charts might get broken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_orig()\n",
    "%matplotlib notebook\n",
    "%config InlineBackend.close_figures=False\n",
    "\n",
    "grouping.display(train_t = data[train_mask][cols_pred_num+cols_pred_cat+[col_target]],\n",
    "                 columns = sorted(cols_pred_num),\n",
    "                 cat_columns = sorted(cols_pred_cat),\n",
    "                 target_column = col_target,\n",
    "                 #filename = 'myIntGrouping',\n",
    "                 bin_count=20,\n",
    "                 woe_smooth_coef=0.001,\n",
    "                 group_count=5,\n",
    "                 min_samples=100,\n",
    "                 min_samples_cat=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the graphical environment to be used by the normal non-interactive charts\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.close_figures=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to apply the grouping to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_woe = grouping.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the grouping on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the grouping from a file (don't forget to set the right filename) and add the WOE columns to the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scoring.grouping import Grouping\n",
    "#grouping = Grouping(columns = sorted(cols_pred_num),\n",
    "#                    cat_columns = sorted(cols_pred_cat),\n",
    "#                    group_count=5, \n",
    "#                    min_samples=100, \n",
    "#                    min_samples_cat=100) \n",
    "#g_filename = 'myIntGrouping'\n",
    "#grouping.load(g_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fitted WOEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import print_binning_stats_num, print_binning_stats_cat\n",
    "\n",
    "if len(grouping.bins_data_) > 0:\n",
    "    for v,g in sorted(grouping.bins_data_.items(), key=operator.itemgetter(0)):\n",
    "        print('-'*125)\n",
    "        print(v)\n",
    "        if v in cols_pred_num:\n",
    "            print_binning_stats_num(data[train_mask][[col_target, v]], v, col_target, g['bins'], g['woes'], g['nan_woe']\n",
    "                           ,savepath=output_folder+'/predictors/'+v+'_')  \n",
    "        elif v in cols_pred_cat:\n",
    "            print_binning_stats_cat(data[train_mask][[col_target, v]], v, col_target\n",
    "                           ,g['bins'].keys(), g['bins'].values() ,g['woes'], g['unknown_woe']\n",
    "                           ,savepath=output_folder+'/predictors/'+v+'_')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add WOE variabes to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_woe = grouping.transform(data)\n",
    "for c in data_woe:\n",
    "    if c+'_WOE' in data:\n",
    "        data = data.drop(c+'_WOE', 1)\n",
    "        print('Column',c+'_WOE','dropped as it already existed in the data set.')\n",
    "data = data.join(data_woe,rsuffix='_WOE')\n",
    "print('Added WOE variables. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor power analysis\n",
    "\n",
    "Calculates IV and Gini of each predictor, sorts the predictors by their power. The power is calculated for each of the samples (train, validate, test, OOT, H.OOT). **If one or more of the samples are empty, comment the according part of the code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_woe = [s + '_WOE' for s in cols_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.metrics import iv,gini,lift\n",
    "\n",
    "power_tab = []\n",
    "for j in range(0,len(cols_woe)):\n",
    "    power_tab.append({'Name':cols_woe[j]\n",
    "                    ,'IV Train':iv(data.loc[train_mask,col_target],data.loc[train_mask,cols_woe[j]])\n",
    "                    ,'Gini Train':gini(data.loc[train_mask,col_target],-data.loc[train_mask,cols_woe[j]])\n",
    "                    ,'IV Validate':iv(data.loc[valid_mask,col_target],data.loc[valid_mask,cols_woe[j]])\n",
    "                    ,'Gini Validate':gini(data.loc[valid_mask,col_target],-data.loc[valid_mask,cols_woe[j]])\n",
    "                    ,'IV Test':iv(data.loc[test_mask,col_target],data.loc[test_mask,cols_woe[j]])\n",
    "                    ,'Gini Test':gini(data.loc[test_mask,col_target],-data.loc[test_mask,cols_woe[j]])\n",
    "                    ,'IV OOT':iv(data.loc[oot_mask,col_target],data.loc[oot_mask,cols_woe[j]])\n",
    "                    ,'Gini OOT':gini(data.loc[oot_mask,col_target],-data.loc[oot_mask,cols_woe[j]])\n",
    "                    ,'IV HOOT':iv(data.loc[hoot_mask,col_target],data.loc[hoot_mask,cols_woe[j]])\n",
    "                    ,'Gini HOOT':gini(data.loc[hoot_mask,col_target],-data.loc[hoot_mask,cols_woe[j]])\n",
    "                         })\n",
    "power_out = pd.DataFrame.from_records(power_tab)\n",
    "power_out = power_out.set_index('Name')\n",
    "power_out = power_out.sort_values('Gini Train',ascending=False)\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "display(power_out)\n",
    "pd.options.display.max_rows = 15\n",
    "power_out.to_csv(output_folder+'/predictors/covariates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable clustering\n",
    "\n",
    "Show correlation matrix of all the WOE variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cormat_full = data[sorted(cols_woe)].fillna(0).corr()\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 15})\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.close_figures=True\n",
    "a4_dims = (12,10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=a4_dims, dpi=50)\n",
    "fig.suptitle('Correlations of Variables',fontsize=25)\n",
    "sns.heatmap(cormat_full, ax=ax, annot=True, fmt=\"0.1f\", linewidths=.5, annot_kws={\"size\":15},cmap=\"OrRd\")\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.savefig(output_folder+'/analysis/correlation_full.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Hierarchical variable clustering based on correlations.\n",
    "- Starts with each variable as a separate cluster\n",
    "- Creates clusters based on highest average correlations\n",
    "- The stopping criterion is parameter *max_cluster_correlation* - once no correlation between clusters is larger than this parameter, the clustering is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "max_cluster_correlation = 0.5\n",
    "\n",
    "\n",
    "if len(data[train_mask][cols_woe]) > 50000:\n",
    "    data_for_clustering = data[train_mask][cols_woe].sample(50000)\n",
    "else:\n",
    "    data_for_clustering = data[train_mask][cols_woe]\n",
    "\n",
    "Z = linkage(data_for_clustering.fillna(0).transpose(), method='average', metric='correlation')\n",
    "clusters = fcluster(Z, 1-max_cluster_correlation, criterion='distance')\n",
    "\n",
    "a4_dims = (10, int(len(clusters)/4))\n",
    "fig, ax = plt.subplots(figsize=a4_dims, dpi=50)\n",
    "dendrogram(Z, labels=[a+': '+str(b) for a,b in zip(cols_woe,list(clusters))], orientation='right')\n",
    "plt.axvline(x=1-max_cluster_correlation, c='k')\n",
    "plt.xlabel('correlation')\n",
    "plt.xticks([0,0.2,0.4,0.6,0.8,1],[1,0.8,0.6,0.4,0.2,0])\n",
    "plt.savefig(output_folder+'/analysis/clustering_dendrogram.png', bbox_inches='tight', dpi = 72)\n",
    "plt.savefig(output_folder+'/analysis/clustering_dendrogram_full.png', bbox_inches='tight', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table with all the clusters. Subset of WOE variables called *vars_woe_restr* is created which contains only the strongest (based on training Gini) variable of each cluster. It's up to the user to choose whether they want to used the full set (in this workflow by default) or such restricted set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_variables = pd.DataFrame({'Name':cols_woe,'Cluster':list(clusters)},index=cols_woe).join(power_out)\n",
    "clustered_variables = clustered_variables.sort_values(['Cluster','Gini Train'],ascending=[True,False])[[\n",
    "    'Cluster','Gini Train','IV Train']]\n",
    "clustered_variables['Order in cluster'] = clustered_variables.sort_values('Gini Train', ascending=False).groupby('Cluster')\\\n",
    "             .cumcount() + 1\n",
    "pd.options.display.max_rows = 1000\n",
    "display(clustered_variables)\n",
    "pd.options.display.max_rows = 15\n",
    "clustered_variables.to_csv(output_folder+'/predictors/predictor_clusters.csv')\n",
    "\n",
    "vars_woe_restr = list(clustered_variables[clustered_variables['Order in cluster']==1].index)\n",
    "print('Restricted set of WOE variables:',vars_woe_restr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: k-means clustering with given k\n",
    "\n",
    "Performs k-means clustering. The value of parameter *k* is set by the user.\n",
    "\n",
    "This option is better performance-wise, but might give less interpretable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_param = 7\n",
    "\n",
    "\n",
    "if len(data[train_mask][cols_woe]) > 50000:\n",
    "    data_for_clustering = data[train_mask][cols_woe].sample(50000)\n",
    "else:\n",
    "    data_for_clustering = data[train_mask][cols_woe]\n",
    "    \n",
    "km = KMeans(n_clusters = k_param)\n",
    "km.fit(data_for_clustering.fillna(0).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table with all the clusters. Subset of WOE variables called *vars_woe_restr* is created which contains only the strongest (based on training Gini) variable of each cluster. It's up to the user to choose whether they want to used the full set (in this workflow by default) or such restricted set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_variables = pd.DataFrame({'Name':cols_woe,'Cluster':list(km.labels_)},index=cols_woe).join(power_out)\n",
    "clustered_variables = clustered_variables.sort_values(['Cluster','Gini Train'],ascending=[True,False])[[\n",
    "    'Cluster','Gini Train','IV Train']]\n",
    "clustered_variables['Order in cluster'] = clustered_variables.sort_values('Gini Train', ascending=False).groupby('Cluster')\\\n",
    "             .cumcount() + 1\n",
    "pd.options.display.max_rows = 1000\n",
    "display(clustered_variables)\n",
    "pd.options.display.max_rows = 15\n",
    "clustered_variables.to_csv(output_folder+'/predictors/predictor_clusters.csv')\n",
    "\n",
    "vars_woe_restr = list(clustered_variables[clustered_variables['Order in cluster']==1].index)\n",
    "print('Restricted set of WOE variables:',vars_woe_restr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scorecard estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) L1 regularized Logistic Regression\n",
    "\n",
    "Efficient way how to select subset of predictors from a very big set of covariate. Uses grid search through value of L1 regularization parameter. We start with no predictor in the model and try to add predictors from list called **cols_shortlist** which is defined below (by default, we put there all the WOE variables). The best model selected based on validation Gini.\n",
    "\n",
    "Interation process can be tuned using various parameters:\n",
    " - *steps*: number of steps of grid search\n",
    " - *grid_length*: length of the grid for grid search\n",
    " - *max_predictors*: maximal number of predictors to enter the model. Ignored if set to 0.\n",
    " - *max_correlation*: maximal absolute value of correlation of predictors in the model (variable with larger correlation with existing predictors will not be added to the model)\n",
    " - *beta_sgn_criterion*: if this is set to True, all the betas in the model must have the same signature (all positive or all negative)\n",
    " - *stop_immediately*: the iteration process will be stopped immediately after a model which is not fulfilling the criteria (max_predictors, max_correlation or beta_sgn_criterion) is found. No further models are searched for.\n",
    " - *correlation_sample*: for better performance, correlation matrix is calculated just on a sample of data. The size of the sample is set in this parameter\n",
    " \n",
    "The *fit* method can be called with two arguments *fit(X,y)* or with four agruments *fit(X_train,y_train,X_valid,y_valid)*. When called with four arguments, the Gini is measured on the validation sample (i.e. validation sample is used for decisions about what steps to be done in stepwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a shortlist of predictors to enter the modelling in the next steps.\n",
    "cols_shortlist = cols_woe\n",
    "#cols_shortlist = list(set(cols_woe) - set(['unwanted1','unwanted2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.model_selection import L1GiniModelSelection\n",
    "\n",
    "modelL1 = L1GiniModelSelection(steps = 100, grid_length=5, max_predictors=200,\n",
    "                           max_correlation=1, beta_sgn_criterion=False, stop_immediately=False, correlation_sample = 10000)\n",
    "\n",
    "modelL1.fit(data[train_mask][cols_shortlist],data[train_mask][col_target],\n",
    "        data[valid_mask][cols_shortlist],data[valid_mask][col_target]\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_ = np.array(modelL1.coefs_)\n",
    "cs = modelL1.model_progress_['C']\n",
    "plt.figure(figsize = (7,7))\n",
    "plt.plot(np.log10(cs), coefs_) \n",
    "ymin, ymax = plt.ylim()\n",
    "plt.xlabel('log10(C)')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Logistic Regression Path')\n",
    "plt.axis('tight')\n",
    "plt.legend(cols_shortlist, loc='upper center', bbox_to_anchor=(1.20,1.0))\n",
    "plt.savefig(output_folder+'/model/l1path.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,7))\n",
    "ginis = modelL1.model_progress_[['gini train','gini validate']]\n",
    "plt.plot(np.log10(cs), ginis)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.xlabel('log10(C)')\n",
    "plt.ylabel('Ginis')\n",
    "plt.title('Logistic Regression Path')\n",
    "plt.axis('tight')\n",
    "plt.legend(['Train','Validate'], loc='upper center', bbox_to_anchor=(1.20,1.0))\n",
    "plt.savefig(output_folder+'/model/l1gini.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predictors in the model:',list(modelL1.final_predictors_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename1 = 'myModelL1'\n",
    "pickle.dump(modelL1, open(model_filename1, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_filename1 = 'myModelL1'\n",
    "#modelL1 = pickle.load(open(model_filename1, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drawback of regularized model is that it is not calibrated, so it must be refitted afterwards. In this workflow, there is stepwise regression after this L1 regression which can serve this purpose (i.e. fitting model with the same set or subset of predictors, but without the regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Stepwise logistic Regression\n",
    "\n",
    "We run stepwise logistic regression on training data set. We start with no predictor in the model and try to add predictors from list called **cols_shortlist2** which is defined below (by default, we put there all the WOE variables).\n",
    "\n",
    "Stepwise process can be tuned using various parameters:\n",
    " - *initial_predictors*: set of starting predictors (useful for backward method)\n",
    " - *max_iter*: maximal number of iterations\n",
    " - *min_increase*: minimal marginal Gini contribution for predictor to be added\n",
    " - *max_decrease*: minimal marginal Gini diminution for predictor to be removed\n",
    " - *max_predictors*: maximal number of predictors to enter the model. Ignored if set to 0.\n",
    " - *max_correlation*: maximal absolute value of correlation of predictors in the model (variable with larger correlation with existing predictors will not be added to the model). **This parameter works for \"forward\" selection method only.**\n",
    " - *beta_sgn_criterion*: if this is set to True, all the betas in the model must have the same signature (all positive or all negative). **This parameter works for \"forward\" selection method only.**\n",
    " - *penalty, C*: regularization parameters for logitic regression (sklearn library)\n",
    " - *correlation_sample*: for better performance, correlation matrix is calculated just on a sample of data. The size of the sample is set in this parameter\n",
    " - *selection_method*: stepwise or forward or backward\n",
    " \n",
    "The *fit* method can be called with two arguments *fit(X,y)* or with four agruments *fit(X_train,y_train,X_valid,y_valid)*. When called with four arguments, the Gini is measured on the validation sample (i.e. validation sample is used for decisions about what steps to be done in stepwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the output from L1 model as a shortlist for the next step\n",
    "cols_shortlist2 = list(modelL1.final_predictors_)\n",
    "#cols_shortlist2 = cols_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.model_selection import GiniStepwiseLogit\n",
    "\n",
    "modelSW = GiniStepwiseLogit(initial_predictors = set(), max_iter=1000, min_increase=0.8, max_decrease=0.5,\n",
    "                    max_predictors=0, max_correlation=0.45, beta_sgn_criterion=False, \n",
    "                    penalty='l2', C=10e10, correlation_sample=10000,\n",
    "                    selection_method='stepwise')\n",
    "\n",
    "modelSW.fit(data[train_mask][cols_shortlist2],data[train_mask][col_target]\n",
    "        ,data[valid_mask][cols_shortlist2],data[valid_mask][col_target]\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = range(0,len(modelSW.model_progress_[modelSW.model_progress_['addrm']==0]['prednum']))\n",
    "pn = modelSW.model_progress_[modelSW.model_progress_['addrm']==0]['prednum']\n",
    "ginis = modelSW.model_progress_[modelSW.model_progress_['addrm']==0]['Gini']\n",
    "plt.figure(figsize = (7,7))\n",
    "plt.plot(it, ginis)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Gini')\n",
    "plt.title('Stepwise model selection')\n",
    "plt.axis('tight')\n",
    "plt.savefig(output_folder+'/model/stepwisegini.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename2 = 'myModelSW'\n",
    "pickle.dump(modelSW, open(model_filename2, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_filename2 = 'myModelSW'\n",
    "#modelSW = pickle.load(open(model_filename2, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predictors in the model:',list(modelSW.final_predictors_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the dataset\n",
    "First choose which model is your final model (into variable *clf*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = modelL1\n",
    "clf = modelSW\n",
    "\n",
    "cols_final_predictors = list(clf.final_predictors_)\n",
    "pd.DataFrame(cols_final_predictors).to_csv(output_folder+'/predictors/predictors.csv',index=False,header=None)\n",
    "\n",
    "print('FINAL MODEL COEFFICIENTS')\n",
    "print('Intercept:',clf.intercept_[0])\n",
    "for p,b in zip(cols_final_predictors,list(clf.coef_[0])):\n",
    "    print(p,':',b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column with the prediction (probability of default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_score = 'SCORE'\n",
    "\n",
    "data[col_score] = clf.predict(data)\n",
    "print('Column',col_score,'with the prediction added/modified. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scorecard table output\n",
    "Output the scorecard to a table. Stats are calculated on a subset of data given by the mask defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this mask is an union of masks for training, validation, testing and out of time data sets\n",
    "table_mask = train_mask|valid_mask|test_mask|oot_mask|hoot_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard = []\n",
    "\n",
    "if len(grouping.bins_data_) > 0:\n",
    "    for v,g in grouping.bins_data_.items():\n",
    "        if v+'_WOE' in clf.final_predictors_:\n",
    "            ii = list(clf.final_predictors_).index(v+'_WOE')\n",
    "            bin_names = []\n",
    "            bin_woes = []\n",
    "            data['sctable_print_subset'] = 0\n",
    "            for j in range(0,len(g['bins'])):\n",
    "                if (v in cols_pred_num) and (j < len(g['bins'])-1):\n",
    "                    subset = data[(table_mask) & (data[v]>=g['bins'][j]) & (data[v]<g['bins'][j+1])]\n",
    "                    data.loc[(table_mask) & (data[v]>=g['bins'][j]) & (data[v]<g['bins'][j+1]),'sctable_print_subset'] = 1\n",
    "                    obs = subset[col_base].sum()\n",
    "                    bads = subset[col_target].sum()\n",
    "                    scorecard.append({'Variable':v,\n",
    "                                     'Min':g['bins'][j],\n",
    "                                     'Max':g['bins'][j+1],\n",
    "                                     'Value':np.nan,\n",
    "                                     'WOE':g['woes'][j],\n",
    "                                     'Beta':clf.coef_[0][ii],\n",
    "                                     'BiXi':g['woes'][j]*clf.coef_[0][ii],\n",
    "                                     'Observations':obs,\n",
    "                                     'Bads':bads})\n",
    "                elif (v in cols_pred_cat):\n",
    "                    if pd.isnull(list(g['bins'].keys())[j]):\n",
    "                        subset = data[(table_mask) & (pd.isnull(data[v]))]\n",
    "                        data.loc[(table_mask) & (pd.isnull(data[v])),'sctable_print_subset'] = 1\n",
    "                        val = 'null'\n",
    "                    else:\n",
    "                        subset = data[(table_mask) & (data[v]==list(g['bins'].keys())[j])]\n",
    "                        data.loc[(table_mask) & (data[v]==list(g['bins'].keys())[j]),'sctable_print_subset'] = 1\n",
    "                        val = list(g['bins'].keys())[j]\n",
    "                    obs = subset[col_base].sum()\n",
    "                    bads = subset[col_target].sum()\n",
    "                    scorecard.append({'Variable':v,\n",
    "                                     'Min':np.nan,\n",
    "                                     'Max':np.nan,\n",
    "                                     'Value':val,\n",
    "                                     'WOE':g['woes'][list(g['bins'].values())[j]],\n",
    "                                     'Beta':clf.coef_[0][ii],\n",
    "                                     'BiXi':g['woes'][list(g['bins'].values())[j]]*clf.coef_[0][ii],\n",
    "                                     'Observations':obs,\n",
    "                                     'Bads':bads})\n",
    "            if (v in cols_pred_num):\n",
    "                subset = data[(table_mask) & (pd.isnull(data[v]))]\n",
    "                data.loc[(table_mask) & (pd.isnull(data[v])),'sctable_print_subset'] = 1\n",
    "                obs = subset[col_base].sum()\n",
    "                bads = subset[col_target].sum()\n",
    "                scorecard.append({'Variable':v,\n",
    "                                 'Min':np.nan,\n",
    "                                 'Max':np.nan,\n",
    "                                 'Value':'null',\n",
    "                                 'WOE':g['nan_woe'],\n",
    "                                 'Beta':clf.coef_[0][ii],\n",
    "                                 'BiXi':g['nan_woe']*clf.coef_[0][ii],\n",
    "                                 'Observations':obs,\n",
    "                                 'Bads':bads})\n",
    "            elif (v in cols_pred_cat):\n",
    "                subset = data[(table_mask) & data['sctable_print_subset']==0]\n",
    "                obs = subset[col_base].sum()\n",
    "                bads = subset[col_target].sum()\n",
    "                scorecard.append({'Variable':v,\n",
    "                                 'Min':np.nan,\n",
    "                                 'Max':np.nan,\n",
    "                                 'Value':'else',\n",
    "                                 'WOE':g['unknown_woe'],\n",
    "                                 'Beta':clf.coef_[0][ii],\n",
    "                                 'BiXi':g['unknown_woe']*clf.coef_[0][ii],\n",
    "                                 'Observations':obs,\n",
    "                                 'Bads':bads})\n",
    "\n",
    "all_obs = data[table_mask][col_base].sum()\n",
    "all_bads = data[table_mask][col_target].sum() \n",
    "scorecard.append({'Variable':'_Intercept',\n",
    "                  'Value':np.nan,\n",
    "                  'Min':np.nan,\n",
    "                  'Max':np.nan,\n",
    "                  'WOE':1,\n",
    "                  'Beta':clf.intercept_[0],\n",
    "                  'BiXi':1*clf.intercept_[0],\n",
    "                  'Observations':all_obs,\n",
    "                  'Bads':all_bads})\n",
    "\n",
    "data.drop(columns=['sctable_print_subset'],inplace=True)\n",
    "\n",
    "scorecard_out = pd.DataFrame.from_records(scorecard)[\n",
    "    ['Variable','Min','Max','Value','WOE','Beta','BiXi','Observations','Bads']]\n",
    "scorecard_out2 = scorecard_out.copy()\n",
    "scorecard_out2['Value'] = scorecard_out2['Value'] + ','\n",
    "scorecard_out2 = scorecard_out2.groupby(['Variable','WOE']).agg({\n",
    "    'Variable':min,'Min':min,'Max':max,'Value':sum,'WOE':min,'Beta':min,'BiXi':min,'Observations':sum,'Bads':sum\n",
    "})\n",
    "scorecard_out2.loc[pd.isnull(scorecard_out2['Value']),'Value'] = ','\n",
    "scorecard_out2['Value'] = scorecard_out2['Value'].astype(str).str[:-1]\n",
    "scorecard_out2['Goods'] = scorecard_out2['Observations'] - scorecard_out2['Bads']\n",
    "scorecard_out2['Bad Rate'] = scorecard_out2['Bads']/scorecard_out2['Observations']\n",
    "all_badrate = all_bads/all_obs\n",
    "scorecard_out2['Bad Rate relative to population'] = scorecard_out2['Bad Rate'] / all_badrate\n",
    "scorecard_out2['% Observations'] = scorecard_out2['Observations'] / all_obs\n",
    "scorecard_out2['% Bads'] = scorecard_out2['Bads'] / all_bads\n",
    "scorecard_out2['% Goods'] = scorecard_out2['Goods'] / (all_obs-all_bads)\n",
    "scorecard_out2['Lift'] = scorecard_out2['% Bads'] / scorecard_out2['% Goods']\n",
    "scorecard_out2 = pd.DataFrame.from_records(scorecard_out2.sort_values(['Variable','Min','Max','WOE','Value']))\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "#display(scorecard_out)\n",
    "display(scorecard_out2)\n",
    "pd.options.display.max_rows = 15\n",
    "scorecard_out2.to_csv(output_folder+'/model/scorecard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scorecard export as SQL query, Blaze table and Python code\n",
    "\n",
    "Generate SQL code to run the scorecard on Oracle DWH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTER PART TRANSFORMING WOE TO BIXI\n",
    "scoring_sql_outer = ['select\\n1/(1+exp(-s.LINEAR_SCORE)) as SCORE,\\ns.*\\nfrom (\\n    select\\n']\n",
    "#INNER PART TRANSFORMING VARIABLE TO WOE\n",
    "scoring_sql_inner = ['        select\\n']\n",
    "nullWOE = None\n",
    "elseWOE = None\n",
    "tmp_variable = ''\n",
    "for r in scorecard_out.itertuples():\n",
    "    if r.Variable != tmp_variable:\n",
    "        if tmp_variable != '':\n",
    "            #OUTER PART TRANSFORMING WOE TO BIXI\n",
    "            scoring_sql_outer.append('     + ')\n",
    "            #INNER PART TRANSFORMING VARIABLE TO WOE\n",
    "            if elseWOE is None: elseWOE = nullWOE\n",
    "            if elseWOE is None: elseWOE = 0\n",
    "            scoring_sql_inner.append('            else ' + str(elseWOE) + '\\n        end as ' + str(tmp_variable) + '_WOE,\\n')\n",
    "        else:\n",
    "            #OUTER PART TRANSFORMING WOE TO BIXI\n",
    "            scoring_sql_outer.append('    ')\n",
    "        #OUTER PART TRANSFORMING WOE TO BIXI\n",
    "        scoring_sql_outer.append('w.' + str(r.Variable) + '_WOE * ' + str(r.Beta) + '\\n')\n",
    "        #INNER PART TRANSFORMING VARIABLE TO WOE\n",
    "        scoring_sql_inner.append('        case\\n')\n",
    "        tmp_variable = r.Variable\n",
    "        nullWOE = None\n",
    "        elseWOE = None\n",
    "    if r.Value == 'null':\n",
    "        scoring_sql_inner.append('            when ' + str(r.Variable) + ' is null then ' + str(r.WOE) + '\\n')\n",
    "        nullWOE = r.WOE\n",
    "    elif r.Value == 'else':\n",
    "        elseWOE = r.WOE\n",
    "    elif pd.notnull(r.Value):\n",
    "        scoring_sql_inner.append('            when ' + str(r.Variable) + ' = \"' + str(r.Value) + '\" then ' + str(r.WOE) + '\\n')\n",
    "    elif pd.notnull(r.Min):\n",
    "        if np.isfinite(r.Max):\n",
    "            scoring_sql_inner.append('            when ' + str(r.Variable) + ' < ' + str(r.Max) + ' then ' + str(r.WOE) + '\\n')\n",
    "        else:\n",
    "            scoring_sql_inner.append('            when ' + str(r.Variable) + ' >= ' + str(r.Min) + ' then ' + str(r.WOE) + '\\n')\n",
    "    elif r.Variable == '_Intercept':\n",
    "        scoring_sql_inner.append('            when 1=1 then ' + str(r.WOE) + '\\n')\n",
    "#OUTER PART TRANSFORMING WOE TO BIXI\n",
    "scoring_sql_outer.append('    as LINEAR_SCORE,\\n    w.*\\n    from (\\n')\n",
    "#INNER PART TRANSFORMING VARIABLE TO WOE\n",
    "if elseWOE is None: elseWOE = nullWOE\n",
    "if elseWOE is None: elseWOE = 0\n",
    "scoring_sql_inner.append('            else ' + str(elseWOE) + '\\n        end as ' + str(tmp_variable) + '_WOE\\n')\n",
    "scoring_sql_inner.append('        from _SOURCETABLENAME_\\n')\n",
    "scoring_sql_outer = ''.join(scoring_sql_outer)\n",
    "scoring_sql_inner = ''.join(scoring_sql_inner)\n",
    "scoring_sql_final = scoring_sql_outer + scoring_sql_inner + '    ) w\\n) s'\n",
    "scoring_sql_final = scoring_sql_final.replace('\"',\"'\").replace('_Intercept','Intercept')\n",
    "print(scoring_sql_final,file=open(output_folder+'/model/scorecard.sql', \"w\", encoding='utf-8'))\n",
    "print(scoring_sql_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate table for easy import to Blaze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balze_table =  pd.DataFrame(columns = ['Characteristic','Bin','Label','Score','ScorePoints',\n",
    "                        'Range','Formula','Number_of_Values','Value1','Value2'])\n",
    "\n",
    "for r in scorecard_out.itertuples():  \n",
    "    if (r.Variable=='_Intercept'):\n",
    "        balze_table=balze_table.append(pd.DataFrame( data= [['Intercept',1,'K00_All',\n",
    "                                                         r.BiXi,0,1,'Intercept=\\'integer\\'',\n",
    "                                                         0,'','']],\n",
    "                                   columns = ['Characteristic','Bin','Label','Score','ScorePoints',\n",
    "                                                  'Range','Formula','Number_of_Values','Value1','Value2']))\n",
    "        balze_table=balze_table.append(pd.DataFrame( data= [['Intercept',2,'All Other',\n",
    "                                                         0,0,1,'Intercept=\\'integer\\'',\n",
    "                                                         0,'','']],\n",
    "                                   columns = ['Characteristic','Bin','Label','Score','ScorePoints',\n",
    "                                                  'Range','Formula','Number_of_Values','Value1','Value2']))\n",
    "\n",
    "prv_Variable = ''\n",
    "prv_BiXi = ''\n",
    "prv_Value1 = ''\n",
    "Bin = 1\n",
    "k=0\n",
    "for r in scorecard_out.itertuples():  \n",
    "    if (r.Variable==prv_Variable):\n",
    "        prv_Bin=Bin\n",
    "        Bin+=1\n",
    "    else:  \n",
    "        prv_Bin=Bin\n",
    "        Bin=1\n",
    "        k=0\n",
    "    if (r.BiXi!=prv_BiXi):\n",
    "        k+=1  \n",
    "    Number_of_Values=1   \n",
    "    Value1 = ''\n",
    "    Value2 = ''\n",
    "    Formula = ''\n",
    "    if pd.notnull(r.Value):\n",
    "        Value1 = r.Value\n",
    "        Label = 'K'+str(k)+'_{'+str(Value1)+'}'\n",
    "        Formula = r.Variable + '= \\'character\\''\n",
    "    if r.Value=='null':\n",
    "        Value1 = ''\n",
    "        Label = 'NA'\n",
    "        Formula = ''\n",
    "    if r.Value=='else':\n",
    "        prv_Value1 = r.Value\n",
    "        Value1 = ''\n",
    "        Label = 'All Other'   \n",
    "        Formula=''\n",
    "    if pd.notnull(r.Min) and pd.notnull(r.Max) and (r.Min!=-np.inf) and (r.Max!=np.Inf):\n",
    "        Number_of_Values=2\n",
    "        Value1 = r.Min\n",
    "        Value2 = r.Max\n",
    "        Label = 'K'+str(k)+'_'+str(Value1)+'_'+str(Value2)\n",
    "        Formula = r.Variable+ ' \\'integer1\\' <= .. <\\'integer2\\''\n",
    "    if pd.notnull(r.Min) and (r.Max==np.inf):\n",
    "        Value1 = r.Min\n",
    "        Label = 'K'+str(k)+'_'+str(Value1)\n",
    "        Formula = r.Variable+ ' >=\\'integer\\''  \n",
    "    if pd.notnull(r.Max) and (r.Min==-np.inf):\n",
    "        Value1 = r.Max\n",
    "        Label = 'K'+str(k)+'_'+str(Value1) \n",
    "        Formula = r.Variable+ ' <\\'integer\\''\n",
    "    if (r.Variable!=prv_Variable) and (r.Variable!='_Intercept'):\n",
    "        if prv_Value1!='else' and prv_Variable != '':\n",
    "            balze_table=balze_table.append(pd.DataFrame( data= [[prv_Variable,prv_Bin+1,'All Other',\n",
    "                                                             0,'','','',\n",
    "                                                             '','','']],\n",
    "                                       columns = ['Characteristic','Bin','Label','Score','ScorePoints',\n",
    "                                                      'Range','Formula','Number_of_Values','Value1','Value2']))\n",
    "        prv_Value1 = ''\n",
    "        prv_Variable = r.Variable\n",
    "    if (r.Variable!='_Intercept'):   \n",
    "        balze_table=balze_table.append(pd.DataFrame( data= [[r.Variable,Bin,Label,\n",
    "                                                             r.BiXi,0,1,Formula,\n",
    "                                                             Number_of_Values,Value1,Value2]],\n",
    "                                       columns = ['Characteristic','Bin','Label','Score','ScorePoints',\n",
    "                                                      'Range','Formula','Number_of_Values','Value1','Value2']))\n",
    "        \n",
    "balze_table.to_csv(output_folder+'/model/blaze_table.csv',sep = ';')  \n",
    "display(balze_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Python code to run the scorecard in any Python script independently on this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTER PART TRANSFORMING WOE TO BIXI\n",
    "scoring_python_outer = ['\\n    LINEAR_SCORE = \\\\\\n']\n",
    "                        \n",
    "#INNER PART TRANSFORMING VARIABLE TO WOE\n",
    "scoring_python_inner = ['def score(row):']\n",
    "tmp_variable = ''\n",
    "nullWOE = 0\n",
    "for r in scorecard_out.itertuples():\n",
    "    if r.Variable != tmp_variable:\n",
    "        if tmp_variable != '':\n",
    "            #OUTER PART TRANSFORMING WOE TO BIXI\n",
    "            scoring_python_outer.append(' + \\\\\\n    ')\n",
    "            #INNER PART TRANSFORMING VARIABLE TO WOE\n",
    "            scoring_python_inner.append('    else: ' + str(tmp_variable) + '_WOE = ' + str(nullWOE) + '\\n')\n",
    "        else:\n",
    "            #OUTER PART TRANSFORMING WOE TO BIXI\n",
    "            scoring_python_outer.append('    ')\n",
    "\n",
    "        #OUTER PART TRANSFORMING WOE TO BIXI\n",
    "        scoring_python_outer.append(str(r.Variable) + '_WOE * ' + str(r.Beta))\n",
    "        #INNER PART TRANSFORMING VARIABLE TO WOE\n",
    "        scoring_python_inner.append('\\n')\n",
    "        tmp_variable = r.Variable\n",
    "        nullWOE = 0\n",
    "        \n",
    "        scoring_python_inner.append('    if ')\n",
    "    else:\n",
    "        scoring_python_inner.append('    elif ')\n",
    "        \n",
    "    if r.Value == 'null':\n",
    "        scoring_python_inner.append('row[\\'' + str(r.Variable) + '\\'] != row[\\'' + str(r.Variable) + '\\']: ' + str(tmp_variable) + '_WOE = ' + str(r.WOE) + '\\n')\n",
    "        nullWOE = r.WOE\n",
    "    elif pd.notnull(r.Value):\n",
    "        scoring_python_inner.append('row[\\'' + str(r.Variable) + '\\'] == \"' + str(r.Value) + '\": ' + str(tmp_variable) + '_WOE = ' + str(r.WOE) + '\\n')\n",
    "    elif pd.notnull(r.Min):\n",
    "        if np.isfinite(r.Max):\n",
    "            scoring_python_inner.append('row[\\'' + str(r.Variable) + '\\'] < ' + str(r.Max) + ': ' + str(tmp_variable) + '_WOE = ' + str(r.WOE) + '\\n')\n",
    "        else:\n",
    "            scoring_python_inner.append('row[\\'' + str(r.Variable) + '\\'] >= ' + str(r.Min) + ': ' + str(tmp_variable) + '_WOE = ' + str(r.WOE) + '\\n')\n",
    "    elif r.Variable == '_Intercept':\n",
    "        scoring_python_inner.append('1==1: _Intercept_WOE = ' + str(r.WOE) + '\\n')\n",
    "\n",
    "#INNER PART TRANSFORMING VARIABLE TO WOE\n",
    "scoring_python_inner.append('    else: ' + str(tmp_variable) + '_WOE = ' + str(nullWOE) + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "scoring_python_outer.append('\\n\\n    SCORE = 1-1/(1+np.exp(LINEAR_SCORE))\\n') \n",
    "scoring_python_outer.append('\\n    return SCORE\\n') \n",
    "\n",
    "scoring_python_outer = ''.join(scoring_python_outer)\n",
    "scoring_python_inner = ''.join(scoring_python_inner)\n",
    "scoring_python_final = scoring_python_inner + scoring_python_outer\n",
    "print(scoring_python_final,file=open(output_folder+'/model/scorecard.py', \"w\"))\n",
    "print(scoring_python_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance characteristics\n",
    "Performance characteristics of the model (Gini, Lift) and their visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.metrics import gini, lift\n",
    "lift_perc = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some od these samples (train, valid, test, OOT, HOOT) are empty (i.e. you don't use them), **you need to comment their according rows in some of the following cells.** Such rows are marked by short comments at their ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = pd.DataFrame({'sample':[\n",
    "    'train', #train\n",
    "    'valid', #valid\n",
    "    'test', #test\n",
    "    'oot', #OOT\n",
    "    'hoot' #HOOT\n",
    "    ], 'gini':[\n",
    "    gini(data[train_mask][col_target],data[train_mask][col_score]) #train\n",
    "    ,gini(data[valid_mask][col_target],data[valid_mask][col_score]) #valid\n",
    "    ,gini(data[test_mask][col_target],data[test_mask][col_score]) #test\n",
    "    ,gini(data[oot_mask][col_target],data[oot_mask][col_score]) #OOT\n",
    "    ,gini(data[hoot_mask][col_target],data[hoot_mask][col_score]) #HOOT\n",
    "    ], 'lift_'+str(lift_perc):[\n",
    "    lift(data[train_mask][col_target],-data[train_mask][col_score],lift_perc) #train\n",
    "    ,lift(data[valid_mask][col_target],-data[valid_mask][col_score],lift_perc) #valid\n",
    "    ,lift(data[test_mask][col_target],-data[test_mask][col_score],lift_perc) #test\n",
    "    ,lift(data[oot_mask][col_target],-data[oot_mask][col_score],lift_perc) #OOT\n",
    "    ,lift(data[hoot_mask][col_target],-data[hoot_mask][col_score],lift_perc) #HOOT\n",
    "    ]}).set_index('sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(perf)\n",
    "perf.to_csv(output_folder+'/performance/performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate data for Gini and Lift curves\n",
    "from scoring.tools import calculate_gini_and_lift\n",
    "train_stats, train_curve = calculate_gini_and_lift(data[train_mask], col_target, col_score, pct = lift_perc) #train\n",
    "train_curve = list(zip(*train_curve))                                                                        #train\n",
    "valid_stats, valid_curve = calculate_gini_and_lift(data[valid_mask], col_target, col_score, pct = lift_perc) #valid\n",
    "valid_curve = list(zip(*valid_curve))                                                                        #valid\n",
    "test_stats, test_curve = calculate_gini_and_lift(data[test_mask], col_target, col_score, pct = lift_perc)    #test\n",
    "test_curve = list(zip(*test_curve))                                                                          #test\n",
    "oot_stats, oot_curve = calculate_gini_and_lift(data[oot_mask], col_target, col_score, pct = lift_perc)       #oot\n",
    "oot_curve = list(zip(*oot_curve))                                                                            #oot\n",
    "hoot_stats, hoot_curve = calculate_gini_and_lift(data[hoot_mask], col_target, col_score, pct = lift_perc)    #hoot\n",
    "hoot_curve = list(zip(*hoot_curve))                                                                          #hoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,7))\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.plot([0] + list(train_curve[2]),[0] + list(train_curve[3]), label = 'Train', color = 'g') #train\n",
    "plt.plot([0] + list(valid_curve[2]), [0] + list(valid_curve[3]), label = 'Validation', color = 'r') #valid\n",
    "plt.plot([0] + list(test_curve[2]), [0] + list(test_curve[3]), label = 'Test', color = 'y') #test\n",
    "plt.plot([0] + list(oot_curve[2]), [0] + list(oot_curve[3]), label = 'OOT', color = 'b') #oot\n",
    "plt.plot([0] + list(hoot_curve[2]), [0] + list(hoot_curve[3]), label = 'Hist.OOT', color = 'm') #hoot\n",
    "plt.plot(list(range(0, 101)), list(range(0, 101)), color='k')\n",
    "plt.xlabel('Cumulative good count')\n",
    "plt.ylabel('Cumulative bad count')\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.savefig(output_folder+'/performance/roc.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.axis([0, 100, 0, max(train_curve[1])+0.5])\n",
    "plt.plot(train_curve[0], train_curve[1], label = 'Train', color = 'g') #train\n",
    "plt.plot(valid_curve[0], valid_curve[1], label = 'Validation', color = 'r') #valid\n",
    "plt.plot(test_curve[0], test_curve[1], label = 'Test', color = 'y') #test\n",
    "plt.plot(oot_curve[0], oot_curve[1], label = 'OOT', color = 'b') #oot\n",
    "plt.plot(hoot_curve[0], hoot_curve[1], label = 'Hist.OOT', color = 'm') #hoot\n",
    "plt.xlabel('Cumulative count [%]')\n",
    "plt.ylabel('Lift')\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.savefig(output_folder+'/performance/lift.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def proc_gini(x,y,z):\n",
    "    fpr, tpr, _ = roc_curve(x[y], x[z], pos_label=0)\n",
    "    roc_gini = (auc(fpr, tpr)-0.5)*2\n",
    "    return roc_gini\n",
    "%matplotlib inline\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "len1 = 0\n",
    "\n",
    "grouped = data[hoot_mask].groupby(col_month, axis=0) #hoot\n",
    "res_hoot= grouped.apply(proc_gini, col_target ,col_score) #hoot\n",
    "plt.plot(range(len1,len1+len(res_hoot)),-res_hoot, linewidth=2.0,label='hist. OOT', color = 'm', marker='o') #hoot\n",
    "\n",
    "if res_hoot is not None: len1 = len1 + len(res_hoot)\n",
    "\n",
    "grouped = data[train_mask].groupby(col_month, axis=0) #train\n",
    "res_train= grouped.apply(proc_gini, col_target ,col_score) #train\n",
    "plt.plot(range(len1,len1+len(res_train)),-res_train, linewidth=2.0,label='Train', color = 'g', marker='o') #train\n",
    "grouped = data[valid_mask].groupby(col_month, axis=0) #valid\n",
    "res_valid= grouped.apply(proc_gini, col_target ,col_score) #valid\n",
    "plt.plot(range(len1,len1+len(res_valid)),-res_valid, linewidth=2.0,label='Validation', color = 'r', marker='o') #valid\n",
    "grouped = data[test_mask].groupby(col_month, axis=0) #test\n",
    "res_test= grouped.apply(proc_gini, col_target ,col_score) #test\n",
    "plt.plot(range(len1,len1+len(res_test)),-res_test, linewidth=2.0,label='Test', color = 'y', marker='o') #test\n",
    "\n",
    "if res_train is not None: len1 = len1 + len(res_train)\n",
    "\n",
    "grouped = data[oot_mask].groupby(col_month, axis=0) #oot\n",
    "res_oot= grouped.apply(proc_gini, col_target ,col_score) #oot\n",
    "plt.plot(range(len1,len1+len(res_oot)),-res_oot, linewidth=2.0,label='OOT', color = 'b', marker='o') #oot\n",
    "\n",
    "plt.xticks(range(len(res_train)+len(res_oot)), np.sort(data[col_month].unique()), rotation=45)\n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.title('Gini by months')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Gini')\n",
    "plt.savefig(output_folder+'/performance/ginistability.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import plot_calib\n",
    "plot_calib(data[col_score],data[col_target],bins=20,savepath=output_folder+'/model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "Calculate and visualise correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cormat = data[cols_final_predictors].corr()\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 15})\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.close_figures=True\n",
    "a4_dims = (12,10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=a4_dims, dpi=50)\n",
    "fig.suptitle('Correlations of Predictors',fontsize=25)\n",
    "sns.heatmap(cormat, ax=ax, annot=True, fmt=\"0.1f\", linewidths=.5, annot_kws={\"size\":15},cmap=\"OrRd\")\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.savefig(output_folder+'/analysis/correlation.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the list of the highest correlation (restricted to correlations that are, in absolute value, higher than *max_ok_correlation* parameter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ok_correlation = 0.0\n",
    "\n",
    "# find highest pairwise correlation (correlation greater than .. in absolute value)\n",
    "hicors = []\n",
    "for i in range(0,len(cormat)):\n",
    "    for j in range(0,len(cormat)):\n",
    "        if ((cormat.iloc[i][j] > max_ok_correlation or cormat.iloc[i][j] < -max_ok_correlation) and i < j):\n",
    "            hicors.append((i,j,cormat.index[i],cormat.index[j],cormat.iloc[i][j],abs(cormat.iloc[i][j])))\n",
    "hicors.sort(key= lambda tup: tup[5], reverse=True)\n",
    "\n",
    "hicors2 = pd.DataFrame(list(zip(*list(zip(*hicors))[2:5])))\n",
    "\n",
    "# print list of highest correlations\n",
    "hicors2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time stability of predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set metadata for the stability charts. Two types of charts will be drawn:\n",
    "- Stability of default rate, for which the variables with default and with base need to be set\n",
    "- Stability of population, for which the variable with observation count needs to be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_for_default = col_target\n",
    "base_for_default = col_base\n",
    "data['ones'] = 1\n",
    "obs_for_population = 'ones'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import stability_chart\n",
    "\n",
    "for j in list(clf.final_predictors_):\n",
    "    stability_chart(data[j],data[target_for_default],data[base_for_default],data[obs_for_population],data[col_month],\n",
    "                   savepath=output_folder+'/stability/'+j+'_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with another score\n",
    "Similar charts to what were already done for the new scorecard are now drawn to compare the new scorecard to another scorecard. The value of the old score should be saved in a special column of original data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_oldscore = 'OLD_SCORE'\n",
    "\n",
    "#if the score gives the complementary probability (of non-default), run this:\n",
    "data[col_oldscore]=1-data[col_oldscore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_oldscore = pd.DataFrame({'scorecard':[\n",
    "     'old' #valid\n",
    "    ,'old' #test\n",
    "    ,'old' #oot\n",
    "    ,'old' #hoot\n",
    "    ,'new' #valid\n",
    "    ,'new' #test\n",
    "    ,'new' #oot\n",
    "    ,'new' #hoot\n",
    "    ],'sample':[\n",
    "    'valid' #valid\n",
    "    ,'test' #test\n",
    "    ,'oot'  #oot\n",
    "    ,'hoot' #hoot\n",
    "    ,'valid'#valid\n",
    "    ,'test' #test\n",
    "    ,'oot'  #oot\n",
    "    ,'hoot' #hoot\n",
    "    ], 'gini':[\n",
    "    gini(data[valid_mask][col_target],data[valid_mask][col_oldscore]) #valid\n",
    "    ,gini(data[test_mask][col_target],data[test_mask][col_oldscore]) #test\n",
    "    ,gini(data[oot_mask][col_target],data[oot_mask][col_oldscore]) #oot\n",
    "    ,gini(data[hoot_mask][col_target],data[hoot_mask][col_oldscore]) #hoot\n",
    "    ,gini(data[valid_mask][col_target],data[valid_mask][col_score]) #valid\n",
    "    ,gini(data[test_mask][col_target],data[test_mask][col_score]) #test\n",
    "    ,gini(data[oot_mask][col_target],data[oot_mask][col_score]) #oot\n",
    "    ,gini(data[hoot_mask][col_target],data[hoot_mask][col_score]) #hoot\n",
    "    ], 'lift_'+str(lift_perc):[\n",
    "    lift(data[valid_mask][col_target],-data[valid_mask][col_oldscore],lift_perc) #valid\n",
    "    ,lift(data[test_mask][col_target],-data[test_mask][col_oldscore],lift_perc) #test\n",
    "    ,lift(data[oot_mask][col_target],-data[oot_mask][col_oldscore],lift_perc) #oot\n",
    "    ,lift(data[hoot_mask][col_target],-data[hoot_mask][col_oldscore],lift_perc) #hoot\n",
    "    ,lift(data[valid_mask][col_target],-data[valid_mask][col_score],lift_perc) #valid\n",
    "    ,lift(data[test_mask][col_target],-data[test_mask][col_score],lift_perc) #test\n",
    "    ,lift(data[oot_mask][col_target],-data[oot_mask][col_score],lift_perc) #oot\n",
    "    ,lift(data[hoot_mask][col_target],-data[hoot_mask][col_score],lift_perc) #hoot\n",
    "    ]}).set_index('sample').pivot(columns='scorecard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(perf_oldscore)\n",
    "perf_oldscore.to_csv(output_folder+'/performance/performance_oldscore.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.tools import calculate_gini_and_lift\n",
    "newscore_stats, newscore_curve = calculate_gini_and_lift(data[valid_mask|test_mask|oot_mask|hoot_mask],\n",
    "                                                         col_target, col_score, pct = lift_perc)\n",
    "newscore_curve = list(zip(*newscore_curve))\n",
    "oldscore_stats, oldscore_curve = calculate_gini_and_lift(data[valid_mask|test_mask|oot_mask|hoot_mask],\n",
    "                                                         col_target, col_oldscore, pct = lift_perc)\n",
    "oldscore_curve = list(zip(*oldscore_curve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,7))\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.plot([0] + list(newscore_curve[2]),[0] + list(newscore_curve[3]), label = 'new score', color = 'g')\n",
    "plt.plot([0] + list(oldscore_curve[2]), [0] + list(oldscore_curve[3]), label = 'old score', color = 'r')\n",
    "plt.plot(list(range(0, 101)), list(range(0, 101)), color='k')\n",
    "plt.xlabel('Cumulative good count')\n",
    "plt.ylabel('Cumulative bad count')\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.savefig(output_folder+'/performance/roc_oldscore.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.axis([0, 100, 0, max(train_curve[1])+0.5])\n",
    "plt.plot(newscore_curve[0], newscore_curve[1], label = 'new score', color = 'g')\n",
    "plt.plot(oldscore_curve[0], oldscore_curve[1], label = 'old score', color = 'r')\n",
    "plt.xlabel('Cumulative count [%]')\n",
    "plt.ylabel('Lift')\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.savefig(output_folder+'/performance/lift_oldscore.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def proc_gini(x,y,z):\n",
    "    fpr, tpr, _ = roc_curve(x[y], x[z], pos_label=0)\n",
    "    roc_gini = (auc(fpr, tpr)-0.5)*2\n",
    "    return roc_gini\n",
    "%matplotlib inline\n",
    "plt.figure(figsize = (10,7))\n",
    "grouped = data[valid_mask|test_mask|oot_mask|hoot_mask].groupby(col_month, axis=0)\n",
    "res_new= grouped.apply(proc_gini, col_target ,col_score)\n",
    "plt.plot(range(len(res_new)),-res_new, linewidth=2.0,label='new score', color = 'g', marker='o')\n",
    "\n",
    "grouped = data[valid_mask|test_mask|oot_mask|hoot_mask].groupby(col_month, axis=0)\n",
    "res_old= grouped.apply(proc_gini, col_target ,col_oldscore)\n",
    "plt.plot(range(len(res_old)),-res_old, linewidth=2.0,label='old score', color = 'r', marker='o')\n",
    "\n",
    "plt.xticks(range(len(res_valid)+len(res_oot)), np.sort(data[col_month].unique()), rotation=45)\n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.title('Gini by months')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Gini')\n",
    "plt.savefig(output_folder+'/performance/ginistability_oldscore.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices for the observable population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import transmatrix\n",
    "    \n",
    "transmatrix(oldscore = data[valid_mask|test_mask|oot_mask|hoot_mask][col_oldscore],\n",
    "            newscore = data[valid_mask|test_mask|oot_mask|hoot_mask][col_score],\n",
    "            target = data[valid_mask|test_mask|oot_mask|hoot_mask][target_for_default],\n",
    "            base = data[valid_mask|test_mask|oot_mask|hoot_mask][base_for_default],\n",
    "            obs = data[valid_mask|test_mask|oot_mask|hoot_mask][base_for_default],\n",
    "            draw_default_matrix=True,\n",
    "            draw_transition_matrix=True,\n",
    "            savepath=output_folder+'/analysis/devpop_',\n",
    "            quantiles_count = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition matrix for the whole population (put also the rejected etc. here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_mask = data.data_type != 'train'\n",
    "\n",
    "transmatrix(oldscore = data[pop_mask][col_oldscore],\n",
    "            newscore = data[pop_mask][col_score],\n",
    "            target = data[pop_mask][target_for_default],\n",
    "            base = data[pop_mask][base_for_default],\n",
    "            obs = data[pop_mask][obs_for_population],\n",
    "            draw_default_matrix=False,\n",
    "            draw_transition_matrix=True,\n",
    "            savepath=output_folder+'/analysis/allpop_',\n",
    "            quantiles_count = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on short target\n",
    "If there is also a shorter (e.g. FPD30) target in the original dataset, we draw also charts for performance on this target in this part of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of the short target column\n",
    "col_short = \"FPD\"\n",
    "#name of the short target's base column\n",
    "col_shortbase = \"FPD_BASE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have base column in your data set, the following code adds it. **Otherwise, don't run it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if col_shortbase not in data:\n",
    "    data[col_shortbase] = 0\n",
    "    data.loc[data[col_short]==0,col_shortbase] = 1\n",
    "    data.loc[data[col_short]==1,col_shortbase] = 1\n",
    "    print('Column',col_shortbase,'added/modified. Number of columns:',data.shape[1])\n",
    "else:\n",
    "    print('Column',col_base,'already exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortbase_mask = ((data.data_type == 'valid')|(data.data_type == 'test')|(data.data_type == 'oot')|(data.data_type == 'hoot')) \\\n",
    "&(data[col_shortbase] == 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_shorttarget = pd.DataFrame({'gini':[\n",
    "    gini(data[shortbase_mask][col_short],data[shortbase_mask][col_score])\n",
    "    ], 'lift_'+str(lift_perc):[\n",
    "    lift(data[shortbase_mask][col_short],-data[shortbase_mask][col_score],lift_perc)\n",
    "    ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(perf_shorttarget)\n",
    "perf_shorttarget.to_csv(output_folder+'/performance/performance_shorttarget.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def proc_gini(x,y,z):\n",
    "    fpr, tpr, _ = roc_curve(x[y], x[z], pos_label=0)\n",
    "    roc_gini = (auc(fpr, tpr)-0.5)*2\n",
    "    return roc_gini\n",
    "%matplotlib inline\n",
    "plt.figure(figsize = (10,7))\n",
    "grouped = data[valid_mask|test_mask|oot_mask|hoot_mask].groupby(col_month, axis=0)\n",
    "res_new= grouped.apply(proc_gini, col_target ,col_score)\n",
    "plt.plot(range(len(res_new)),-res_new, linewidth=2.0,label='target', color = 'g', marker='o')\n",
    "\n",
    "grouped = data[shortbase_mask].groupby(col_month, axis=0)\n",
    "res_short= grouped.apply(proc_gini, col_short ,col_score)\n",
    "plt.plot(range(len(res_short)),-res_short, linewidth=2.0,label='short target', color = 'r', marker='o')\n",
    "\n",
    "plt.xticks(range(len(res_short)), np.sort(data[col_month].unique()), rotation=45)\n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.title('Gini by months')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Gini')\n",
    "plt.savefig(output_folder+'/performance/ginistability_shorttarget.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML documentation\n",
    "\n",
    "Create a basic HTML document with important scorecard characteristics. The results of some of the previous parts of the workflow have to be already created on disk, this part will just wrap them up.\n",
    "\n",
    "If some specific parts (short target analysis, old score comparison) were not done, use the parameters below and set them to *False*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_scorecard_name = 'My PSW model'\n",
    "txt_author_name = 'Pavel SÅ¯va'\n",
    "short_target_analysis_done = True\n",
    "old_score_comparison_done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_folder+'/documentation.html', 'w', encoding='utf-8') as f:\n",
    "    f.write('<html>\\n<head>\\n<title>'+txt_scorecard_name+'</title>\\n')\n",
    "    f.write('<meta charset=\"utf-8\">\\n')    \n",
    "    f.write('<style>\\nbody{font: normal 10pt Helvetica, Arial, sans-serif;}\\n'+ \\\n",
    "            '.textbold{font-weight:bold;}\\n' + \\\n",
    "            '.divcode{font-family:Courier New,Courier,Lucida Sans Typewriter,Lucida Typewriter,monospace;}\\n' + \\\n",
    "            '.divpic{padding-bottom: 20pt;}\\n' + \\\n",
    "            '.textlabel{font-style:italic;font-size:8pt;}\\n' + \\\n",
    "            'table{border-collapse:collapse;}\\n' + \\\n",
    "            '</style>\\n')\n",
    "    f.write('</head>\\n<body>')\n",
    "    f.write('<h1>'+txt_scorecard_name+' - documentation</h1>\\n')\n",
    "    f.write('<h2>Document information</h2>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divtext\"><span class=\"textbold\">Author:</span> '+txt_author_name+'</div>\\n')\n",
    "    f.write(' <div class=\"divtext\"><span class=\"textbold\">Date:</span> '+ \\\n",
    "            datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")+'</div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h2>Data sample</h2>\\n')\n",
    "    f.write('<h3>Target</h3>')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divtext\"><span class=\"textbold\">Target variable:</span> '+\\\n",
    "            pd.read_csv(output_folder+'/model/metadata.csv',header=None,index_col=0).loc['col_target'][1]+'</div>\\n')\n",
    "    f.write(' <div class=\"divtext\"><span class=\"textbold\">Base variable:</span> '+\\\n",
    "            pd.read_csv(output_folder+'/model/metadata.csv',header=None,index_col=0).loc['col_base'][1]+'</div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h3>Sample characteristics</h3>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"analysis/data.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Observations and defaults in time</span></div>\\n')\n",
    "    f.write(' <div class=\"divtab\">\\n'+pd.read_csv(output_folder+'/analysis/summary.csv',header=[0,1],index_col=0) \\\n",
    "            .to_html(na_rep='')+'\\n </div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h3>Covariates</h3>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divtab\">\\n'+pd.read_csv(output_folder+'/predictors/covariates.csv',header=0,index_col=0) \\\n",
    "            .to_html(na_rep='')+'\\n </div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h2>Final scorecard</h2>\\n')\n",
    "    f.write('<h3>Scorecard</h3>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divtab\">\\n'+pd.read_csv(output_folder+'/model/scorecard.csv',header=0,index_col=0) \\\n",
    "            .to_html(na_rep='')+'\\n </div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h3>Scoring SQL</h3>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divcode\">\\n'+open(output_folder+'/model/scorecard.sql', 'r').read() \\\n",
    "            .replace(' ','&nbsp;').replace('\\n','<br />')+'\\n </div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h2>Predictors</h2>\\n')\n",
    "    for pred in pd.read_csv(output_folder+'/predictors/predictors.csv',index_col=None,header=None)[0].tolist():\n",
    "        pred0 = ''.join(pred.split())[:-4]\n",
    "        f.write('<h3>'+pred0+'</h3>\\n')\n",
    "        f.write('<h4>Grouping</h4>')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"predictors/'+pred0+'_binning.png\" /></div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "        f.write('<h4>Stability</h4>')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"stability/'+pred+'_stability.png\" /></div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "    f.write('<h2>Correlations</h2>\\n')\n",
    "    f.write('<h3>Correlation matrix between WOE variables</h3>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"analysis/correlation.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Correlation of WOE variables</span></div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h2>Model evaluation</h2>\\n')\n",
    "    f.write('<h3>Performance</h3>\\n')\n",
    "    f.write('<h4>General performance</h4>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divtab\">\\n'+pd.read_csv(output_folder+'/performance/performance.csv',header=0,index_col=0) \\\n",
    "            .to_html(na_rep='')+'\\n </div>\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"performance/roc.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">ROC curve</span></div>\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"performance/lift.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Lift curve</span></div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h4>Performance stability</h4>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"performance/ginistability.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Stability of Gini in time</span></div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    if short_target_analysis_done:\n",
    "        f.write('<h3>Performance on shorter target</h3>\\n')\n",
    "        f.write('<h4>General performance</h4>\\n')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divtab\">\\n'+ \\\n",
    "                pd.read_csv(output_folder+'/performance/performance_shorttarget.csv',header=0,index_col=0) \\\n",
    "                .to_html(na_rep='')+'\\n </div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "        f.write('<h4>Performance stability</h4>\\n')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"performance/ginistability_shorttarget.png\" />\\n' + \\\n",
    "                ' <br /><span class=\"textlabel\">Stability of Gini in time</span></div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "        f.write('<h3>Calibration</h3>\\n')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"model/calibration.png\" />\\n' + \\\n",
    "                ' <br /><span class=\"textlabel\">Model calibration chart</span></div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "    if old_score_comparison_done:\n",
    "        f.write('<h2>Comparison with current model</h2>\\n')\n",
    "        f.write('<h3>Performance comparison</h3>\\n')\n",
    "        f.write('<h4>General performance</h4>\\n')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divtab\">\\n'+ \\\n",
    "                pd.read_csv(output_folder+'/performance/performance_oldscore.csv',header=[0,1],index_col=0) \\\n",
    "                .to_html(na_rep='')+'\\n </div>\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"performance/roc_oldscore.png\" />\\n' + \\\n",
    "                ' <br /><span class=\"textlabel\">ROC curve</span></div>\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"performance/lift_oldscore.png\" />\\n' + \\\n",
    "                ' <br /><span class=\"textlabel\">Lift curve</span></div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "        f.write('<h4>Performance stability</h4>\\n')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"performance/ginistability_oldscore.png\" />\\n' + \\\n",
    "                ' <br /><span class=\"textlabel\">Stability of Gini in time</span></div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "        f.write('<h3>Transition matrices</h3>\\n')\n",
    "        f.write('<h4>Bad rate matrix</h4>\\n')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"analysis/devpop_matrix_default.png\" />\\n' + \\\n",
    "                ' <br /><span class=\"textlabel\">Default rate matrix</span></div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "        f.write('<h4>Transition matrix - development sample</h4>\\n')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"analysis/devpop_matrix_transition.png\" />\\n' + \\\n",
    "                ' <br /><span class=\"textlabel\">Transition matrix</span></div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "        f.write('<h4>Transition matrix - whole population</h4>\\n')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"analysis/allpop_matrix_transition.png\" />\\n' + \\\n",
    "                ' <br /><span class=\"textlabel\">Transition matrix</span></div>\\n')\n",
    "        f.write('</div>\\n')  \n",
    "    f.write('</body></html>')\n",
    "    print('Created documentation in file',f.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
