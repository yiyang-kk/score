{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Credit Python Scoring Workflow v.0.2.0\n",
    "\n",
    "**Contributors:**\n",
    "- Pavel SÅ¯va (HCI Reserach & Development)\n",
    "- Marek Teller (HCI Reserach & Development)\n",
    "- Martin Kotek (HCI Reserach & Development)\n",
    "- Sergey Gerasimov (HCRU Big Data & Scoring)\n",
    "- Valentina Kalenichenko (HCRU Big Data & Scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "- time, datetime - ability to get current time for logs\n",
    "- math - basic mathematical functions (as logarithm etc.))\n",
    "- random - generate random selection from probability distributions\n",
    "- NumPy - for scientific, mathematical, numerical calculations\n",
    "- Pandas - for efficient work with large data structures\n",
    "- cx_Oracle and sqlalchemy - for loading data from Oracle database (DWH etc.)\n",
    "- statsmodels - library with some statistical functions and models\n",
    "- scikit-learn - all important machine learning (and statistical) algorithms used for training the models\n",
    "- matplotlib - for plotting the charts\n",
    "- seaborn - for statistical visualisations\n",
    "- scoring - functions and objects from scoring.py (part of our scoring workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cx_Oracle\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils import as_float_array\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set()\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "Importing data from a CSV file. It is important to set the following parameters:\n",
    "\n",
    "encoding: usually 'utf-8' or windows-xxxx on Windows machines, where xxxx is 1250 for Central Europe, 1251 for Cyrilic etc.\n",
    "sep: separator of columns in the file\n",
    "decimal: decimal dot or coma\n",
    "index_col: which columns is used as index - should be the unique credit case identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:/Analyses/HQ_20171213_PythonWorkflow/ExampleData2.csv', sep = ',', decimal = '.', \n",
    "                   encoding = 'windows-1251', index_col = 'ID', low_memory = False)\n",
    "print('Data loaded on',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally the data can be loaded also from a database. The function read_sql uses cache, so the data don't have to be downloaded from the database repeatedly. The cache will be located in a new folder called **db_cache**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#engine = create_engine('oracle://PAVELS[GP_HQ_RISK]:password@(description=(address=(protocol=tcp)(host=dbdwhru.homecredit.ru)(port=1521))(connect_data=(sid=DWHRU)))', echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scoring.db import read_sql\n",
    "#data = read_sql('select * from owner_dwh.f_application_tt where rownum=1',engine, index_col = 'sk_application')\n",
    "#print('Data loaded on',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to download data from the database again (and not from cache), use the parameter refresh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scoring.db import read_sql\n",
    "#data = read_sql('select * from owner_dwh.f_application_tt where rownum=1',engine, index_col = 'sk_application', refresh=True)\n",
    "#print('Data loaded on',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows:',data.shape[0])\n",
    "print('Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata definitions\n",
    "Assigning ID column, target column, time column and month column. The month column don't have to exist in the dataset, it will be created later in this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of the time column\n",
    "col_time = \"TIME\"\n",
    "#name of the month column\n",
    "col_month = \"MONTH\"\n",
    "#name of the day column\n",
    "col_day = \"DAY\"\n",
    "#name of the target column\n",
    "col_target = \"DEF\"\n",
    "#name of the base column\n",
    "col_base = \"BASE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have base column in your data set, the following code adds it (value 1 for each observation). **Otherwise, don't run it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[col_base] = 1\n",
    "print('Column',col_base,'added/modified. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the month and day column from the time column\n",
    "- take the time column and tell in which format the time is saved in\n",
    "- strip the format just to year, month, day string\n",
    "- convert the string to number\n",
    "- the new column will be added to the dataset as day\n",
    "- truncate this column to just year and month and add it to dataset as month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,col_day] = pd.to_numeric(pd.to_datetime(data[col_time], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y%m%d'))\n",
    "data[col_month] = data[col_day].apply(lambda x: math.trunc(x/100))\n",
    "print('Columns',col_day,'and',col_month,'added/modified. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the predictors list from a csv file. The csv should have just one column, without any header, containing the name of the variables that should be used as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_pred = list(pd.read_csv(r'C:/Analyses/HQ_20171213_PythonWorkflow/ExamplePredList.csv', sep = ',', decimal = '.', \n",
    "                   encoding = 'windows-1251', low_memory = False, header = None)[0])\n",
    "\n",
    "cols_pred_cat = list(set([c[0] for c in list(zip(data.columns, data.dtypes)) if c[1]=='O']) & set(cols_pred))\n",
    "cols_pred_num = list(set([c[0] for c in list(zip(data.columns, data.dtypes)) if c[1]!='O']) & set(cols_pred))\n",
    "\n",
    "# ALTERNATIVELY, DEFINE THE PREDICTOR NAMES MANUALLY\n",
    "\n",
    "#cols_pred_num = [\"Numerical_1\",\"Numerical_2\",\"Numerical_3\",\"Numerical_4\",\"Numerical_5\"]\n",
    "#cols_pred_cat = [\"Categorical_1\",\"Categorical_2\",\"Categorical_3\",\"Categorical_4\",\"Categorical_5\"]\n",
    "\n",
    "\n",
    "cols_pred = cols_pred_num + cols_pred_cat\n",
    "\n",
    "print(len(cols_pred_num),'numerical predictors:')\n",
    "for p in cols_pred_num: print(p)\n",
    "print(len(cols_pred_cat),'categorical predictors:')\n",
    "for p in cols_pred_cat: print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descrip = data.describe(include='all').transpose()\n",
    "pd.options.display.max_rows = 1000\n",
    "display(descrip)\n",
    "pd.options.display.max_rows = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**explore_df** function creates a simple text report about the important variable. The report can be then printed either to the screen or to a file.\n",
    "\n",
    "In the following code, only such part of data that has col_base = 1 is analyzed. You can remove the condition if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.data_exploration import explore_df\n",
    "st = explore_df(data[data[col_base]==1],col_month,col_target,cols_pred)\n",
    "print(st,file=open(\"data_exp.txt\", \"w\"))\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default rate in time**: Simple visualisation of observation count and default rate in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import plot_dataset\n",
    "plot_dataset(data,col_month,col_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "\n",
    "- Split data into three parts (in time training, in time validation, in time test, out of time).\n",
    "- Adds a new column indicating to which part the observations belong.\n",
    "- The split parameters are set at the beginning of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_train = 0.7\n",
    "share_validation = 0.3\n",
    "last_intime_day = 20170531\n",
    "\n",
    "data['random_value'] = 1\n",
    "data['random_value'] = data['random_value'].apply(lambda x: random.uniform(0, 1)) \n",
    "\n",
    "data.loc[(data['random_value']<=share_train)&(data[col_day]<=last_intime_day),'data_type'] = 'train'\n",
    "data.loc[(data['random_value']>share_train)&(data['random_value']<=share_train+share_validation)&(data[col_day]<=last_intime_day),\n",
    "      'data_type'] = 'valid'\n",
    "data.loc[(data['random_value']>share_train+share_validation)&(data[col_day]<=last_intime_day),'data_type'] = 'test'\n",
    "data.loc[(data[col_day]>last_intime_day),'data_type'] = 'oot'\n",
    "\n",
    "data= data.drop(['random_value'],axis = 1)\n",
    "\n",
    "train_mask = (data.data_type == 'train')& (data[col_base] == 1) \n",
    "valid_mask = (data.data_type == 'valid')& (data[col_base] == 1) \n",
    "test_mask = (data.data_type == 'test')& (data[col_base] == 1) \n",
    "oot_mask = (data.data_type == 'oot')& (data[col_base] == 1) \n",
    "\n",
    "print('Train observations:',data[train_mask].shape[0])\n",
    "print('Validation observations:',data[valid_mask].shape[0])\n",
    "print('Test observations:',data[test_mask].shape[0])\n",
    "print('Out-of-time observations:',data[oot_mask].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and WOE transformation of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't use such variables which have only 1 unique level. Grouping don't work for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descrip_train = data.loc[train_mask,cols_pred].describe(include='all').transpose()\n",
    "\n",
    "# comment the following 2 rows if there are no numerical predictors\n",
    "del_num = set(descrip_train[descrip_train['min']==descrip_train['max']].index)\n",
    "cols_pred_num = list(set(cols_pred_num) - del_num)\n",
    "\n",
    "# comment the following 2 rows if there are no categorical predictors\n",
    "del_cat = set(descrip_train[descrip_train['unique']==1].index)\n",
    "cols_pred_cat = list(set(cols_pred_cat) - del_cat)\n",
    "\n",
    "cols_pred = cols_pred_num + cols_pred_cat\n",
    "print('Variables',list(del_num),',',list(del_cat),'will not be further used as they have only 1 unique level.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic grouping of numerical and categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.grouping import Grouping\n",
    "\n",
    "grouping = Grouping(columns = cols_pred,group_count=5, min_samples=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping is fitted on training data and applied to the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping.fit(data[train_mask][cols_pred],data[train_mask][col_target])\n",
    "data_woe = grouping.transform(data)\n",
    "if len(grouping.bins_data_) > 0:\n",
    "    for v,g in grouping.bins_data_.items():\n",
    "        print('Variable:',v)\n",
    "        print('Bins:',g['bins'])\n",
    "        print('WOEs:',g['woes'])\n",
    "        print('nan WOE:',g['nan_woe'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save grouping to an external file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'woes'\n",
    "grouping.save(model_filename)\n",
    "print('Grouping data saved to file',model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the grouping from a file (don't forget to set the right filename) and add the WOE columns to the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_filename = 'woes'\n",
    "#grouping.load(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fitted WOEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "if len(grouping.bins_data_) > 0:\n",
    "    for v,g in grouping.bins_data_.items():\n",
    "        bin_names = []\n",
    "        bin_woes = []\n",
    "        for j in range(0,len(g['bins'])):\n",
    "            if (g['bins'].dtype == 'float64') and (j < len(g['bins'])-1):\n",
    "                bin_names.append(str(round(g['bins'][j],2))+' - '+str(round(g['bins'][j+1],2)))\n",
    "                bin_woes.append(g['woes'][j])\n",
    "            elif (g['bins'].dtype != 'float64'):\n",
    "                bin_names.append(g['bins'][j])\n",
    "                bin_woes.append(g['woes'][j])\n",
    "        bin_names.append('nan')\n",
    "        bin_woes.append(g['nan_woe'])\n",
    "        plt.figure(figsize = (10,3))\n",
    "        plt.plot(np.arange(len(bin_woes)),bin_woes, marker='o')#,'o')\n",
    "        plt.xticks(np.arange(len(bin_woes)), bin_names, rotation = 90)\n",
    "        plt.title(v)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add WOE variabes to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_woe = grouping.transform(data)\n",
    "for c in data_woe:\n",
    "    if c+'_WOE' in data:\n",
    "        data = data.drop(c+'_WOE', 1)\n",
    "        print('Column',c+'_WOE','dropped as it already existed in the data set.')\n",
    "data = data.join(data_woe,rsuffix='_WOE')\n",
    "print('Added WOE variables. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor power analysis\n",
    "\n",
    "Calculates IV and Gini of each predictor, sorts the predictors by their power. The power is calculated for each of the samples (train, validate, test, OOT). If one or more of the samples are empty, comment the according part of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_woe = [s + '_WOE' for s in cols_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.metrics import iv,gini,lift\n",
    "\n",
    "power_tab = []\n",
    "for j in range(0,len(cols_woe)):\n",
    "    power_tab.append({'Name':cols_woe[j],\n",
    "                    'IV Train':iv(data.loc[train_mask,col_target],data.loc[train_mask,cols_woe[j]]),\n",
    "                    'Gini Train':gini(data.loc[train_mask,col_target],-data.loc[train_mask,cols_woe[j]]),\n",
    "                    'IV Validate':iv(data.loc[valid_mask,col_target],data.loc[valid_mask,cols_woe[j]]),\n",
    "                    'Gini Validate':gini(data.loc[valid_mask,col_target],-data.loc[valid_mask,cols_woe[j]]),\n",
    "                    #'IV Test':iv(data.loc[test_mask,col_target],data.loc[test_mask,cols_woe[j]]),\n",
    "                    #'Gini Test':gini(data.loc[test_mask,col_target],-data.loc[test_mask,cols_woe[j]]),\n",
    "                    'IV OOT':iv(data.loc[oot_mask,col_target],data.loc[oot_mask,cols_woe[j]]),\n",
    "                    'Gini OOT':gini(data.loc[oot_mask,col_target],-data.loc[oot_mask,cols_woe[j]])\n",
    "                         })\n",
    "power_out = pd.DataFrame.from_records(power_tab)\n",
    "power_out = power_out.set_index('Name')\n",
    "power_out = power_out.sort_values('Gini Train',ascending=False)\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "display(power_out)\n",
    "pd.options.display.max_rows = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a shortlist of predictors to enter the modelling in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_shortlist = cols_woe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise logistic Regression\n",
    "\n",
    "We run stepwise logistic regression on training data set. We start with no predictor in the model and try to add predictors from list called **cols_shortlist** which is defined below (by default, we put there all the WOE variables).\n",
    "\n",
    "Stepwise process can be tuned using various parameters:\n",
    " - *initial_predictors*: set of starting predictors (useful for backward method)\n",
    " - *max_iter*: maximal number of iterations\n",
    " - *min_increase*: minimal marginal Gini contribution for predictor to be added\n",
    " - *max_decrease*: minimal marginal Gini diminution for predictor to be removed\n",
    " - *max_correlation*: maximal absolute value of correlation of predictors in the model (variable with larger correlation with existing predictors will not be added to the model)\n",
    " - *beta_sgn_correlation*: if this is set to True, all the betas in the model must have the same signature (all positive or all negative)\n",
    " - *penalty, C*: regularization parameters for logitic regression (sklearn library)\n",
    " - *correlation_sample*: for better performance, correlation matrix is calculated just on a sample of data. The size of the sample is set in this parameter\n",
    " - *selection_method*: stepwise or forward or backward\n",
    " \n",
    "The *fit* method can be called with two arguments *fit(X,y)* or with four agruments *fit(X_train,y_train,X_valid,y_valid)*. When called with four arguments, the Gini is measured on the validation sample (i.e. validation sample is used for decisions about what steps to be done in stepwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.model_selection import GiniStepwiseLogit\n",
    "\n",
    "clf = GiniStepwiseLogit(initial_predictors = set(), max_iter=1000, min_increase=0.7, max_decrease=0.5, max_predictors=0,\n",
    "                    max_correlation=0.45, beta_sgn_criterion=False, penalty='l2', C=10e10, correlation_sample=10000,\n",
    "                    selection_method='stepwise')\n",
    "\n",
    "clf.fit(data[train_mask][cols_shortlist],data[train_mask][col_target]\n",
    "#        ,data[valid_mask][cols_shortlist],data[valid_mask][col_target]\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = range(0,len(clf.model_progress_[clf.model_progress_['addrm']==0]['prednum']))\n",
    "pn = clf.model_progress_[clf.model_progress_['addrm']==0]['prednum']\n",
    "ginis = clf.model_progress_[clf.model_progress_['addrm']==0]['Gini']\n",
    "plt.figure(figsize = (7,7))\n",
    "plt.plot(it, ginis)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Gini')\n",
    "plt.title('Stepwise model selection')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 regularized Logistic Regression\n",
    "\n",
    "This alternative to stepwise feature selection is better for data sets with high number of covariate because it makes significantly less model fits when searching for the optimal predictor set. Use it for such data sets instead of stepwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.model_selection import L1GiniModelSelection\n",
    "\n",
    "clf = L1GiniModelSelection(steps = 100, grid_length=5, max_predictors=50,\n",
    "                           max_correlation=1, beta_sgn_criterion=False, correlation_sample = 10000)\n",
    "\n",
    "clf.fit(data[train_mask][cols_shortlist],data[train_mask][col_target],\n",
    "        data[valid_mask][cols_shortlist],data[valid_mask][col_target]\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_ = np.array(clf.coefs_)\n",
    "cs = clf.model_progress_['C']\n",
    "plt.figure(figsize = (7,7))\n",
    "plt.plot(np.log10(cs), coefs_)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.xlabel('log10(C)')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Logistic Regression Path')\n",
    "plt.axis('tight')\n",
    "plt.legend(cols_shortlist, loc='upper center', bbox_to_anchor=(1.20,1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,7))\n",
    "ginis = clf.model_progress_[['gini train','gini validate']]\n",
    "plt.plot(np.log10(cs), ginis)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.xlabel('log10(C)')\n",
    "plt.ylabel('Ginis')\n",
    "plt.title('Logistic Regression Path')\n",
    "plt.axis('tight')\n",
    "plt.legend(['Train','Validate'], loc='upper center', bbox_to_anchor=(1.20,1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_final_predictors = list(clf.final_predictors_)\n",
    "print('FINAL PREDICTORS SELECTED TO THE MODEL:',cols_final_predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column with the prediction (probability of default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_score = 'SCORE'\n",
    "\n",
    "data[col_score] = clf.predict(data)\n",
    "print('Column',col_score,'with the prediction added/modified. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scorecard table output\n",
    "Output the scorecard to a table. Stats are calculated on a subset of data given by the mask defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this mask is an union of masks for training, validation, testing and out of time data sets\n",
    "table_mask = train_mask|valid_mask|test_mask|oot_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard = []\n",
    "\n",
    "if len(grouping.bins_data_) > 0:\n",
    "    for v,g in grouping.bins_data_.items():\n",
    "        if v+'_WOE' in clf.final_predictors_:\n",
    "            ii = list(clf.final_predictors_).index(v+'_WOE')\n",
    "            bin_names = []\n",
    "            bin_woes = []\n",
    "            for j in range(0,len(g['bins'])):\n",
    "                if (g['bins'].dtype == 'float64') and (j < len(g['bins'])-1):\n",
    "                    subset = data[(table_mask) & (data[v]>=g['bins'][j]) & (data[v]<g['bins'][j+1])]\n",
    "                    obs = subset[col_base].sum()\n",
    "                    bads = subset[col_target].sum()\n",
    "                    scorecard.append({'Variable':v,\n",
    "                                     'Min':g['bins'][j],\n",
    "                                     'Max':g['bins'][j+1],\n",
    "                                     'Value':np.nan,\n",
    "                                     'WOE':g['woes'][j],\n",
    "                                     'Beta':clf.final_coef_[0][ii],\n",
    "                                     'BiXi':g['woes'][j]*clf.final_coef_[0][ii],\n",
    "                                     'Observations':obs,\n",
    "                                     'Bads':bads})\n",
    "                elif (g['bins'].dtype != 'float64'):\n",
    "                    subset = data[(table_mask) & (data[v]==g['bins'][j])]\n",
    "                    obs = subset[col_base].sum()\n",
    "                    bads = subset[col_target].sum()\n",
    "                    scorecard.append({'Variable':v,\n",
    "                                     'Min':np.nan,\n",
    "                                     'Max':np.nan,\n",
    "                                     'Value':g['bins'][j],\n",
    "                                     'WOE':g['woes'][j],\n",
    "                                     'Beta':clf.final_coef_[0][ii],\n",
    "                                     'BiXi':g['woes'][j]*clf.final_coef_[0][ii],\n",
    "                                     'Observations':obs,\n",
    "                                     'Bads':bads})\n",
    "            subset = data[(table_mask) & (pd.isnull(data[v]))]\n",
    "            obs = subset[col_base].sum()\n",
    "            bads = subset[col_target].sum()\n",
    "            scorecard.append({'Variable':v,\n",
    "                             'Min':np.nan,\n",
    "                             'Max':np.nan,\n",
    "                             'Value':'null',\n",
    "                             'WOE':g['nan_woe'],\n",
    "                             'Beta':clf.final_coef_[0][ii],\n",
    "                             'BiXi':g['nan_woe']*clf.final_coef_[0][ii],\n",
    "                             'Observations':obs,\n",
    "                             'Bads':bads})\n",
    "\n",
    "all_obs = data[table_mask][col_base].sum()\n",
    "all_bads = data[table_mask][col_target].sum() \n",
    "scorecard.append({'Variable':'_Intercept',\n",
    "                  'Value':np.nan,\n",
    "                  'Min':np.nan,\n",
    "                  'Max':np.nan,\n",
    "                  'WOE':0,\n",
    "                  'Beta':clf.final_model_.intercept_[0],\n",
    "                  'BiXi':1*clf.final_model_.intercept_[0],\n",
    "                  'Observations':all_obs,\n",
    "                  'Bads':all_bads})\n",
    "\n",
    "scorecard_out = pd.DataFrame.from_records(scorecard)[\n",
    "    ['Variable','Min','Max','Value','WOE','Beta','BiXi','Observations','Bads']]\n",
    "scorecard_out2 = scorecard_out.copy()\n",
    "scorecard_out2['Value'] = scorecard_out2['Value'] + ','\n",
    "scorecard_out2 = scorecard_out2.groupby(['Variable','WOE']).agg({\n",
    "    'Variable':min,'Min':min,'Max':max,'Value':sum,'WOE':min,'Beta':min,'BiXi':min,'Observations':sum,'Bads':sum\n",
    "})\n",
    "scorecard_out2.loc[pd.isnull(scorecard_out2['Value']),'Value'] = ','\n",
    "scorecard_out2['Value'] = scorecard_out2['Value'].astype(str).str[:-1]\n",
    "scorecard_out2['Goods'] = scorecard_out2['Observations'] - scorecard_out2['Bads']\n",
    "scorecard_out2['Bad Rate'] = scorecard_out2['Bads']/scorecard_out2['Observations']\n",
    "all_badrate = all_bads/all_obs\n",
    "scorecard_out2['Bad Rate relative to population'] = scorecard_out2['Bad Rate'] / all_badrate\n",
    "scorecard_out2['% Observations'] = scorecard_out2['Observations'] / all_obs\n",
    "scorecard_out2['% Bads'] = scorecard_out2['Bads'] / all_bads\n",
    "scorecard_out2['% Goods'] = scorecard_out2['Goods'] / (all_obs-all_bads)\n",
    "scorecard_out2['Lift'] = scorecard_out2['% Bads'] / scorecard_out2['% Goods']\n",
    "scorecard_out2 = pd.DataFrame.from_records(scorecard_out2.sort_values(['Variable','Min','Max','WOE','Value']))\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "#display(scorecard_out)\n",
    "display(scorecard_out2)\n",
    "pd.options.display.max_rows = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance characteristics\n",
    "Performance characteristics of the model (Gini, Lift) and their visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.metrics import gini, lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Train Gini:',gini(data[train_mask][col_target],data[train_mask][col_score]))\n",
    "print ('Valid Gini:',gini(data[valid_mask][col_target],data[valid_mask][col_score]))\n",
    "#print ('Test Gini:',gini(data[test_mask][col_target],data[test_mask][col_score]))\n",
    "print ('OOT Gini:',gini(data[oot_mask][col_target],data[oot_mask][col_score]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Train Lift 10%:',lift(data[train_mask][col_target],-data[train_mask][col_score],10))\n",
    "print ('Valid Lift 10%:',lift(data[valid_mask][col_target],-data[valid_mask][col_score],10))\n",
    "#print ('Test Lift 10%:',lift(data[test_mask][col_target],-data[test_mask][col_score],10))\n",
    "print ('OOT Lift 10%:',lift(data[oot_mask][col_target],-data[oot_mask][col_score],10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate data for Gini and Lift curves\n",
    "from scoring.tools import calculate_gini_and_lift\n",
    "train_stats, train_curve = calculate_gini_and_lift(data[train_mask], col_target, col_score, pct = 10)\n",
    "train_curve = list(zip(*train_curve))\n",
    "valid_stats, valid_curve = calculate_gini_and_lift(data[valid_mask], col_target, col_score, pct = 10)\n",
    "valid_curve = list(zip(*valid_curve))\n",
    "#test_stats, test_curve = calculate_gini_and_lift(data[test_mask], col_target, col_score, pct = 10)\n",
    "#test_curve = list(zip(*test_curve))\n",
    "oot_stats, oot_curve = calculate_gini_and_lift(data[oot_mask], col_target, col_score, pct = 10)\n",
    "oot_curve = list(zip(*oot_curve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,7))\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.plot([0] + list(train_curve[2]),[0] + list(train_curve[3]), label = 'train', color = 'g')\n",
    "plt.plot([0] + list(valid_curve[2]), [0] + list(valid_curve[3]), label = 'validation', color = 'r')\n",
    "#plt.plot([0] + list(test_curve[2]), [0] + list(test_curve[3]), label = 'test', color = 'y')\n",
    "plt.plot([0] + list(oot_curve[2]), [0] + list(oot_curve[3]), label = 'out-of-time', color = 'b')\n",
    "plt.plot(list(range(0, 101)), list(range(0, 101)), color='k')\n",
    "plt.xlabel('Cumulative good count')\n",
    "plt.ylabel('Cumulative bad count')\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.axis([0, 100, 0, max(train_curve[1])+0.5])\n",
    "plt.plot(train_curve[0], train_curve[1], label = 'train', color = 'g')\n",
    "plt.plot(valid_curve[0], valid_curve[1], label = 'validation', color = 'r')\n",
    "#plt.plot(test_curve[0], test_curve[1], label = 'test', color = 'y')\n",
    "plt.plot(oot_curve[0], oot_curve[1], label = 'out-of-time', color = 'b')\n",
    "plt.xlabel('Cumulative count [%]')\n",
    "plt.ylabel('Lift')\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def proc_gini(x,y,z):\n",
    "    fpr, tpr, _ = roc_curve(x[y], x[z], pos_label=0)\n",
    "    roc_gini = (auc(fpr, tpr)-0.5)*2\n",
    "    return roc_gini\n",
    "%matplotlib inline\n",
    "plt.figure(figsize = (10,7))\n",
    "grouped = data[train_mask].groupby(col_month, axis=0)\n",
    "res_train= grouped.apply(proc_gini, col_target ,col_score)\n",
    "plt.plot(range(len(res_train)),-res_train, linewidth=2.0,label='Train', color = 'g', marker='o')\n",
    "\n",
    "grouped = data[valid_mask].groupby(col_month, axis=0)\n",
    "res_valid= grouped.apply(proc_gini, col_target ,col_score)\n",
    "plt.plot(range(len(res_valid)),-res_valid, linewidth=2.0,label='Validation', color = 'r', marker='o')\n",
    "\n",
    "grouped = data[oot_mask].groupby(col_month, axis=0)\n",
    "res_oot= grouped.apply(proc_gini, col_target ,col_score)\n",
    "plt.plot(range(len(res_train),len(res_train)+len(res_oot)),-res_oot, linewidth=2.0,label='OOT', color = 'b', marker='o')\n",
    "\n",
    "#grouped = data[test_mask].groupby(col_month, axis=0)\n",
    "#res_test= grouped.apply(proc_gini, col_target ,col_score)\n",
    "#plt.plot(range(len(res_test)),-res_test, linewidth=2.0,label='Test', color = 'y', marker='o')\n",
    "\n",
    "plt.xticks(range(len(res_train)+len(res_oot)), np.sort(data[col_month].unique()), rotation=45)\n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.title('Gini by months')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Gini')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import plot_calib\n",
    "plot_calib(data[col_score],data[col_target],bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "Calculate and visualise correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cormat = data[cols_final_predictors].corr()\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 15})\n",
    "sns.set()\n",
    "a4_dims = (12,10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=a4_dims, dpi=50)\n",
    "fig.suptitle('Correlations of Predictors',fontsize=25)\n",
    "sns.heatmap(cormat, ax=ax, annot=True, fmt=\"0.1f\", linewidths=.5, annot_kws={\"size\":15},cmap=\"OrRd\")\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the list of the highest correlation (restricted to correlations that are, in absolute value, higher than *max_ok_correlation* parameter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ok_correlation = 0.0\n",
    "\n",
    "# find highest pairwise correlation (correlation greater than .. in absolute value)\n",
    "hicors = []\n",
    "for i in range(0,len(cormat)):\n",
    "    for j in range(0,len(cormat)):\n",
    "        if ((cormat.iloc[i][j] > max_ok_correlation or cormat.iloc[i][j] < -max_ok_correlation) and i < j):\n",
    "            hicors.append((i,j,cormat.index[i],cormat.index[j],cormat.iloc[i][j],abs(cormat.iloc[i][j])))\n",
    "hicors.sort(key= lambda tup: tup[5], reverse=True)\n",
    "\n",
    "hicors2 = pd.DataFrame(list(zip(*list(zip(*hicors))[2:5])))\n",
    "\n",
    "# print list of highest correlations\n",
    "hicors2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time stability of predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the charts to be drawn more quickly, set a reasonable size of sample\n",
    "sample_size = 10000\n",
    "\n",
    "data_chart = data.copy()\n",
    "if sample_size < len(data_chart):\n",
    "    data_chart = data_chart.sample(sample_size, random_state=241)\n",
    "\n",
    "for j in list(clf.final_predictors_):\n",
    "    sns.set(rc={\"figure.figsize\": (30, 20)})\n",
    "\n",
    "    for i in data_chart[j].unique():\n",
    "        tmp = pd.DataFrame(np.sort(data_chart[col_month].unique()), columns = [col_month] ).join(\n",
    "        pd.DataFrame(data_chart.groupby(by = [j,col_month])[col_target].sum()).loc[i], on = col_month).set_index(col_month).join(\n",
    "        data_chart.groupby(by = [j,col_month])[col_base].sum().loc[i]).join(data_chart.groupby(by = [col_month])[col_base].sum(), rsuffix='_all')\n",
    "        tmp['bad_rate'] = tmp[col_target]/tmp[col_base]\n",
    "        tmp['obs_rate'] = tmp[col_base]/tmp[col_base+str('_all')]\n",
    "        plt.subplot(221)\n",
    "        plt.plot(range(len(data_chart[col_month].unique())),tmp['bad_rate'],\n",
    "                label = i,linewidth=4)\n",
    "        plt.xticks(range(len(data_chart[col_month].unique())), np.sort(data_chart[col_month].unique()), rotation=45,\n",
    "                   fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "        plt.title(j+str(': bad rate'),fontsize=25)\n",
    "        \n",
    "        plt.subplot(222)\n",
    "        plt.plot(range(len(data_chart[col_month].unique())),tmp['obs_rate'],\n",
    "                label = i,linewidth=4) \n",
    "        plt.xticks(range(len(data_chart[col_month].unique())), np.sort(data_chart[col_month].unique()), rotation=45,\n",
    "                   fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "        plt.title(j+str(': observation rate'),fontsize=25)\n",
    "        \n",
    "    plt.legend(loc='center', bbox_to_anchor=(1.2, 0.5),fontsize=25) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with another score\n",
    "Similar charts to what were already done for the new scorecard are now drawn to compare the new scorecard to another scorecard. The value of the old score should be saved in a special column of original data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_oldscore = 'OLD_SCORE'\n",
    "\n",
    "#if the score gives the complementary probability (of non-default), run this:\n",
    "data[col_oldscore]=1-data[col_oldscore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('New score Gini (validation sample):',gini(data[valid_mask][col_target],data[valid_mask][col_score]))\n",
    "print ('Old score Gini (validation sample):',gini(data[valid_mask][col_target],data[valid_mask][col_oldscore]))\n",
    "print ('New score Gini (out-of-time sample):',gini(data[oot_mask][col_target],data[oot_mask][col_score]))\n",
    "print ('Old score Gini (out-of-time sample):',gini(data[oot_mask][col_target],data[oot_mask][col_oldscore]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('New score Lift 10% (validation sample):',lift(data[valid_mask][col_target],-data[valid_mask][col_score],10))\n",
    "print ('Old score Lift 10% (validation sample):',lift(data[valid_mask][col_target],-data[valid_mask][col_oldscore],10))\n",
    "print ('New score Lift 10% (out-of-time sample):',lift(data[oot_mask][col_target],-data[oot_mask][col_score],10))\n",
    "print ('Old score Lift 10% (out-of-time sample):',lift(data[oot_mask][col_target],-data[oot_mask][col_oldscore],10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.tools import calculate_gini_and_lift\n",
    "newscore_stats, newscore_curve = calculate_gini_and_lift(data[valid_mask|oot_mask], col_target, col_score, pct = 10)\n",
    "newscore_curve = list(zip(*newscore_curve))\n",
    "oldscore_stats, oldscore_curve = calculate_gini_and_lift(data[valid_mask|oot_mask], col_target, col_oldscore, pct = 10)\n",
    "oldscore_curve = list(zip(*oldscore_curve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,7))\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.plot([0] + list(newscore_curve[2]),[0] + list(newscore_curve[3]), label = 'new score', color = 'g')\n",
    "plt.plot([0] + list(oldscore_curve[2]), [0] + list(oldscore_curve[3]), label = 'old score', color = 'r')\n",
    "plt.plot(list(range(0, 101)), list(range(0, 101)), color='k')\n",
    "plt.xlabel('Cumulative good count')\n",
    "plt.ylabel('Cumulative bad count')\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.axis([0, 100, 0, max(train_curve[1])+0.5])\n",
    "plt.plot(newscore_curve[0], newscore_curve[1], label = 'new score', color = 'g')\n",
    "plt.plot(oldscore_curve[0], oldscore_curve[1], label = 'old score', color = 'r')\n",
    "plt.xlabel('Cumulative count [%]')\n",
    "plt.ylabel('Lift')\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def proc_gini(x,y,z):\n",
    "    fpr, tpr, _ = roc_curve(x[y], x[z], pos_label=0)\n",
    "    roc_gini = (auc(fpr, tpr)-0.5)*2\n",
    "    return roc_gini\n",
    "%matplotlib inline\n",
    "plt.figure(figsize = (10,7))\n",
    "grouped = data[valid_mask|oot_mask].groupby(col_month, axis=0)\n",
    "res_new= grouped.apply(proc_gini, col_target ,col_score)\n",
    "plt.plot(range(len(res_new)),-res_new, linewidth=2.0,label='new score', color = 'g', marker='o')\n",
    "\n",
    "grouped = data[valid_mask|oot_mask].groupby(col_month, axis=0)\n",
    "res_old= grouped.apply(proc_gini, col_target ,col_oldscore)\n",
    "plt.plot(range(len(res_old)),-res_old, linewidth=2.0,label='old score', color = 'r', marker='o')\n",
    "\n",
    "plt.xticks(range(len(res_valid)+len(res_oot)), np.sort(data[col_month].unique()), rotation=45)\n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.title('Gini by months')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Gini')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_score_dec = pd.DataFrame(pd.qcut(data[valid_mask|oot_mask][col_score],10,labels=False))\n",
    "old_score_dec = pd.DataFrame(pd.qcut(data[valid_mask|oot_mask][col_oldscore],10,labels=False))\n",
    "dec_data = pd.concat([new_score_dec,old_score_dec,data[valid_mask|oot_mask][col_target]],axis=1)\n",
    "dec_data_agg = dec_data.groupby([col_oldscore,col_score]).agg({\n",
    "    col_target:['sum','count']\n",
    "})\n",
    "dec_data_agg.columns = ['bads','obs']\n",
    "dec_data_agg2 = dec_data.groupby([col_oldscore]).agg({\n",
    "    col_target:['count']\n",
    "})\n",
    "dec_data_agg2.columns = ['old decile obs']\n",
    "dec_data_all = dec_data_agg.reset_index().join(dec_data_agg2,on=[col_oldscore]).set_index([col_oldscore,col_score])\n",
    "dec_data_all['default rate'] = dec_data_all['bads']/dec_data_all['obs']\n",
    "dec_data_all['share'] = dec_data_all['obs']/dec_data_all['old decile obs']\n",
    "matrix_DR = np.matrix(dec_data_all.unstack()[['default rate']])\n",
    "matrix_OS = np.matrix(dec_data_all.unstack()[['share']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default rate for each decile/decile combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(matrix_DR, annot=True, cmap=sns.cubehelix_palette(light=1, as_cmap=True))\n",
    "ax.set_ylabel('old score decile')\n",
    "ax.set_xlabel('new score decile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(matrix_OS, annot=True, cmap=sns.cubehelix_palette(light=1, as_cmap=True))\n",
    "ax.set_ylabel('old score decile')\n",
    "ax.set_xlabel('new score decile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on short target\n",
    "If there is also a shorter (e.g. FPD30) target in the original dataset, we draw also charts for performance on this target in this part of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of the short target column\n",
    "col_short = \"FPD\"\n",
    "#name of the short target's base column\n",
    "col_shortbase = \"FPD_BASE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have base column in your data set, the following code adds it (value 1 for each observation). **Otherwise, don't run it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[col_shortbase] = 1\n",
    "print('Column',col_shortbase,'added/modified. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortbase_mask = ((data.data_type == 'valid')|(data.data_type == 'oot'))& (data[col_shortbase] == 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Short target Gini:',gini(data[shortbase_mask][col_short],data[shortbase_mask][col_score]))\n",
    "print ('Short target Lift 10%:',lift(data[shortbase_mask][col_short],-data[shortbase_mask][col_score],10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def proc_gini(x,y,z):\n",
    "    fpr, tpr, _ = roc_curve(x[y], x[z], pos_label=0)\n",
    "    roc_gini = (auc(fpr, tpr)-0.5)*2\n",
    "    return roc_gini\n",
    "%matplotlib inline\n",
    "plt.figure(figsize = (10,7))\n",
    "grouped = data[valid_mask|oot_mask].groupby(col_month, axis=0)\n",
    "res_new= grouped.apply(proc_gini, col_target ,col_score)\n",
    "plt.plot(range(len(res_new)),-res_new, linewidth=2.0,label='target', color = 'g', marker='o')\n",
    "\n",
    "grouped = data[shortbase_mask].groupby(col_month, axis=0)\n",
    "res_short= grouped.apply(proc_gini, col_short ,col_score)\n",
    "plt.plot(range(len(res_short)),-res_short, linewidth=2.0,label='short target', color = 'r', marker='o')\n",
    "\n",
    "plt.xticks(range(len(res_short)), np.sort(data[col_month].unique()), rotation=45)\n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.title('Gini by months')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Gini')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
