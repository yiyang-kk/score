{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Credit Python Scoring Workflow v.0.2.2\n",
    "\n",
    "**Contributors:**\n",
    "- Pavel SÅ¯va (HCI Reserach & Development)\n",
    "- Marek Teller (HCI Reserach & Development)\n",
    "- Martin Kotek (HCI Reserach & Development)\n",
    "- Jan Zeller (HCI Research & Development)\n",
    "- Sergey Gerasimov (HCRU Big Data & Scoring)\n",
    "- Valentina Kalenichenko (HCRU Big Data & Scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "- time, datetime - ability to get current time for logs\n",
    "- math - basic mathematical functions (as logarithm etc.))\n",
    "- random - generate random selection from probability distributions\n",
    "- NumPy - for scientific, mathematical, numerical calculations\n",
    "- Pandas - for efficient work with large data structures\n",
    "- cx_Oracle and sqlalchemy - for loading data from Oracle database (DWH etc.)\n",
    "- statsmodels - library with some statistical functions and models\n",
    "- scikit-learn - all important machine learning (and statistical) algorithms used for training the models\n",
    "- matplotlib - for plotting the charts\n",
    "- seaborn - for statistical visualisations\n",
    "- os - for setting output paths for generated image files\n",
    "- scoring - functions and objects from scoring.py (part of our scoring workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import operator\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cx_Oracle\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils import as_float_array\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os.path\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'C:/py_src/scoring/hcfsc')\n",
    "import scoring\n",
    "#import importlib\n",
    "#importlib.reload(scoring)\n",
    "#importlib.reload(scoring.plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set()\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 15\n",
    "output_folder = 'documentation'\n",
    "\n",
    "if not os.path.exists(output_folder): os.makedirs(output_folder)\n",
    "if not os.path.exists(output_folder+'/performance'): os.makedirs(output_folder+'/performance')\n",
    "if not os.path.exists(output_folder+'/predictors'): os.makedirs(output_folder+'/predictors')\n",
    "if not os.path.exists(output_folder+'/stability'): os.makedirs(output_folder+'/stability')\n",
    "if not os.path.exists(output_folder+'/analysis'): os.makedirs(output_folder+'/analysis')\n",
    "if not os.path.exists(output_folder+'/model'): os.makedirs(output_folder+'/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "Importing data from a CSV file. It is important to set the following parameters:\n",
    "\n",
    "encoding: usually 'utf-8' or windows-xxxx on Windows machines, where xxxx is 1250 for Central Europe, 1251 for Cyrilic etc.\n",
    "sep: separator of columns in the file\n",
    "decimal: decimal dot or coma\n",
    "index_col: which columns is used as index - should be the unique credit case identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:/Analyses/HQ_20171213_PythonWorkflow/ExampleData2.csv', sep = ',', decimal = '.', \n",
    "                   encoding = 'utf-8', index_col = 'ID', low_memory = False)\n",
    "print('Data loaded on',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally the data can be loaded also from a database. The function read_sql uses cache, so the data don't have to be downloaded from the database repeatedly. The cache will be located in a new folder called **db_cache**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#engine = create_engine('oracle://HCHQ_SUVA[AP_RISK]:password@(DESCRIPTION =(ADDRESS = (PROTOCOL = TCP)(HOST = KZCL01.KZ.PROD)(PORT = 1521))(CONNECT_DATA =(UR = A)(SERVER = DEDICATED)(SERVICE_NAME = HDWKZ.KZ.PROD)))', echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scoring.db import read_sql\n",
    "#data = read_sql('select * from owner_dwh.f_application_base_tt where rownum=1',engine, index_col = 'skp_application')\n",
    "#print('Data loaded on',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to download data from the database again (and not from cache), use the parameter refresh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scoring.db import read_sql\n",
    "#data = read_sql('select * from owner_dwh.f_application_base_tt where rownum=1',engine, index_col = 'skp_application',refresh=True)\n",
    "#print('Data loaded on',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows:',data.shape[0])\n",
    "print('Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata definitions\n",
    "Assigning ID column, target column, time column and month column. The month column don't have to exist in the dataset, it will be created later in this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of the time column\n",
    "col_time = \"TIME\"\n",
    "#name of the month column\n",
    "col_month = \"MONTH\"\n",
    "#name of the day column\n",
    "col_day = \"DAY\"\n",
    "#name of the target column\n",
    "col_target = \"DEF\"\n",
    "#name of the base column\n",
    "col_base = \"BASE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have base column in your data set, the following code adds it (value 1 for each observation). **Otherwise, don't run it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[col_base] = 1\n",
    "print('Column',col_base,'added/modified. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the month and day column from the time column\n",
    "- take the time column and tell in which format the time is saved in\n",
    "- strip the format just to year, month, day string\n",
    "- convert the string to number\n",
    "- the new column will be added to the dataset as day\n",
    "- truncate this column to just year and month and add it to dataset as month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,col_day] = pd.to_numeric(pd.to_datetime(data[col_time], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y%m%d'))\n",
    "data[col_month] = data[col_day].apply(lambda x: math.trunc(x/100))\n",
    "print('Columns',col_day,'and',col_month,'added/modified. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the predictors list from a csv file. The csv should have just one column, without any header, containing the name of the variables that should be used as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_pred = list(pd.read_csv(r'C:/Analyses/HQ_20171213_PythonWorkflow/ExamplePredList.csv', sep = ',', decimal = '.', \n",
    "                   encoding = 'windows-1251', low_memory = False, header = None)[0])\n",
    "\n",
    "cols_pred_cat = list(set([c[0] for c in list(zip(data.columns, data.dtypes)) if c[1]=='O']) & set(cols_pred))\n",
    "cols_pred_num = list(set([c[0] for c in list(zip(data.columns, data.dtypes)) if c[1]!='O']) & set(cols_pred))\n",
    "\n",
    "# ALTERNATIVELY, DEFINE THE PREDICTOR NAMES MANUALLY\n",
    "\n",
    "#cols_pred_num = [\"Numerical_1\",\"Numerical_2\",\"Numerical_3\",\"Numerical_4\",\"Numerical_5\"]\n",
    "#cols_pred_cat = [\"Categorical_1\",\"Categorical_2\",\"Categorical_3\",\"Categorical_4\",\"Categorical_5\"]\n",
    "\n",
    "\n",
    "cols_pred = cols_pred_num + cols_pred_cat\n",
    "\n",
    "print(len(cols_pred_num),'numerical predictors:')\n",
    "for p in cols_pred_num: print(p)\n",
    "print('-'*100)\n",
    "print()\n",
    "print(len(cols_pred_cat),'categorical predictors:')\n",
    "for p in cols_pred_cat: print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descrip = data.describe(include='all').transpose()\n",
    "pd.options.display.max_rows = 1000\n",
    "display(descrip)\n",
    "pd.options.display.max_rows = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**exploreNominal** and **exploreInterval** functions give graphical data exploratory analyses. They can also output even more comprehensive analysis into html files. You just need to specify the folder for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.data_exploration import exploreNominal, exploreInterval\n",
    "\n",
    "for c in sorted(cols_pred_num):\n",
    "    exploreInterval(data[c],data[col_target],htmlOut=True,OutFolder2='dexp',bin_count=10)\n",
    "\n",
    "for c in sorted(cols_pred_cat):\n",
    "    exploreNominal(data[c],data[col_target],htmlOut=True,OutFolder2='dexp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**explore_df** function creates a simple text report about the important variable. The report can be then printed either to the screen or to a file.\n",
    "\n",
    "In the following code, only such part of data that has col_base = 1 is analyzed. You can remove the condition if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.data_exploration import explore_df\n",
    "st = explore_df(data[data[col_base]==1],col_month,col_target,cols_pred)\n",
    "print(st,file=open(\"data_exp.txt\", \"w\"))\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default rate in time**: Simple visualisation of observation count and default rate in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import plot_dataset\n",
    "plot_dataset(data,col_month,col_target,'Count and bad rate',col_base,savepath=output_folder+'/analysis/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "\n",
    "- Split data into five parts (in time training, in time validation, in time test, out of time, historical out of time).\n",
    "- Adds a new column indicating to which part the observations belong.\n",
    "- The split parameters are set at the beginning of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_train = 0.6\n",
    "share_validation = 0.2\n",
    "first_train_day = 20170201 #first day of train, everything before it will be considered \"old\", historical out of time\n",
    "first_oot_day = 20170601 #first day of \"new\" out of time, i.e. out of time after train\n",
    "\n",
    "data['random_value'] = 1\n",
    "data['random_value'] = data['random_value'].apply(lambda x: random.uniform(0, 1)) \n",
    "\n",
    "data.loc[(data['random_value']<=share_train)&(data[col_day]<first_oot_day)&\n",
    "         (data[col_day]>=first_train_day),'data_type'] = 'train'\n",
    "data.loc[(data['random_value']>share_train)&(data['random_value']<=share_train+share_validation)&(data[col_day]<first_oot_day)&\n",
    "         (data[col_day]>=first_train_day),'data_type'] = 'valid'\n",
    "data.loc[(data['random_value']>share_train+share_validation)&(data[col_day]<first_oot_day)&\n",
    "         (data[col_day]>=first_train_day),'data_type'] = 'test'\n",
    "data.loc[(data[col_day]>=first_oot_day),'data_type'] = 'oot'\n",
    "data.loc[(data[col_day]<first_train_day),'data_type'] = 'hoot'\n",
    "\n",
    "data= data.drop(['random_value'],axis = 1)\n",
    "\n",
    "train_mask = (data.data_type == 'train')& (data[col_base] == 1) \n",
    "valid_mask = (data.data_type == 'valid')& (data[col_base] == 1) \n",
    "test_mask = (data.data_type == 'test')& (data[col_base] == 1) \n",
    "oot_mask = (data.data_type == 'oot')& (data[col_base] == 1) \n",
    "hoot_mask = (data.data_type == 'hoot')& (data[col_base] == 1) \n",
    "\n",
    "print('Train observations:',data[train_mask].shape[0])\n",
    "print('Validation observations:',data[valid_mask].shape[0])\n",
    "print('Test observations:',data[test_mask].shape[0])\n",
    "print('Out-of-time observations:',data[oot_mask].shape[0])\n",
    "print('Historical-out-of-time observations:',data[hoot_mask].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data summary (number of defaults, number in base, number of observations, default rate) by month and by sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary = data.groupby([col_month,'data_type']).aggregate({\n",
    "    col_target:'sum',col_base:['sum','count']\n",
    "})\n",
    "data_summary.columns = [col_target,col_base,'Rows']\n",
    "data_summary[col_target+' rate'] = data_summary[col_target]/data_summary[col_base]\n",
    "\n",
    "data_summary = data_summary.reset_index(level='data_type').pivot(columns='data_type')\n",
    "display(data_summary)\n",
    "data_summary.to_csv(output_folder+'/analysis/summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and WOE transformation of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't use such variables which have only 0 or 1 unique level. Grouping don't work for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descrip_train = data.loc[train_mask,cols_pred].describe(include='all').transpose()\n",
    "\n",
    "# comment the following 3 rows if there are no numerical predictors\n",
    "del_num = set(descrip_train[(descrip_train['min']==descrip_train['max'])|(descrip_train['count']==0)].index)\n",
    "del_num = set(cols_pred_num) & del_num\n",
    "cols_pred_num = list(set(cols_pred_num) - del_num)\n",
    "\n",
    "# comment the following 3 rows if there are no categorical predictors\n",
    "del_cat = set(descrip_train[(descrip_train['unique']==1)|(descrip_train['count']==0)].index)\n",
    "del_cat = set(cols_pred_cat) & del_cat\n",
    "cols_pred_cat = list(set(cols_pred_cat) - del_cat)\n",
    "\n",
    "cols_pred = cols_pred_num + cols_pred_cat\n",
    "print('Variables',list(del_num),',',list(del_cat),'will not be further used as they have only 1 unique level.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic grouping of numerical and categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.grouping import Grouping\n",
    "\n",
    "grouping = Grouping(columns = cols_pred,group_count=5, min_samples=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping is fitted on training data and applied to the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping.fit(data[train_mask][cols_pred],data[train_mask][col_target])\n",
    "data_woe = grouping.transform(data)\n",
    "if len(grouping.bins_data_) > 0:\n",
    "    for v,g in grouping.bins_data_.items():\n",
    "        print('Variable:',v)\n",
    "        print('Bins:',g['bins'])\n",
    "        print('WOEs:',g['woes'])\n",
    "        print('nan WOE:',g['nan_woe'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save grouping to an external file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'woes'\n",
    "grouping.save(model_filename)\n",
    "print('Grouping data saved to file',model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the grouping from a file (don't forget to set the right filename) and add the WOE columns to the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_filename = 'woes'\n",
    "#grouping.load(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fitted WOEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scoring.plot import print_binning_stats\n",
    "\n",
    "if len(grouping.bins_data_) > 0:\n",
    "    for v,g in sorted(grouping.bins_data_.items(), key=operator.itemgetter(0)):\n",
    "        print('-'*125)\n",
    "        print(v)\n",
    "        print_binning_stats(data[train_mask][[col_target, v]], v, col_target, g['bins'], g['woes'], g['nan_woe']\n",
    "                           ,savepath=output_folder+'/predictors/'+v+'_')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add WOE variabes to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_woe = grouping.transform(data)\n",
    "for c in data_woe:\n",
    "    if c+'_WOE' in data:\n",
    "        data = data.drop(c+'_WOE', 1)\n",
    "        print('Column',c+'_WOE','dropped as it already existed in the data set.')\n",
    "data = data.join(data_woe,rsuffix='_WOE')\n",
    "print('Added WOE variables. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor power analysis\n",
    "\n",
    "Calculates IV and Gini of each predictor, sorts the predictors by their power. The power is calculated for each of the samples (train, validate, test, OOT). If one or more of the samples are empty, comment the according part of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_woe = [s + '_WOE' for s in cols_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.metrics import iv,gini,lift\n",
    "\n",
    "power_tab = []\n",
    "for j in range(0,len(cols_woe)):\n",
    "    power_tab.append({'Name':cols_woe[j],\n",
    "                    'IV Train':iv(data.loc[train_mask,col_target],data.loc[train_mask,cols_woe[j]]),\n",
    "                    'Gini Train':gini(data.loc[train_mask,col_target],-data.loc[train_mask,cols_woe[j]]),\n",
    "                    'IV Validate':iv(data.loc[valid_mask,col_target],data.loc[valid_mask,cols_woe[j]]),\n",
    "                    'Gini Validate':gini(data.loc[valid_mask,col_target],-data.loc[valid_mask,cols_woe[j]]),\n",
    "                    'IV Test':iv(data.loc[test_mask,col_target],data.loc[test_mask,cols_woe[j]]),\n",
    "                    'Gini Test':gini(data.loc[test_mask,col_target],-data.loc[test_mask,cols_woe[j]]),\n",
    "                    'IV OOT':iv(data.loc[oot_mask,col_target],data.loc[oot_mask,cols_woe[j]]),\n",
    "                    'Gini OOT':gini(data.loc[oot_mask,col_target],-data.loc[oot_mask,cols_woe[j]]),\n",
    "                    'IV HOOT':iv(data.loc[hoot_mask,col_target],data.loc[hoot_mask,cols_woe[j]]),\n",
    "                    'Gini HOOT':gini(data.loc[hoot_mask,col_target],-data.loc[hoot_mask,cols_woe[j]])\n",
    "                         })\n",
    "power_out = pd.DataFrame.from_records(power_tab)\n",
    "power_out = power_out.set_index('Name')\n",
    "power_out = power_out.sort_values('Gini Train',ascending=False)\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "display(power_out)\n",
    "pd.options.display.max_rows = 15\n",
    "power_out.to_csv(output_folder+'/predictors/covariates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 regularized Logistic Regression\n",
    "\n",
    "Efficient way how to select subset of predictors from a very big set of covariate. Uses grid search through value of L1 regularization parameter. The best model selected based on validation Gini.\n",
    "\n",
    "Interation process can be tuned using various parameters:\n",
    " - *steps*: number of steps of grid search\n",
    " - *grid_length*: length of the grid for grid search\n",
    " - *max_predictors*: maximal number of predictors to enter the model. Ignored if set to 0.\n",
    " - *max_correlation*: maximal absolute value of correlation of predictors in the model (variable with larger correlation with existing predictors will not be added to the model)\n",
    " - *beta_sgn_criterion*: if this is set to True, all the betas in the model must have the same signature (all positive or all negative)\n",
    " - *stop_immediately*: the iteration process will be stopped immediately after a model which is not fulfilling the criteria (max_predictors, max_correlation or beta_sgn_criterion) is found. No further models are searched for.\n",
    " - *correlation_sample*: for better performance, correlation matrix is calculated just on a sample of data. The size of the sample is set in this parameter\n",
    " \n",
    "The *fit* method can be called with two arguments *fit(X,y)* or with four agruments *fit(X_train,y_train,X_valid,y_valid)*. When called with four arguments, the Gini is measured on the validation sample (i.e. validation sample is used for decisions about what steps to be done in stepwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a shortlist of predictors to enter the modelling in the next steps.\n",
    "cols_shortlist = cols_woe\n",
    "#cols_shortlist = list(set(cols_woe) - set(['unwanted1','unwanted2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.model_selection import L1GiniModelSelection\n",
    "\n",
    "modelL1 = L1GiniModelSelection(steps = 100, grid_length=5, max_predictors=200,\n",
    "                           max_correlation=1, beta_sgn_criterion=False, stop_immediately=False, correlation_sample = 10000)\n",
    "\n",
    "modelL1.fit(data[train_mask][cols_shortlist],data[train_mask][col_target],\n",
    "        data[valid_mask][cols_shortlist],data[valid_mask][col_target]\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_ = np.array(modelL1.coefs_)\n",
    "cs = modelL1.model_progress_['C']\n",
    "plt.figure(figsize = (7,7))\n",
    "plt.plot(np.log10(cs), coefs_)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.xlabel('log10(C)')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Logistic Regression Path')\n",
    "plt.axis('tight')\n",
    "plt.legend(cols_shortlist, loc='upper center', bbox_to_anchor=(1.20,1.0))\n",
    "plt.savefig(output_folder+'/model/l1path.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,7))\n",
    "ginis = modelL1.model_progress_[['gini train','gini validate']]\n",
    "plt.plot(np.log10(cs), ginis)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.xlabel('log10(C)')\n",
    "plt.ylabel('Ginis')\n",
    "plt.title('Logistic Regression Path')\n",
    "plt.axis('tight')\n",
    "plt.legend(['Train','Validate'], loc='upper center', bbox_to_anchor=(1.20,1.0))\n",
    "plt.savefig(output_folder+'/model/l1gini.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predictors in the model:',list(modelL1.final_predictors_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drawback of regularized model is that it is not calibrated, so it must be refitted afterwards. In this workflow, there is stepwise regression after this L1 regression which can serve this purpose (i.e. fitting model with the same set or subset of predictors, but without the regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise logistic Regression\n",
    "\n",
    "We run stepwise logistic regression on training data set. We start with no predictor in the model and try to add predictors from list called **cols_shortlist** which is defined below (by default, we put there all the WOE variables).\n",
    "\n",
    "Stepwise process can be tuned using various parameters:\n",
    " - *initial_predictors*: set of starting predictors (useful for backward method)\n",
    " - *max_iter*: maximal number of iterations\n",
    " - *min_increase*: minimal marginal Gini contribution for predictor to be added\n",
    " - *max_decrease*: minimal marginal Gini diminution for predictor to be removed\n",
    " - *max_predictors*: maximal number of predictors to enter the model. Ignored if set to 0.\n",
    " - *max_correlation*: maximal absolute value of correlation of predictors in the model (variable with larger correlation with existing predictors will not be added to the model)\n",
    " - *beta_sgn_criterion*: if this is set to True, all the betas in the model must have the same signature (all positive or all negative)\n",
    " - *penalty, C*: regularization parameters for logitic regression (sklearn library)\n",
    " - *correlation_sample*: for better performance, correlation matrix is calculated just on a sample of data. The size of the sample is set in this parameter\n",
    " - *selection_method*: stepwise or forward or backward\n",
    " \n",
    "The *fit* method can be called with two arguments *fit(X,y)* or with four agruments *fit(X_train,y_train,X_valid,y_valid)*. When called with four arguments, the Gini is measured on the validation sample (i.e. validation sample is used for decisions about what steps to be done in stepwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the output from L1 model as a shortlist for the next step\n",
    "cols_shortlist2 = list(modelL1.final_predictors_)\n",
    "#cols_shortlist2 = cols_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.model_selection import GiniStepwiseLogit\n",
    "\n",
    "modelSW = GiniStepwiseLogit(initial_predictors = set(), max_iter=1000, min_increase=0.7, max_decrease=0.5, max_predictors=0,\n",
    "                    max_correlation=0.45, beta_sgn_criterion=False, penalty='l2', C=10e10, correlation_sample=10000,\n",
    "                    selection_method='stepwise')\n",
    "\n",
    "modelSW.fit(data[train_mask][cols_shortlist2],data[train_mask][col_target]\n",
    "        ,data[valid_mask][cols_shortlist2],data[valid_mask][col_target]\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = range(0,len(modelSW.model_progress_[modelSW.model_progress_['addrm']==0]['prednum']))\n",
    "pn = modelSW.model_progress_[modelSW.model_progress_['addrm']==0]['prednum']\n",
    "ginis = modelSW.model_progress_[modelSW.model_progress_['addrm']==0]['Gini']\n",
    "plt.figure(figsize = (7,7))\n",
    "plt.plot(it, ginis)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Gini')\n",
    "plt.title('Stepwise model selection')\n",
    "plt.axis('tight')\n",
    "plt.savefig(output_folder+'/model/stepwisegini.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predictors in the model:',list(modelSW.final_predictors_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score the dataset\n",
    "First choose which model is your final model (into variable *clf*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = modelL1\n",
    "clf = modelSW\n",
    "\n",
    "cols_final_predictors = list(clf.final_predictors_)\n",
    "\n",
    "print('FINAL MODEL COEFFICIENTS')\n",
    "print('Intercept:',clf.intercept_[0])\n",
    "for p,b in zip(cols_final_predictors,list(clf.coef_[0])):\n",
    "    print(p,':',b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column with the prediction (probability of default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_score = 'SCORE'\n",
    "\n",
    "data[col_score] = clf.predict(data)\n",
    "print('Column',col_score,'with the prediction added/modified. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scorecard table output\n",
    "Output the scorecard to a table. Stats are calculated on a subset of data given by the mask defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this mask is an union of masks for training, validation, testing and out of time data sets\n",
    "table_mask = train_mask|valid_mask|test_mask|oot_mask|hoot_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard = []\n",
    "\n",
    "if len(grouping.bins_data_) > 0:\n",
    "    for v,g in grouping.bins_data_.items():\n",
    "        if v+'_WOE' in clf.final_predictors_:\n",
    "            ii = list(clf.final_predictors_).index(v+'_WOE')\n",
    "            bin_names = []\n",
    "            bin_woes = []\n",
    "            for j in range(0,len(g['bins'])):\n",
    "                if (g['bins'].dtype == 'float64') and (j < len(g['bins'])-1):\n",
    "                    subset = data[(table_mask) & (data[v]>=g['bins'][j]) & (data[v]<g['bins'][j+1])]\n",
    "                    obs = subset[col_base].sum()\n",
    "                    bads = subset[col_target].sum()\n",
    "                    scorecard.append({'Variable':v,\n",
    "                                     'Min':g['bins'][j],\n",
    "                                     'Max':g['bins'][j+1],\n",
    "                                     'Value':np.nan,\n",
    "                                     'WOE':g['woes'][j],\n",
    "                                     'Beta':clf.coef_[0][ii],\n",
    "                                     'BiXi':g['woes'][j]*clf.coef_[0][ii],\n",
    "                                     'Observations':obs,\n",
    "                                     'Bads':bads})\n",
    "                elif (g['bins'].dtype != 'float64'):\n",
    "                    subset = data[(table_mask) & (data[v]==g['bins'][j])]\n",
    "                    obs = subset[col_base].sum()\n",
    "                    bads = subset[col_target].sum()\n",
    "                    scorecard.append({'Variable':v,\n",
    "                                     'Min':np.nan,\n",
    "                                     'Max':np.nan,\n",
    "                                     'Value':g['bins'][j],\n",
    "                                     'WOE':g['woes'][j],\n",
    "                                     'Beta':clf.coef_[0][ii],\n",
    "                                     'BiXi':g['woes'][j]*clf.coef_[0][ii],\n",
    "                                     'Observations':obs,\n",
    "                                     'Bads':bads})\n",
    "            subset = data[(table_mask) & (pd.isnull(data[v]))]\n",
    "            obs = subset[col_base].sum()\n",
    "            bads = subset[col_target].sum()\n",
    "            scorecard.append({'Variable':v,\n",
    "                             'Min':np.nan,\n",
    "                             'Max':np.nan,\n",
    "                             'Value':'null',\n",
    "                             'WOE':g['nan_woe'],\n",
    "                             'Beta':clf.coef_[0][ii],\n",
    "                             'BiXi':g['nan_woe']*clf.coef_[0][ii],\n",
    "                             'Observations':obs,\n",
    "                             'Bads':bads})\n",
    "\n",
    "all_obs = data[table_mask][col_base].sum()\n",
    "all_bads = data[table_mask][col_target].sum() \n",
    "scorecard.append({'Variable':'_Intercept',\n",
    "                  'Value':np.nan,\n",
    "                  'Min':np.nan,\n",
    "                  'Max':np.nan,\n",
    "                  'WOE':1,\n",
    "                  'Beta':clf.intercept_[0],\n",
    "                  'BiXi':1*clf.intercept_[0],\n",
    "                  'Observations':all_obs,\n",
    "                  'Bads':all_bads})\n",
    "\n",
    "scorecard_out = pd.DataFrame.from_records(scorecard)[\n",
    "    ['Variable','Min','Max','Value','WOE','Beta','BiXi','Observations','Bads']]\n",
    "scorecard_out2 = scorecard_out.copy()\n",
    "scorecard_out2['Value'] = scorecard_out2['Value'] + ','\n",
    "scorecard_out2 = scorecard_out2.groupby(['Variable','WOE']).agg({\n",
    "    'Variable':min,'Min':min,'Max':max,'Value':sum,'WOE':min,'Beta':min,'BiXi':min,'Observations':sum,'Bads':sum\n",
    "})\n",
    "scorecard_out2.loc[pd.isnull(scorecard_out2['Value']),'Value'] = ','\n",
    "scorecard_out2['Value'] = scorecard_out2['Value'].astype(str).str[:-1]\n",
    "scorecard_out2['Goods'] = scorecard_out2['Observations'] - scorecard_out2['Bads']\n",
    "scorecard_out2['Bad Rate'] = scorecard_out2['Bads']/scorecard_out2['Observations']\n",
    "all_badrate = all_bads/all_obs\n",
    "scorecard_out2['Bad Rate relative to population'] = scorecard_out2['Bad Rate'] / all_badrate\n",
    "scorecard_out2['% Observations'] = scorecard_out2['Observations'] / all_obs\n",
    "scorecard_out2['% Bads'] = scorecard_out2['Bads'] / all_bads\n",
    "scorecard_out2['% Goods'] = scorecard_out2['Goods'] / (all_obs-all_bads)\n",
    "scorecard_out2['Lift'] = scorecard_out2['% Bads'] / scorecard_out2['% Goods']\n",
    "scorecard_out2 = pd.DataFrame.from_records(scorecard_out2.sort_values(['Variable','Min','Max','WOE','Value']))\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "#display(scorecard_out)\n",
    "display(scorecard_out2)\n",
    "pd.options.display.max_rows = 15\n",
    "scorecard_out2.to_csv(output_folder+'/model/scorecard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring SQL\n",
    "\n",
    "Generate SQL code to run the scorecard on Oracle DWH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTER PART TRANSFORMING WOE TO BIXI\n",
    "scoring_sql_outer = ['select\\n1/(1+exp(s.LINEAR_SCORE)) as SCORE,\\ns.*\\nfrom (\\n    select\\n']\n",
    "#INNER PART TRANSFORMING VARIABLE TO WOE\n",
    "scoring_sql_inner = ['        select\\n']\n",
    "tmp_variable = ''\n",
    "nullWOE = 0\n",
    "for r in scorecard_out.itertuples():\n",
    "    if r.Variable != tmp_variable:\n",
    "        if tmp_variable != '':\n",
    "            #OUTER PART TRANSFORMING WOE TO BIXI\n",
    "            scoring_sql_outer.append('     + ')\n",
    "            #INNER PART TRANSFORMING VARIABLE TO WOE\n",
    "            scoring_sql_inner.append('            else ' + str(nullWOE) + '\\n        end as ' + str(tmp_variable) + '_WOE,\\n')\n",
    "        else:\n",
    "            #OUTER PART TRANSFORMING WOE TO BIXI\n",
    "            scoring_sql_outer.append('    ')\n",
    "        #OUTER PART TRANSFORMING WOE TO BIXI\n",
    "        scoring_sql_outer.append('w.' + str(r.Variable) + '_WOE * ' + str(r.Beta) + '\\n')\n",
    "        #INNER PART TRANSFORMING VARIABLE TO WOE\n",
    "        scoring_sql_inner.append('        case\\n')\n",
    "        tmp_variable = r.Variable\n",
    "        nullWOE = 0\n",
    "    if r.Value == 'null':\n",
    "        scoring_sql_inner.append('            when ' + str(r.Variable) + ' is null then ' + str(r.WOE) + '\\n')\n",
    "        nullWOE = r.WOE\n",
    "    elif pd.notnull(r.Value):\n",
    "        scoring_sql_inner.append('            when ' + str(r.Variable) + ' = \"' + str(r.Value) + '\" then ' + str(r.WOE) + '\\n')\n",
    "    elif pd.notnull(r.Min):\n",
    "        if np.isfinite(r.Max):\n",
    "            scoring_sql_inner.append('            when ' + str(r.Variable) + ' < ' + str(r.Max) + ' then ' + str(r.WOE) + '\\n')\n",
    "        else:\n",
    "            scoring_sql_inner.append('            when ' + str(r.Variable) + ' >= ' + str(r.Min) + ' then ' + str(r.WOE) + '\\n')\n",
    "    elif r.Variable == '_Intercept':\n",
    "        scoring_sql_inner.append('            when 1=1 then ' + str(r.WOE) + '\\n')\n",
    "#OUTER PART TRANSFORMING WOE TO BIXI\n",
    "scoring_sql_outer.append('    as LINEAR_SCORE,\\n    w.*\\n    from (\\n')\n",
    "#INNER PART TRANSFORMING VARIABLE TO WOE\n",
    "scoring_sql_inner.append('            else ' + str(nullWOE) + '\\n        end as ' + str(tmp_variable) + '_WOE\\n')\n",
    "scoring_sql_inner.append('        from _SOURCETABLENAME_\\n')\n",
    "scoring_sql_outer = ''.join(scoring_sql_outer)\n",
    "scoring_sql_inner = ''.join(scoring_sql_inner)\n",
    "scoring_sql_final = scoring_sql_outer + scoring_sql_inner + '    ) w\\n) s'\n",
    "print(scoring_sql_final,file=open(output_folder+'/model/scorecard.sql', \"w\"))\n",
    "print(scoring_sql_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance characteristics\n",
    "Performance characteristics of the model (Gini, Lift) and their visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.metrics import gini, lift\n",
    "lift_perc = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = pd.DataFrame({'sample':[\n",
    "    'train',\n",
    "    'valid',\n",
    "    'test',\n",
    "    'oot',\n",
    "    'hoot'\n",
    "    ], 'gini':[\n",
    "    gini(data[train_mask][col_target],data[train_mask][col_score]),\n",
    "    gini(data[valid_mask][col_target],data[valid_mask][col_score]),\n",
    "    gini(data[test_mask][col_target],data[test_mask][col_score]),\n",
    "    gini(data[oot_mask][col_target],data[oot_mask][col_score]),\n",
    "    gini(data[hoot_mask][col_target],data[hoot_mask][col_score])\n",
    "    ], 'lift_'+str(lift_perc):[\n",
    "    lift(data[train_mask][col_target],-data[train_mask][col_score],lift_perc),\n",
    "    lift(data[valid_mask][col_target],-data[valid_mask][col_score],lift_perc),\n",
    "    lift(data[test_mask][col_target],-data[test_mask][col_score],lift_perc),\n",
    "    lift(data[oot_mask][col_target],-data[oot_mask][col_score],lift_perc),\n",
    "    lift(data[hoot_mask][col_target],-data[hoot_mask][col_score],lift_perc)\n",
    "    ]}).set_index('sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(perf)\n",
    "perf.to_csv(output_folder+'/performance/performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate data for Gini and Lift curves\n",
    "from scoring.tools import calculate_gini_and_lift\n",
    "train_stats, train_curve = calculate_gini_and_lift(data[train_mask], col_target, col_score, pct = lift_perc)\n",
    "train_curve = list(zip(*train_curve))\n",
    "valid_stats, valid_curve = calculate_gini_and_lift(data[valid_mask], col_target, col_score, pct = lift_perc)\n",
    "valid_curve = list(zip(*valid_curve))\n",
    "test_stats, test_curve = calculate_gini_and_lift(data[test_mask], col_target, col_score, pct = lift_perc)\n",
    "test_curve = list(zip(*test_curve))\n",
    "oot_stats, oot_curve = calculate_gini_and_lift(data[oot_mask], col_target, col_score, pct = lift_perc)\n",
    "oot_curve = list(zip(*oot_curve))\n",
    "hoot_stats, hoot_curve = calculate_gini_and_lift(data[hoot_mask], col_target, col_score, pct = lift_perc)\n",
    "hoot_curve = list(zip(*hoot_curve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,7))\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.plot([0] + list(train_curve[2]),[0] + list(train_curve[3]), label = 'Train', color = 'g')\n",
    "plt.plot([0] + list(valid_curve[2]), [0] + list(valid_curve[3]), label = 'Validation', color = 'r')\n",
    "plt.plot([0] + list(test_curve[2]), [0] + list(test_curve[3]), label = 'Test', color = 'y')\n",
    "plt.plot([0] + list(oot_curve[2]), [0] + list(oot_curve[3]), label = 'OOT', color = 'b')\n",
    "plt.plot([0] + list(hoot_curve[2]), [0] + list(hoot_curve[3]), label = 'Hist.OOT', color = 'm')\n",
    "plt.plot(list(range(0, 101)), list(range(0, 101)), color='k')\n",
    "plt.xlabel('Cumulative good count')\n",
    "plt.ylabel('Cumulative bad count')\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.savefig(output_folder+'/performance/roc.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.axis([0, 100, 0, max(train_curve[1])+0.5])\n",
    "plt.plot(train_curve[0], train_curve[1], label = 'Train', color = 'g')\n",
    "plt.plot(valid_curve[0], valid_curve[1], label = 'Validation', color = 'r')\n",
    "plt.plot(test_curve[0], test_curve[1], label = 'Test', color = 'y')\n",
    "plt.plot(oot_curve[0], oot_curve[1], label = 'OOT', color = 'b')\n",
    "plt.plot(hoot_curve[0], hoot_curve[1], label = 'Hist.OOT', color = 'm')\n",
    "plt.xlabel('Cumulative count [%]')\n",
    "plt.ylabel('Lift')\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.savefig(output_folder+'/performance/lift.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def proc_gini(x,y,z):\n",
    "    fpr, tpr, _ = roc_curve(x[y], x[z], pos_label=0)\n",
    "    roc_gini = (auc(fpr, tpr)-0.5)*2\n",
    "    return roc_gini\n",
    "%matplotlib inline\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "len1 = 0\n",
    "\n",
    "grouped = data[hoot_mask].groupby(col_month, axis=0)\n",
    "res_hoot= grouped.apply(proc_gini, col_target ,col_score)\n",
    "plt.plot(range(len1,len1+len(res_hoot)),-res_hoot, linewidth=2.0,label='hist. OOT', color = 'm', marker='o')\n",
    "\n",
    "if res_hoot is not None: len1 = len1 + len(res_hoot)\n",
    "\n",
    "grouped = data[train_mask].groupby(col_month, axis=0)\n",
    "res_train= grouped.apply(proc_gini, col_target ,col_score)\n",
    "plt.plot(range(len1,len1+len(res_train)),-res_train, linewidth=2.0,label='Train', color = 'g', marker='o')\n",
    "grouped = data[valid_mask].groupby(col_month, axis=0)\n",
    "res_valid= grouped.apply(proc_gini, col_target ,col_score)\n",
    "plt.plot(range(len1,len1+len(res_valid)),-res_valid, linewidth=2.0,label='Validation', color = 'r', marker='o')\n",
    "grouped = data[test_mask].groupby(col_month, axis=0)\n",
    "res_test= grouped.apply(proc_gini, col_target ,col_score)\n",
    "plt.plot(range(len1,len1+len(res_test)),-res_test, linewidth=2.0,label='Test', color = 'y', marker='o')\n",
    "\n",
    "if res_train is not None: len1 = len1 + len(res_train)\n",
    "\n",
    "grouped = data[oot_mask].groupby(col_month, axis=0)\n",
    "res_oot= grouped.apply(proc_gini, col_target ,col_score)\n",
    "plt.plot(range(len1,len1+len(res_oot)),-res_oot, linewidth=2.0,label='OOT', color = 'b', marker='o')\n",
    "\n",
    "plt.xticks(range(len(res_train)+len(res_oot)), np.sort(data[col_month].unique()), rotation=45)\n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.title('Gini by months')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Gini')\n",
    "plt.savefig(output_folder+'/performance/ginistability.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import plot_calib\n",
    "plot_calib(data[col_score],data[col_target],bins=20,savepath=output_folder+'/model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "Calculate and visualise correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cormat = data[cols_final_predictors].corr()\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 15})\n",
    "sns.set()\n",
    "a4_dims = (12,10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=a4_dims, dpi=50)\n",
    "fig.suptitle('Correlations of Predictors',fontsize=25)\n",
    "sns.heatmap(cormat, ax=ax, annot=True, fmt=\"0.1f\", linewidths=.5, annot_kws={\"size\":15},cmap=\"OrRd\")\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.savefig(output_folder+'/analysis/correlation.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the list of the highest correlation (restricted to correlations that are, in absolute value, higher than *max_ok_correlation* parameter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ok_correlation = 0.0\n",
    "\n",
    "# find highest pairwise correlation (correlation greater than .. in absolute value)\n",
    "hicors = []\n",
    "for i in range(0,len(cormat)):\n",
    "    for j in range(0,len(cormat)):\n",
    "        if ((cormat.iloc[i][j] > max_ok_correlation or cormat.iloc[i][j] < -max_ok_correlation) and i < j):\n",
    "            hicors.append((i,j,cormat.index[i],cormat.index[j],cormat.iloc[i][j],abs(cormat.iloc[i][j])))\n",
    "hicors.sort(key= lambda tup: tup[5], reverse=True)\n",
    "\n",
    "hicors2 = pd.DataFrame(list(zip(*list(zip(*hicors))[2:5])))\n",
    "\n",
    "# print list of highest correlations\n",
    "hicors2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time stability of predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set metadata for the stability charts. Two types of charts will be drawn:\n",
    "- Stability of default rate, for which the variables with default and with base need to be set\n",
    "- Stability of population, for which the variable with observation count needs to be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_for_default = col_target\n",
    "base_for_default = col_base\n",
    "data['ones'] = 1\n",
    "obs_for_population = 'ones'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import stability_chart\n",
    "\n",
    "for j in list(clf.final_predictors_):\n",
    "    stability_chart(data[j],data[target_for_default],data[base_for_default],data[obs_for_population],data[col_month],\n",
    "                   savepath=output_folder+'/stability/'+j+'_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with another score\n",
    "Similar charts to what were already done for the new scorecard are now drawn to compare the new scorecard to another scorecard. The value of the old score should be saved in a special column of original data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_oldscore = 'OLD_SCORE'\n",
    "\n",
    "#if the score gives the complementary probability (of non-default), run this:\n",
    "data[col_oldscore]=1-data[col_oldscore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_oldscore = pd.DataFrame({'scorecard':[\n",
    "    'old',\n",
    "    'old',\n",
    "    'old',\n",
    "    'old',\n",
    "    'new',\n",
    "    'new',\n",
    "    'new',\n",
    "    'new'\n",
    "    ],'sample':[\n",
    "    'valid',\n",
    "    'test',\n",
    "    'oot',\n",
    "    'hoot',\n",
    "    'valid',\n",
    "    'test',\n",
    "    'oot',\n",
    "    'hoot'\n",
    "    ], 'gini':[\n",
    "    gini(data[valid_mask][col_target],data[valid_mask][col_oldscore]),\n",
    "    gini(data[test_mask][col_target],data[test_mask][col_oldscore]),\n",
    "    gini(data[oot_mask][col_target],data[oot_mask][col_oldscore]),\n",
    "    gini(data[hoot_mask][col_target],data[hoot_mask][col_oldscore]),\n",
    "    gini(data[valid_mask][col_target],data[valid_mask][col_score]),\n",
    "    gini(data[test_mask][col_target],data[test_mask][col_score]),\n",
    "    gini(data[oot_mask][col_target],data[oot_mask][col_score]),\n",
    "    gini(data[hoot_mask][col_target],data[hoot_mask][col_score])\n",
    "    ], 'lift_'+str(lift_perc):[\n",
    "    lift(data[valid_mask][col_target],-data[valid_mask][col_oldscore],lift_perc),\n",
    "    lift(data[test_mask][col_target],-data[test_mask][col_oldscore],lift_perc),\n",
    "    lift(data[oot_mask][col_target],-data[oot_mask][col_oldscore],lift_perc),\n",
    "    lift(data[hoot_mask][col_target],-data[hoot_mask][col_oldscore],lift_perc),\n",
    "    lift(data[valid_mask][col_target],-data[valid_mask][col_score],lift_perc),\n",
    "    lift(data[test_mask][col_target],-data[test_mask][col_score],lift_perc),\n",
    "    lift(data[oot_mask][col_target],-data[oot_mask][col_score],lift_perc),\n",
    "    lift(data[hoot_mask][col_target],-data[hoot_mask][col_score],lift_perc)\n",
    "    ]}).set_index('sample').pivot(columns='scorecard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(perf_oldscore)\n",
    "perf_oldscore.to_csv(output_folder+'/performance/performance_oldscore.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.tools import calculate_gini_and_lift\n",
    "newscore_stats, newscore_curve = calculate_gini_and_lift(data[valid_mask|test_mask|oot_mask|hoot_mask],\n",
    "                                                         col_target, col_score, pct = lift_perc)\n",
    "newscore_curve = list(zip(*newscore_curve))\n",
    "oldscore_stats, oldscore_curve = calculate_gini_and_lift(data[valid_mask|test_mask|oot_mask|hoot_mask],\n",
    "                                                         col_target, col_oldscore, pct = lift_perc)\n",
    "oldscore_curve = list(zip(*oldscore_curve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,7))\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.plot([0] + list(newscore_curve[2]),[0] + list(newscore_curve[3]), label = 'new score', color = 'g')\n",
    "plt.plot([0] + list(oldscore_curve[2]), [0] + list(oldscore_curve[3]), label = 'old score', color = 'r')\n",
    "plt.plot(list(range(0, 101)), list(range(0, 101)), color='k')\n",
    "plt.xlabel('Cumulative good count')\n",
    "plt.ylabel('Cumulative bad count')\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.savefig(output_folder+'/performance/roc_oldscore.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.axis([0, 100, 0, max(train_curve[1])+0.5])\n",
    "plt.plot(newscore_curve[0], newscore_curve[1], label = 'new score', color = 'g')\n",
    "plt.plot(oldscore_curve[0], oldscore_curve[1], label = 'old score', color = 'r')\n",
    "plt.xlabel('Cumulative count [%]')\n",
    "plt.ylabel('Lift')\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.savefig(output_folder+'/performance/lift_oldscore.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def proc_gini(x,y,z):\n",
    "    fpr, tpr, _ = roc_curve(x[y], x[z], pos_label=0)\n",
    "    roc_gini = (auc(fpr, tpr)-0.5)*2\n",
    "    return roc_gini\n",
    "%matplotlib inline\n",
    "plt.figure(figsize = (10,7))\n",
    "grouped = data[valid_mask|test_mask|oot_mask|hoot_mask].groupby(col_month, axis=0)\n",
    "res_new= grouped.apply(proc_gini, col_target ,col_score)\n",
    "plt.plot(range(len(res_new)),-res_new, linewidth=2.0,label='new score', color = 'g', marker='o')\n",
    "\n",
    "grouped = data[valid_mask|test_mask|oot_mask|hoot_mask].groupby(col_month, axis=0)\n",
    "res_old= grouped.apply(proc_gini, col_target ,col_oldscore)\n",
    "plt.plot(range(len(res_old)),-res_old, linewidth=2.0,label='old score', color = 'r', marker='o')\n",
    "\n",
    "plt.xticks(range(len(res_valid)+len(res_oot)), np.sort(data[col_month].unique()), rotation=45)\n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.title('Gini by months')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Gini')\n",
    "plt.savefig(output_folder+'/performance/ginistability_oldscore.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices for the observable population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import transmatrix\n",
    "    \n",
    "transmatrix(oldscore = data[valid_mask|test_mask|oot_mask|hoot_mask][col_oldscore],\n",
    "            newscore = data[valid_mask|test_mask|oot_mask|hoot_mask][col_score],\n",
    "            target = data[valid_mask|test_mask|oot_mask|hoot_mask][target_for_default],\n",
    "            base = data[valid_mask|test_mask|oot_mask|hoot_mask][base_for_default],\n",
    "            obs = data[valid_mask|test_mask|oot_mask|hoot_mask][base_for_default],\n",
    "            draw_default_matrix=True,\n",
    "            draw_transition_matrix=True,\n",
    "            savepath=output_folder+'/analysis/devpop_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition matrix for the whole population (put also the rejected etc. here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_mask = data.data_type != 'train'\n",
    "\n",
    "transmatrix(oldscore = data[pop_mask][col_oldscore],\n",
    "            newscore = data[pop_mask][col_score],\n",
    "            target = data[pop_mask][target_for_default],\n",
    "            base = data[pop_mask][base_for_default],\n",
    "            obs = data[pop_mask][obs_for_population],\n",
    "            draw_default_matrix=False,\n",
    "            draw_transition_matrix=True,\n",
    "            savepath=output_folder+'/analysis/allpop_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on short target\n",
    "If there is also a shorter (e.g. FPD30) target in the original dataset, we draw also charts for performance on this target in this part of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of the short target column\n",
    "col_short = \"FPD\"\n",
    "#name of the short target's base column\n",
    "col_shortbase = \"FPD_BASE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have base column in your data set, the following code adds it (value 1 for each observation). **Otherwise, don't run it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[col_shortbase] = 1\n",
    "print('Column',col_shortbase,'added/modified. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortbase_mask = ((data.data_type == 'valid')|(data.data_type == 'test')|(data.data_type == 'oot')|(data.data_type == 'hoot')) \\\n",
    "&(data[col_shortbase] == 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_shorttarget = pd.DataFrame({'gini':[\n",
    "    gini(data[shortbase_mask][col_short],data[shortbase_mask][col_score])\n",
    "    ], 'lift_'+str(lift_perc):[\n",
    "    lift(data[shortbase_mask][col_short],-data[shortbase_mask][col_score],lift_perc)\n",
    "    ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(perf_shorttarget)\n",
    "perf_shorttarget.to_csv(output_folder+'/performance/performance_shorttarget.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def proc_gini(x,y,z):\n",
    "    fpr, tpr, _ = roc_curve(x[y], x[z], pos_label=0)\n",
    "    roc_gini = (auc(fpr, tpr)-0.5)*2\n",
    "    return roc_gini\n",
    "%matplotlib inline\n",
    "plt.figure(figsize = (10,7))\n",
    "grouped = data[valid_mask|test_mask|oot_mask|hoot_mask].groupby(col_month, axis=0)\n",
    "res_new= grouped.apply(proc_gini, col_target ,col_score)\n",
    "plt.plot(range(len(res_new)),-res_new, linewidth=2.0,label='target', color = 'g', marker='o')\n",
    "\n",
    "grouped = data[shortbase_mask].groupby(col_month, axis=0)\n",
    "res_short= grouped.apply(proc_gini, col_short ,col_score)\n",
    "plt.plot(range(len(res_short)),-res_short, linewidth=2.0,label='short target', color = 'r', marker='o')\n",
    "\n",
    "plt.xticks(range(len(res_short)), np.sort(data[col_month].unique()), rotation=45)\n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.title('Gini by months')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Gini')\n",
    "plt.savefig(output_folder+'/performance/ginistability_shorttarget.png', bbox_inches='tight', dpi = 72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML documentation\n",
    "\n",
    "Create a basic HTML document with important scorecard characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_scorecard_name = 'My model'\n",
    "txt_author_name = 'Pavel SÅ¯va'\n",
    "with open(output_folder+'/documentation.html', 'w') as f:\n",
    "    f.write('<html>\\n<head>\\n<title>'+txt_scorecard_name+'</title>\\n')\n",
    "    f.write('<meta charset=\"windows-1250\">\\n')    \n",
    "    f.write('<style>\\nbody{font: normal 10pt Helvetica, Arial, sans-serif;}\\n'+ \\\n",
    "            '.textbold{font-weight:bold;}\\n' + \\\n",
    "            '.divcode{font-family:Courier New,Courier,Lucida Sans Typewriter,Lucida Typewriter,monospace;}\\n' + \\\n",
    "            '.divpic{padding-bottom: 20pt;}\\n' + \\\n",
    "            '.textlabel{font-style:italic;font-size:8pt;}\\n' + \\\n",
    "            'table{border-collapse:collapse;}\\n' + \\\n",
    "            '</style>\\n')\n",
    "    f.write('</head>\\n<body>')\n",
    "    f.write('<h1>'+txt_scorecard_name+' - documentation</h1>\\n')\n",
    "    f.write('<h2>Document information</h2>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divtext\"><span class=\"textbold\">Author:</span> '+txt_author_name+'</div>\\n')\n",
    "    f.write(' <div class=\"divtext\"><span class=\"textbold\">Date:</span> '+ \\\n",
    "            datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")+'</div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h2>Data sample</h2>\\n')\n",
    "    f.write('<h3>Target</h3>')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divtext\"><span class=\"textbold\">Target variable:</span> '+col_target+'</div>\\n')\n",
    "    f.write(' <div class=\"divtext\"><span class=\"textbold\">Base variable:</span> '+col_base+'</div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h3>Sample characteristics</h3>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"analysis/data.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Observations and defaults in time</span></div>\\n')\n",
    "    f.write(' <div class=\"divtab\">\\n'+data_summary.to_html(na_rep='')+'\\n </div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h3>Covariates</h3>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divtab\">\\n'+power_out.to_html(na_rep='')+'\\n </div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h2>Final scorecard</h2>\\n')\n",
    "    f.write('<h3>Scorecard</h3>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divtab\">\\n'+scorecard_out2.to_html(na_rep='')+'\\n </div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h3>Scoring SQL</h3>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divcode\">\\n'+scoring_sql_final.replace(' ','&nbsp;').replace('\\n','<br />')+'\\n </div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h2>Predictors</h2>\\n')\n",
    "    for pred in cols_final_predictors:\n",
    "        pred0 = ''.join(pred.split())[:-4]\n",
    "        f.write('<h3>'+pred0+'</h3>\\n')\n",
    "        f.write('<h4>Grouping</h4>')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"predictors/'+pred0+'_binning.png\" /></div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "        f.write('<h4>Stability</h4>')\n",
    "        f.write('<div class=\"divpar\">\\n')\n",
    "        f.write(' <div class=\"divpic\"><img src=\"stability/'+pred+'_stability.png\" /></div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "    f.write('<h2>Correlations</h2>\\n')\n",
    "    f.write('<h3>Correlation matrix between WOE variables</h3>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"analysis/correlation.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Correlation of WOE variables</span></div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h2>Model evaluation</h2>\\n')\n",
    "    f.write('<h3>Performance</h3>\\n')\n",
    "    f.write('<h4>General performance</h4>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divtab\">\\n'+perf.to_html(na_rep='')+'\\n </div>\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"performance/roc.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">ROC curve</span></div>\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"performance/lift.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Lift curve</span></div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h4>Performance stability</h4>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"performance/ginistability.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Stability of Gini in time</span></div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h3>Performance on shorter target</h3>\\n')\n",
    "    f.write('<h4>General performance</h4>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divtab\">\\n'+perf_shorttarget.to_html(na_rep='')+'\\n </div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h4>Performance stability</h4>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"performance/ginistability_shorttarget.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Stability of Gini in time</span></div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h3>Calibration</h3>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"model/calibration.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Model calibration chart</span></div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h2>Comparison with current model</h2>\\n')\n",
    "    f.write('<h3>Performance comparison</h3>\\n')\n",
    "    f.write('<h4>General performance</h4>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divtab\">\\n'+perf_oldscore.to_html(na_rep='')+'\\n </div>\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"performance/roc_oldscore.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">ROC curve</span></div>\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"performance/lift_oldscore.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Lift curve</span></div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h4>Performance stability</h4>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"performance/ginistability_oldscore.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Stability of Gini in time</span></div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h3>Transition matrices</h3>\\n')\n",
    "    f.write('<h4>Bad rate matrix</h4>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"analysis/devpop_matrix_default.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Default rate matrix</span></div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h4>Transition matrix - development sample</h4>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"analysis/devpop_matrix_transition.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Transition matrix</span></div>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    f.write('<h4>Transition matrix - whole population</h4>\\n')\n",
    "    f.write('<div class=\"divpar\">\\n')\n",
    "    f.write(' <div class=\"divpic\"><img src=\"analysis/allpop_matrix_transition.png\" />\\n' + \\\n",
    "            ' <br /><span class=\"textlabel\">Transition matrix</span></div>\\n')\n",
    "    f.write('</div>\\n')  \n",
    "    f.write('</body></html>')\n",
    "    print('Created documentation in file',f.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
