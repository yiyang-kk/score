{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:30pt;font-weight:bold\">General Model Workflow</font>\n",
    "\n",
    "**Copyright:**\n",
    "\n",
    "© 2017-2020, Pavel Sůva, Marek Teller, Martin Kotek, Jan Zeller, Marek Mukenšnabl, Kirill Odintsov, Jan Hynek, Elena Kuchina and Home Credit & Finance Bank Limited Liability Company, Moscow, Russia – all rights reserved\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the [License](http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "For list of contributors see [Gitlab page](https://git.homecredit.net/risk/python-scoring-workflow) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages, configure environment\n",
    "\n",
    "For this workflow to work, you need to install some non-standard python packages that are not in standard Anaconda distribution - mainly widgets for GUI of some parts of this workflow:\n",
    "\n",
    "`conda install ipywidgets`\n",
    "\n",
    "`jupyter nbextension enable --py --sys-prefix widgetsnbextension`\n",
    "\n",
    "`conda config --add channels conda-forge`\n",
    "\n",
    "`conda install qgrid` \n",
    "\n",
    "`jupyter nbextension enable --py --sys-prefix qgrid`\n",
    "\n",
    "`conda install tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os.path\n",
    "import pickle\n",
    "import gc\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "import scoring\n",
    "\n",
    "from scipy.special import logit, expit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.close_figures=True\n",
    "from IPython.display import display, Markdown\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 15\n",
    "output_folder = 'documentation'\n",
    "\n",
    "if not os.path.exists(output_folder): os.makedirs(output_folder)\n",
    "if not os.path.exists(output_folder+'/performance'): os.makedirs(output_folder+'/performance')\n",
    "if not os.path.exists(output_folder+'/predictors'): os.makedirs(output_folder+'/predictors')\n",
    "if not os.path.exists(output_folder+'/stability'): os.makedirs(output_folder+'/stability')\n",
    "if not os.path.exists(output_folder+'/stability_short'): os.makedirs(output_folder+'/stability_short')\n",
    "if not os.path.exists(output_folder+'/analysis'): os.makedirs(output_folder+'/analysis')\n",
    "if not os.path.exists(output_folder+'/model'): os.makedirs(output_folder+'/model')\n",
    "if not os.path.exists(output_folder+'/nan_share'): os.makedirs(output_folder+'/nan_share')\n",
    "scoring.check_version('0.8.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring import doctools\n",
    "\n",
    "documentation = doctools.ProjectParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data\n",
    "\n",
    "## Import data\n",
    "\n",
    "Importing data from a CSV file. It is important to set the following parameters:\n",
    "\n",
    "encoding: usually 'utf-8' or windows-xxxx on Windows machines, where xxxx is 1250 for Central Europe, 1251 for Cyrilic etc.\n",
    "sep: separator of columns in the file\n",
    "decimal: decimal dot or coma\n",
    "index_col: which columns is used as index - should be the unique credit case identifier\n",
    "\n",
    "**Defining NA values:** In different datasets, there can be different values to be considered *N/A*. By default, we set only blank fields to be considered *N/A*, however you might want to change it and add values like *'NA'*, *'NAN'*, *'null'* to be also considered *N/A*. User parameter `na_values` for this.\n",
    "\n",
    "The data need to have **index column which has unique value per each row**. If not, it will cause problems later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring import db\n",
    "data = db.read_csv(r'demo_data\\gmdata.CSV', sep = ',', decimal = '.',\n",
    "                   optimize_types=True, encoding = 'utf-8', low_memory = False,\n",
    "                   keep_default_na = False, na_values = [''])\n",
    "print('Data loaded on',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows:',data.shape[0])\n",
    "print('Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata definition\n",
    "\n",
    "Assigning target column, time column and ID column (observation identifier) which have to exist within the dataset.\n",
    "\n",
    "All the rest (i.e. base which tells where the target is observable, month and day which are derived from the time, and weight which gives weight - importance - to each row individually) are created automatically later if they don't exist at this moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THESE COLUMNS MUST BE INCLUDED IN THE DATA SET ###\n",
    "#name of the target column\n",
    "col_target = \"FPD30\"\n",
    "#name of the time column\n",
    "col_time = \"DATETIME\"\n",
    "#name of ID column\n",
    "col_id = \"ID\"\n",
    "\n",
    "### THESE COLUMNS DON'T HAVE TO BE INCLUDED IN THE DATA SET AND ARE CREATED AUTOMATICALLY LATER ###\n",
    "#name of the base column\n",
    "col_base = \"APPROVED\"\n",
    "#name of the month column\n",
    "col_month = \"MONTH\"\n",
    "#name of the day column\n",
    "col_day = \"DAY\"\n",
    "#name of the weight column - CURRENTLY COMMENTED OUT BECAUSE OF REASONS MENTIONED LATER\n",
    "col_weight = 'WEIGHT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation.targets = [(col_target, col_base)]\n",
    "documentation.time_variable = col_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation.rowid_variable = col_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records([['col_time',col_time],['col_month',col_month],['col_day',col_day],['col_target',col_target],['col_base',col_base]]) \\\n",
    ".to_csv(output_folder+'/model/metadata.csv',index=0,header=None)\n",
    "\n",
    "data[col_target] = data[col_target].astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have base column in your data set, the following code adds it (based on if target is filled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if col_base not in data:\n",
    "    data[col_base] = 0\n",
    "    data.loc[data[col_target]==0,col_base] = 1\n",
    "    data.loc[data[col_target]==1,col_base] = 1\n",
    "    print('Column',col_base,'added/modified. Number of columns:',data.shape[1])\n",
    "else:\n",
    "    print('Column',col_base,'already exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have weight column in your data set, the following code adds it, with value = 1 for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if col_weight not in data:\n",
    "    data[col_weight] = 1\n",
    "    print('Column',col_weight,'added/modified. Number of columns:',data.shape[1])\n",
    "else:\n",
    "    print('Column',col_weight,'already exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the month and day column from the time column is doing the following\n",
    "- take the time column and tell in which format the time is saved in - **you need to specify this in variable *dtime_input_format*** (see https://docs.python.org/3/library/time.html#time.strftime for reference)\n",
    "- strip the format just to year, month, day string\n",
    "- convert the string to number\n",
    "- the new column will be added to the dataset as day\n",
    "- truncate this column to just year and month and add it to dataset as month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtime_input_format = '%Y-%m-%d %H:%M:%S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[col_day] = pd.to_numeric(pd.to_datetime(data[col_time], format=dtime_input_format).dt.strftime('%Y%m%d'))\n",
    "data[col_month] = data[col_day].apply(lambda x: math.trunc(x/100))\n",
    "print('Columns',col_day,'and',col_month,'added/modified. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of predictor (subscores) to be analyzed. These are the potential elements of the final General Model score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_pred = [\n",
    "    'INTERNAL',\n",
    "    'TELCO_A',\n",
    "    'TELCO_B',\n",
    "    'BUREAU_X',\n",
    "    'BUREAU_Y',\n",
    "    'UTILITY',\n",
    "    'DEVICE',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descrip = data.describe(include='all').transpose()\n",
    "pd.options.display.max_rows = 1000\n",
    "display(descrip)\n",
    "pd.options.display.max_rows = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default rate in time**: Simple visualisation of observation count and default rate in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import plot_dataset\n",
    "plot_dataset(data,\n",
    "             month_col=col_month,\n",
    "             def_col=col_target,\n",
    "             title='Count and bad rate',\n",
    "             base_col=col_base,\n",
    "             #weightCol=col_weight,\n",
    "             savepath=output_folder+'/analysis/',\n",
    "             zeroYlim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NaN share by month** for each variable in dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.data_exploration import nan_share_development\n",
    "\n",
    "nan_table = nan_share_development(data[cols_pred + [col_month]], col_month, \n",
    "                                  make_images=True, show_images=True, output_path=output_folder+'/nan_share/')\n",
    "display(nan_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to samples\n",
    "\n",
    "Split data into five parts (training and validation). GM model is usually trained on small and time-limited sample, so test and out of time samples are usually not created.\n",
    "\n",
    "This will add a new column indicating to which part the observations belong.\n",
    "\n",
    "- The *splitting_points* (first date of train and first date of out of time sample) can be adjusted (there can be any number of such splitting points) - it should correspond to values of column specified by *time_column* parameter. If empty list is used, no time splits will be done.\n",
    "- For each time interval, you can create multiple random splits (i.e. train/valid/test), the ratio of sizes of these splits is set by parameter *sample_sizes*. In our case we have just one time interval, so only one list of sample sizes.\n",
    "- The random splits can be stratified by multiple variables, which are specified in a list - argument to *stratify_by_columns* parameter\n",
    "- Set the random seed so the results are replicable\n",
    "\n",
    "**Before you run data split, make sure that index in your dataset in unique!** If not, you need to create new unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['INDEX_ORIGINAL'] = data.index\n",
    "# data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.data_manipulation import data_sample_time_split\n",
    "\n",
    "data['data_type'] = data_sample_time_split(data, \n",
    "                           time_column = col_month,\n",
    "                           splitting_points = [],\n",
    "                           sample_sizes = [[ 0.5   , 0.5   ]],\n",
    "                           sample_names = [['train','valid']],\n",
    "                           stratify_by_columns = [col_month,col_target],\n",
    "                           random_seed = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masks: boolean vectors corresponding to rows in the datasets. True if an row is observable and its data type belongs to given sample.\n",
    "\n",
    "`observable_mask` is mask where all observable rows are included (i.e. valid observations from all the samples)\n",
    "\n",
    "`everything_mask` is mask which is True for all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = (data['data_type'] == 'train') & (data[col_base] == 1) \n",
    "valid_mask = (data['data_type'] == 'valid') & (data[col_base] == 1) \n",
    "observable_mask = data[col_base] == 1\n",
    "everything_mask = pd.notnull(data['data_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add masks to _documentation_ object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation.sample_dict = {\n",
    "    \"Train\": train_mask,\n",
    "    \"Valid\": valid_mask,\n",
    "    \"Observable\": observable_mask,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data summary (number of defaults, number in base, number of observations, default rate) by month and by sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary = data.groupby([col_month,'data_type']).aggregate({\n",
    "    col_target:'sum',col_base:['sum','count']\n",
    "})\n",
    "data_summary.columns = [col_target,col_base,'Rows']\n",
    "data_summary[col_target+' rate'] = data_summary[col_target]/data_summary[col_base]\n",
    "display(data_summary)\n",
    "\n",
    "data_summary = data_summary.reset_index(level='data_type').pivot(columns='data_type')\n",
    "display(data_summary)\n",
    "data_summary.to_csv(output_folder+'/analysis/summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reweighting\n",
    "\n",
    "This part serves to set weights based on hit rates of each score (data source).\n",
    "\n",
    "If we have only limited number of observations for each score, but in production, we expect different hit rates, we can assign weights to each observation to give it such importance that it reflects the expected hit rates.\n",
    "\n",
    "E.g. if we have only 25% of hits in the development population for certain data source, but we expect it to be 50% in production, we assign weight=3 to this hit population, so now the non-hit population and hit population have the same sum of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hit rates\n",
    "\n",
    "Calculates hit rates for each data source in our development data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_flags = data[[col_weight, col_month]].copy()\n",
    "for col in cols_pred:\n",
    "    hit_flags[col] = data[col].notnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hits by score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_rates = []\n",
    "for col in cols_pred:\n",
    "    hit_rates.append({'Variable': col,\n",
    "                     'Weighted hit rate %': hit_flags[observable_mask & hit_flags[col]][col_weight].sum()*100 /\n",
    "                                      hit_flags[observable_mask][col_weight].sum(),\n",
    "                     })\n",
    "hit_rates = pd.DataFrame(hit_rates).set_index('Variable')\n",
    "display(hit_rates)\n",
    "hit_rates.to_csv(output_folder+'/predictors/hit_rates_unweighted.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hits by score in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_rates_time = []\n",
    "for col in cols_pred:\n",
    "    for month in hit_flags[col_month].unique():\n",
    "        hit_rates_time.append({'Variable': col,\n",
    "                               'Month': month,\n",
    "                               'Weighted hit rate %': hit_flags[observable_mask & hit_flags[col] & (hit_flags[col_month]==month)][col_weight].sum()*100 /\n",
    "                                                hit_flags[observable_mask & (hit_flags[col_month]==month)][col_weight].sum(),\n",
    "                              })\n",
    "hit_rates_time = pd.DataFrame(hit_rates_time).pivot(index='Variable', columns='Month', values='Weighted hit rate %').reindex(cols_pred)\n",
    "display(hit_rates_time)\n",
    "hit_rates_time.to_csv(output_folder+'/predictors/hit_rate_time.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined hits\n",
    "\n",
    "Caluclate hit rate interactions, i.e. what are the shares of various data source hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_rates_comb = pd.DataFrame(hit_flags[observable_mask].groupby(cols_pred)[col_weight].sum()*100 /\n",
    "                              hit_flags[observable_mask][col_weight].sum()).rename(columns={col_weight:'Weighted obs %'})\n",
    "display(hit_rates_comb)\n",
    "hit_rates_comb.to_csv(output_folder+'/predictors/hit_rate_interactions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting desired situation\n",
    "\n",
    "Now we set what are the hit rates that we expect, i.e. what will be the situation in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_desired = hit_rates.copy()\n",
    "hit_desired['Desired hit rate %'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting desired hit rates in code\n",
    "\n",
    "Either we can hardcode these numbers into the dataframe with desired hit rates..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_desired.loc['INTERNAL','Desired hit rate %'] = 100\n",
    "hit_desired.loc['TELCO_A','Desired hit rate %'] = 50\n",
    "hit_desired.loc['TELCO_B','Desired hit rate %'] = 50\n",
    "hit_desired.loc['BUREAU_X','Desired hit rate %'] = 40\n",
    "hit_desired.loc['BUREAU_Y','Desired hit rate %'] = 40\n",
    "hit_desired.loc['UTILITY','Desired hit rate %'] = 65\n",
    "hit_desired.loc['DEVICE','Desired hit rate %'] = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive tool\n",
    "\n",
    "...or we can use interactive qgrid table to set it in nicer GUI. **Don't forget to run the piece of code under the table which saves the changed values into the dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qgrid\n",
    "\n",
    "hit_widget = qgrid.show_grid(hit_desired, \n",
    "                             column_options={'editable':False}, \n",
    "                             column_definitions={'Desired hit rate %':{'editable':True}},\n",
    "                            )\n",
    "hit_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the changed values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_desired = hit_widget.get_changed_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign weights to reflect desired situation\n",
    "\n",
    "The weights are caluclated, so the weighted hit rates (i.e. hit rates when each observation's importance is multiplied by the newly calculated weights) reflect the desired (expected production) hit rates from `hit_desired` dataframe.\n",
    "\n",
    "**Assumptions:** Hit rates of individual scores are mutually independent and time consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[col_weight] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name, hit_row in hit_desired.iterrows():\n",
    "    if (hit_row['Desired hit rate %'] == 100) or (hit_row['Weighted hit rate %'] == 100):\n",
    "        coef = 1\n",
    "    else:\n",
    "        coef = (hit_row['Desired hit rate %'] * data.loc[observable_mask & pd.isnull(data[var_name]), col_weight].sum()) /\\\n",
    "               ((100-hit_row['Desired hit rate %']) * data.loc[observable_mask & pd.notnull(data[var_name]), col_weight].sum())\n",
    "    data.loc[pd.notnull(data[var_name]), col_weight] = \\\n",
    "        data.loc[pd.notnull(data[var_name]), col_weight] * coef\n",
    "weight_calib_coef = data[col_weight].count() / data[col_weight].sum()\n",
    "data[col_weight] = data[col_weight] * weight_calib_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_flags = data[[col_weight, col_month]].copy()\n",
    "for col in cols_pred:\n",
    "    hit_flags[col] = data[col].notnull()\n",
    "\n",
    "hit_rates_w = []\n",
    "for col in cols_pred:\n",
    "    hit_rates_w.append({'Variable': col,\n",
    "                     'Weighted hit rate %': hit_flags[observable_mask & hit_flags[col]][col_weight].sum()*100 /\n",
    "                                      hit_flags[observable_mask][col_weight].sum(),\n",
    "                     })\n",
    "hit_rates_w = pd.DataFrame(hit_rates_w).set_index('Variable')\n",
    "display(hit_rates_w)\n",
    "hit_rates_w.to_csv(output_folder+'/predictors/hit_rates_weighted.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In the demo data that we have here, the assumptions of independence and time consistency are not fullfiled. Variable DEVICE is observable only in a month when no other variable is observable. This means that the final weights are not able to mimick the desired hit rates*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasources analysis\n",
    "\n",
    "We will analyze each score which can potentially enter the General Model. These scores were defined in list `cols_pred` above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate power of raw scores\n",
    "\n",
    "For each score, we calculate its Gini on such subset, where the target is observable and that particular score is not null. We then see what is the power of the score when we have it (i.e. on its *\"hit\" population*, where hit means that the datasource which returns the score was queried and valid score was returned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.metrics import gini\n",
    "\n",
    "uni_ginis = []\n",
    "for col in cols_pred:\n",
    "    if pd.api.types.is_numeric_dtype(data[col].dtype):\n",
    "        col_gini = np.abs(gini(data[pd.notnull(data[col]) & observable_mask][col_target],\n",
    "                               data[pd.notnull(data[col]) & observable_mask][col],\n",
    "                               data[pd.notnull(data[col]) & observable_mask][col_weight]))\n",
    "        uni_ginis.append({'Variable':col,\n",
    "                          'Gini': col_gini,\n",
    "                        })\n",
    "uni_ginis = pd.DataFrame(uni_ginis)[['Variable','Gini']]\n",
    "display(uni_ginis)\n",
    "uni_ginis.to_csv(output_folder+'/predictors/univariate_gini.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score transformation\n",
    "\n",
    "Each datasource can return score in a different format. \n",
    "- Sometimes it is probability of default or non-default (and these can have various definitions for each source).\n",
    "- Sometimes it is logit, i.e. real number which after expit transformation becomes the probability.\n",
    "- Sometimes it is logit, but linearly transformed to fit into a specific scale.\n",
    "- Sometimes the logic is not known but the score is somehow correlated with defaults.\n",
    "- Sometimes there are specific \"special values\" which are out of standard score scale and which are telling us that something non-standard occured (e.g. error codes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted logit to logit\n",
    "\n",
    "`logit(Probability of default) = -logit(Probability of non-default)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\n",
    "    'INTERNAL'\n",
    "]:\n",
    "    data[col+'_LIN'] = -data[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit from PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\n",
    "    'TELCO_A',\n",
    "]:\n",
    "    data[col+'_LIN'] = logit(data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit from inverted PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\n",
    "    'BUREAU_Y',\n",
    "    'DEVICE',\n",
    "]:\n",
    "    data[col+'_LIN'] = logit(1-data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit by linear scaling\n",
    "\n",
    "when the score is logit of PD, but linearly shifted, we run logistic regression with just this score as sole predictor to calculate proper intercept and slope to shift it back to calibrate it to our default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\n",
    "    'TELCO_B',\n",
    "]:\n",
    "    telco_b_scaler = LogisticRegression(penalty = 'l2', C = 1000, solver='liblinear')\n",
    "    telco_b_scaler.fit(data[(data[col].notnull()) & observable_mask][[col]],\n",
    "                       data[(data[col].notnull()) & observable_mask][col_target])\n",
    "    print(f'Intercept: {telco_b_scaler.intercept_[0]}, Slope: {telco_b_scaler.coef_[0][0]}')\n",
    "    data[col+'_LIN'] = telco_b_scaler.intercept_[0] + telco_b_scaler.coef_[0][0] * data[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WOE for unknown scale and categorical\n",
    "\n",
    "If the score has unknown scale or is categorical, we can calculate Weight of Evidence as with common predictors. We will use the Interactive grouping for this.\n",
    "\n",
    "A new instance of **InteractiveGrouping** class is created. There are two important parameters:\n",
    " - *colums*: list of numerical columns to be grouped\n",
    " - *cat_columns*: list of categorical columns to be grouped\n",
    " - *group_count*: (maximal) number of final groups of each variable\n",
    " - *min_samples*: minimal number of observations in each group of each numerical variable\n",
    " - *min_samples_cat*: minimal number of observations in each group of each categorical variable\n",
    "\n",
    "Then you open the interactive environment using **display** method. The important parameters are:\n",
    " - *train_t*: training dataset the grouping should be based on\n",
    " - *colums*: list of numerical columns to be grouped and displayed\n",
    " - *cat_columns*: list of categorical columns to be grouped and displayed\n",
    " - *target_column*: as the grouping is supervised and calculates WOE values, you need to specify the target column name\n",
    " - *w_column*: vector of weights of obervation (if not filled, grouping behaves as there are equal weights)\n",
    " - *filename*: use only if you want to load a grouping that you created and saved previously\n",
    " - *group_count*: (maximal) number of final groups of each variable\n",
    " - *min_samples*: minimal number of observations in each group of each numerical variable\n",
    " - *min_samples_cat*: minimal number of observations in each group of each categorical variable\n",
    "\n",
    "In the interactive environment, you can see four sections. From top to bottom:\n",
    "- **Chart section**: \n",
    " - For **numerical variables**, there is chart with equifrequncy fine classing (observations as bars, default rate as line), equidistant fine classing and the final groups.\n",
    " - For **categorical varibles** there is chart with each of the original categorical values and a chart with the final groups.\n",
    "- **Variable section**: here you can choose tab with varible which you want to edit. \n",
    " - For **numerical variables**, the tab contains of the borders of the final groups. You can edit these borders, add new with [+] button and remove them with [-] button. You can also manually set WOE for nulls. There is also a button to perform automatic grouping on the selected variable.\n",
    " - For **categorical variables**, the tab contains of two tables. In the top table, you can see some statistics for each of the categorical values. In the rightmost column, there is the number of group which is assigned to the category. You can edit this value (doubleclick on it) to change the grouping. In the bottom table you can see statistics for the groups. It is not editable. There is also a button to perform automatic grouping on the selected variable.\n",
    "- **Save section**: here you can save the grouping. Edit the file name and click the [Apply and Save] button.\n",
    "- **Settings section**: If you perform automatic grouping on some varible, the grouping algorithm uses some parameters. These parameters can be set here. You can set how many final groups do you want to have and what is their minimal size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.grouping import Grouping, InteractiveGrouping\n",
    "\n",
    "cols_togroup_num = [\n",
    "                       'BUREAU_X',\n",
    "]\n",
    "cols_togroup_cat = [\n",
    "                       'UTILITY',\n",
    "]\n",
    "\n",
    "grouping = InteractiveGrouping(columns = cols_togroup_num,\n",
    "                               cat_columns = cols_togroup_cat,\n",
    "                               group_count=5,\n",
    "                               min_samples=100, \n",
    "                               min_samples_cat=100,\n",
    "                               woe_smooth_coef=0.001)\n",
    "\n",
    "sns.reset_orig()\n",
    "%matplotlib notebook\n",
    "%config InlineBackend.close_figures=False\n",
    "\n",
    "grouping.display(#train_t = data[train_mask][cols_togroup_num+cols_togroup_cat+[col_target]],\n",
    "                 train_t = data[train_mask][cols_togroup_num+cols_togroup_cat+[col_target]+[col_weight]], #for call with weight\n",
    "                 columns = cols_togroup_num,\n",
    "                 cat_columns = cols_togroup_cat,\n",
    "                 target_column = col_target,\n",
    "                 w_column = col_weight,\n",
    "                 #filename = 'myIntGrouping',\n",
    "                 bin_count=20,\n",
    "                 woe_smooth_coef=0.001,\n",
    "                 group_count=5,\n",
    "                 min_samples=100,\n",
    "                 min_samples_cat=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to *Apply and Save* your changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the graphical environment to be used by the normal non-interactive charts\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.close_figures=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the grouping from a file (don't forget to set the right filename) and add the WOE columns to the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scoring.grouping import Grouping\n",
    "# grouping = Grouping(columns = cols_togroup_num,\n",
    "#                     cat_columns = cols_togroup_cat,\n",
    "#                     group_count=5, \n",
    "#                     min_samples=100, \n",
    "#                     min_samples_cat=100,\n",
    "#                     woe_smooth_coef=0.001) \n",
    "# g_filename = 'myIntGrouping'\n",
    "# grouping.load(g_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the grouping to the data. *Grouping.transform()* method now automatically renames columns with proper suffix. If you need to transform just subset of columns use parameter *columns_to_transform=\\[...\\]*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_woe = grouping.transform(data, transform_to='woe', progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add WOE variabes to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woe_columns_to_replace = list()\n",
    "for column in data_woe.columns:\n",
    "    if column in data:\n",
    "        woe_columns_to_replace.append(column)\n",
    "        print('Column', column ,'dropped as it already existed in the data set.')\n",
    "data = data.drop(woe_columns_to_replace, axis='columns')\n",
    "data = data.join(data_woe)\n",
    "\n",
    "del data_woe\n",
    "gc.collect()\n",
    "\n",
    "print('Added WOE variables. Number of columns:',data.shape[1])\n",
    "cols_woe = [s + '_WOE' for s in cols_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score distribution and calibration\n",
    "\n",
    "Once we transformed all the scores to form which is after expit transformation linearly dependent on default rate, we can draw charts of its distribution and also compare the predicted probability in each quantile with predicted default rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import score_calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of *transformed* scores (i.e. each score must be in its logit form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_pred_transformed = [\n",
    "    'INTERNAL_LIN',\n",
    "    'TELCO_A_LIN',\n",
    "    'TELCO_B_LIN',\n",
    "    'BUREAU_X_WOE',\n",
    "    'BUREAU_Y_LIN',\n",
    "    'UTILITY_WOE',\n",
    "    'DEVICE_LIN',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the following charts consists of:\n",
    "- red columns: histogram of score - number of observations in each quantile, tied to the left y axis\n",
    "- black dashed line: predicted probability of default (i.e. `exp(x)/1+exp(x)`) of x axis\n",
    "- blue line: acutal default rate in each quantile, tied to the right y axis, should correspond to the black dashed line\n",
    "- green dashed line: actual default rate of observations where the score is null, tied to the right y axis\n",
    "\n",
    "The numbers under each chart are:\n",
    "- average predicted default rate (i.e. average of values of black dashed line of the whole population where the score is not null)\n",
    "- average hit default rate (i.e. average of values of blue line of the whole population where the score is not null)\n",
    "- average non-hit default rate (i.e. value of the green dashed line)\n",
    "- hit gini (metric of quality of prediction for population where the score is not null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_pred_transformed:\n",
    "    if col[-3:] == 'WOE':\n",
    "        shift = np.log(data[observable_mask][col_target].sum() / (1-data[observable_mask][col_target]).sum())\n",
    "        scale=-1\n",
    "    else:\n",
    "        shift=0\n",
    "        scale=1\n",
    "    score_calibration(data=data[observable_mask],\n",
    "                      score=col,\n",
    "                      target=col_target,\n",
    "                      weight=col_weight,\n",
    "                      shift=shift,\n",
    "                      scale=scale,\n",
    "                      ispd=False,\n",
    "                      savefile=output_folder+'/predictors/'+col+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model step by step\n",
    "\n",
    "In this part, we will build the model step by setp, so after transformation all variables to logit, we have to:\n",
    "- impute the missing values of the subscores by valid numbers. These numbers should be based on relationship of score and default from the observations where the score is known\n",
    "- fit the model using these imputed variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation based on missing\n",
    "\n",
    "In case we have a certain data source available only for certain observations, we need to fill the data source's score with some placeholder values to be able to fit the regression model correctly.\n",
    "\n",
    "This placeholder value should correspond to the average default rate of this group in such way that there is the same relationship between default rate and score as the observations where the score is known.\n",
    "\n",
    "After each imputation, we draw a chart of the score distribution with a vertical reference line corresponding to the imputed value as a sanity check whether the makes sense to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic imputation assuming linear dependency of score expit and target\n",
    "\n",
    "Calculates imputation value for score that is missing for some observations. Based on default rate in hit sample, average score in hit sample and default rate in no-hit sample, it caluclates score for no-hit sample (using simple proportion formula - \"trojčlenka\"). The score is taken in its logit form, then converted to probability of default and then the calculation is done (or, if parameter `ispd=True`, it is assumed that score already is in probability of default form).\n",
    "\n",
    "$$\\text{Imputation score (PD)} = \\frac{ \\text{Avg non-hit default rate}}{\\text{Avg hit default rate}}\\times\\text{Avg hit score (PD)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.score_imputation import missing_value\n",
    "from scoring.plot import score_calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_val = missing_value(sample_hit = data[train_mask & ~np.isnan(data[\"TELCO_A_LIN\"])],\n",
    "                        sample_nohit = data[train_mask & np.isnan(data[\"TELCO_A_LIN\"])],\n",
    "                        score = \"TELCO_A_LIN\",\n",
    "                        target = col_target,\n",
    "                        weight = col_weight,\n",
    "                        ispd = False)\n",
    "print(f'Imputation value: {fill_val}')\n",
    "\n",
    "data[\"TELCO_A_LIN_MV\"] = data[\"TELCO_A_LIN\"].copy()\n",
    "data.loc[np.isnan(data[\"TELCO_A_LIN\"]), \"TELCO_A_LIN_MV\"] = fill_val\n",
    "\n",
    "score_calibration(data=data[train_mask],\n",
    "                  score=\"TELCO_A_LIN\",\n",
    "                  target=col_target,\n",
    "                  weight=col_weight,\n",
    "                  vertical_lines=fill_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_val = missing_value(sample_hit = data[train_mask & ~np.isnan(data[\"TELCO_B_LIN\"])],\n",
    "                        sample_nohit = data[train_mask & np.isnan(data[\"TELCO_B_LIN\"])],\n",
    "                        score = \"TELCO_B_LIN\",\n",
    "                        target = col_target,\n",
    "                        ispd = False)\n",
    "print(fill_val)\n",
    "\n",
    "data[\"TELCO_B_LIN_MV\"] = data[\"TELCO_B_LIN\"].copy()\n",
    "data.loc[np.isnan(data[\"TELCO_B_LIN\"]), \"TELCO_B_LIN_MV\"] = fill_val\n",
    "\n",
    "score_calibration(data=data[train_mask],\n",
    "                  score=\"TELCO_B_LIN\",\n",
    "                  target=col_target,\n",
    "                  weight=col_weight,\n",
    "                  vertical_lines=fill_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_val = missing_value(sample_hit = data[train_mask & ~np.isnan(data[\"DEVICE_LIN\"])],\n",
    "                        sample_nohit = data[train_mask & np.isnan(data[\"DEVICE_LIN\"])],\n",
    "                        score = \"DEVICE_LIN\",\n",
    "                        target = col_target,\n",
    "                        weight = col_weight,\n",
    "                        ispd = False)\n",
    "print(fill_val)\n",
    "\n",
    "data[\"DEVICE_LIN_MV\"] = data[\"DEVICE_LIN\"].copy()\n",
    "data.loc[np.isnan(data[\"DEVICE_LIN\"]), \"DEVICE_LIN_MV\"] = fill_val\n",
    "\n",
    "score_calibration(data=data[train_mask],\n",
    "                  score=\"DEVICE_LIN\",\n",
    "                  target=col_target,\n",
    "                  weight=col_weight,\n",
    "                  vertical_lines=fill_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative imputer using not assuming linear dependence of score expit and target\n",
    "\n",
    "Calculates imputation value for score that is missing for some observations. It divides hit sample into certain number of quantiles (sorted by score) and smoothens them (by joining neighbors together) to be monotonic in average default rate. Then, it finds the quantile whose average target value is closest to average target of no-hit sample. Average score of this quantile is then used as imputation value.\n",
    "```   \n",
    "Imputation score = Avg score of quantile which is argmin(|Avg default in quantile - default of non-hits|)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.score_imputation import quantile_imputer\n",
    "from scoring.plot import score_calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_val = quantile_imputer(sample_hit = data[train_mask & ~np.isnan(data[\"TELCO_A_LIN\"])],\n",
    "                        sample_nohit = data[train_mask & np.isnan(data[\"TELCO_A_LIN\"])],\n",
    "                        score = \"TELCO_A_LIN\",\n",
    "                        target = col_target,\n",
    "                        weight = col_weight,\n",
    "                        quantiles = 100)\n",
    "print(fill_val)\n",
    "\n",
    "data[\"TELCO_A_LIN_MV\"] = data[\"TELCO_A_LIN\"].copy()\n",
    "data.loc[np.isnan(data[\"TELCO_A_LIN\"]), \"TELCO_A_LIN_MV\"] = fill_val\n",
    "\n",
    "score_calibration(data=data[train_mask],\n",
    "                  score=\"TELCO_A_LIN\",\n",
    "                  target=col_target,\n",
    "                  weight=col_weight,\n",
    "                  vertical_lines=fill_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_val = quantile_imputer(sample_hit = data[train_mask & ~np.isnan(data[\"TELCO_B_LIN\"])],\n",
    "                        sample_nohit = data[train_mask & np.isnan(data[\"TELCO_B_LIN\"])],\n",
    "                        score = \"TELCO_B_LIN\",\n",
    "                        target = col_target,\n",
    "                        quantiles = 100)\n",
    "print(fill_val)\n",
    "\n",
    "data[\"TELCO_B_LIN_MV\"] = data[\"TELCO_B_LIN\"].copy()\n",
    "data.loc[np.isnan(data[\"TELCO_B_LIN\"]), \"TELCO_B_LIN_MV\"] = fill_val\n",
    "\n",
    "score_calibration(data=data[train_mask],\n",
    "                  score=\"TELCO_B_LIN\",\n",
    "                  target=col_target,\n",
    "                  weight=col_weight,\n",
    "                  vertical_lines=fill_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_val = quantile_imputer(sample_hit = data[train_mask & ~np.isnan(data[\"DEVICE_LIN\"])],\n",
    "                        sample_nohit = data[train_mask & np.isnan(data[\"DEVICE_LIN\"])],\n",
    "                        score = \"DEVICE_LIN\",\n",
    "                        target = col_target,\n",
    "                        weight = col_weight,\n",
    "                        quantiles = 100)\n",
    "print(fill_val)\n",
    "\n",
    "data[\"DEVICE_LIN_MV\"] = data[\"DEVICE_LIN\"].copy()\n",
    "data.loc[np.isnan(data[\"DEVICE_LIN\"]), \"DEVICE_LIN_MV\"] = fill_val\n",
    "\n",
    "score_calibration(data=data[train_mask],\n",
    "                  score=\"DEVICE_LIN\",\n",
    "                  target=col_target,\n",
    "                  weight=col_weight,\n",
    "                  vertical_lines=fill_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation based on special values\n",
    "\n",
    "Sometimes, we don't want to impute all the missing values together by one imputation values, because there are multiple types of missings (e.g. no-hit and no-ask).\n",
    "\n",
    "We can use various masks to define these populations to be imputed, and call our imputation functions for each one separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"BUREAU_Y_STATUS\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_val_nohit = missing_value(sample_hit = data[train_mask & (data[\"BUREAU_Y_STATUS\"]=='HIT')],\n",
    "                              sample_nohit = data[train_mask & (data[\"BUREAU_Y_STATUS\"]=='NO-HIT')],\n",
    "                              score = \"BUREAU_Y_LIN\",\n",
    "                              target = col_target,\n",
    "                              weight = col_weight,\n",
    "                              ispd = False)\n",
    "print(fill_val_nohit)\n",
    "\n",
    "fill_val_noask = missing_value(sample_hit = data[train_mask & (data[\"BUREAU_Y_STATUS\"]=='HIT')],\n",
    "                              sample_nohit = data[train_mask & (data[\"BUREAU_Y_STATUS\"]=='NO-ASK')],\n",
    "                              score = \"BUREAU_Y_LIN\",\n",
    "                              target = col_target,\n",
    "                              weight = col_weight,\n",
    "                              ispd = False)\n",
    "print(fill_val_noask)\n",
    "\n",
    "data[\"BUREAU_Y_LIN_MV\"] = data[\"BUREAU_Y_LIN\"].copy()\n",
    "data.loc[(data[\"BUREAU_Y_STATUS\"]=='NO-HIT'), \"BUREAU_Y_LIN_MV\"] = fill_val_nohit\n",
    "data.loc[(data[\"BUREAU_Y_STATUS\"]=='NO-ASK'), \"BUREAU_Y_LIN_MV\"] = fill_val_noask\n",
    "\n",
    "score_calibration(data=data[train_mask],\n",
    "                  score=\"DEVICE_LIN\",\n",
    "                  target=col_target,\n",
    "                  weight=col_weight,\n",
    "                  vertical_lines=[fill_val_nohit, fill_val_noask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_val_nohit = quantile_imputer(sample_hit = data[train_mask & (data[\"BUREAU_Y_STATUS\"]=='HIT')],\n",
    "                              sample_nohit = data[train_mask & (data[\"BUREAU_Y_STATUS\"]=='NO-HIT')],\n",
    "                              score = \"BUREAU_Y_LIN\",\n",
    "                              target = col_target,\n",
    "                              weight = col_weight,\n",
    "                              quantiles = 100)\n",
    "print(fill_val_nohit)\n",
    "\n",
    "fill_val_noask = quantile_imputer(sample_hit = data[train_mask & (data[\"BUREAU_Y_STATUS\"]=='HIT')],\n",
    "                              sample_nohit = data[train_mask & (data[\"BUREAU_Y_STATUS\"]=='NO-ASK')],\n",
    "                              score = \"BUREAU_Y_LIN\",\n",
    "                              target = col_target,\n",
    "                              weight = col_weight,\n",
    "                              quantiles = 100)\n",
    "print(fill_val_noask)\n",
    "\n",
    "data[\"BUREAU_Y_LIN_MV\"] = data[\"BUREAU_Y_LIN\"].copy()\n",
    "data.loc[(data[\"BUREAU_Y_STATUS\"]=='NO-HIT'), \"BUREAU_Y_LIN_MV\"] = fill_val_nohit\n",
    "data.loc[(data[\"BUREAU_Y_STATUS\"]=='NO-ASK'), \"BUREAU_Y_LIN_MV\"] = fill_val_noask\n",
    "\n",
    "score_calibration(data=data[train_mask],\n",
    "                  score=\"DEVICE_LIN\",\n",
    "                  target=col_target,\n",
    "                  weight=col_weight,\n",
    "                  vertical_lines=[fill_val_nohit, fill_val_noask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual imputation\n",
    "\n",
    "In some cases, we want to impute the missing values by values that we come up with expertly.\n",
    "\n",
    "In the following list of dictionaries, we define:\n",
    "- `fill_variable` - name of variable that should be imputed, i.e. *what*\n",
    "- `nohit_condition` - pandas query defining which rows the variable should be imputed in, i.e. *where*\n",
    "- `manual_value` - value the variable should be imputed by, i.e. *how*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_manual_imputation_meta = [\n",
    "    {\n",
    "        'fill_variable': 'TELCO_A_LIN',\n",
    "        'nohit_condition': 'TELCO_A_LIN.isnull()',\n",
    "        'manual_value': -2.2,\n",
    "    },\n",
    "    {\n",
    "        'fill_variable': 'TELCO_B_LIN',\n",
    "        'nohit_condition': 'TELCO_B_LIN.isnull()',\n",
    "        'manual_value': -2.2,\n",
    "    },\n",
    "    {\n",
    "        'fill_variable': 'DEVICE_LIN',\n",
    "        'nohit_condition': 'DEVICE.isnull()',\n",
    "        'manual_value': -2.6,\n",
    "    },\n",
    "    {\n",
    "        'fill_variable': 'BUREAU_Y_LIN',\n",
    "        'nohit_condition': 'BUREAU_Y_STATUS==\"NO-HIT\"',\n",
    "        'manual_value': -2.8,\n",
    "    },\n",
    "    {\n",
    "        'fill_variable': 'BUREAU_Y_LIN',\n",
    "        'nohit_condition': 'BUREAU_Y_STATUS==\"NO-ASK\"',\n",
    "        'manual_value': -2.3,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run imputation based on the previously defined list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imp in mv_manual_imputation_meta:\n",
    "    if imp['fill_variable']+'_MV' not in data.columns:\n",
    "        data[imp['fill_variable']+'_MV'] = data[imp['fill_variable']].copy()\n",
    "    data.loc[data.eval(imp['nohit_condition'],engine='python'), imp['fill_variable']+'_MV'] = imp['manual_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define list of transformed and imputed variable that will be used as predictors in logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_pred_transformed_mv = ['INTERNAL_LIN',\n",
    "                           'TELCO_A_LIN_MV',\n",
    "                           'TELCO_B_LIN_MV',\n",
    "                           'BUREAU_X_WOE',\n",
    "                           'BUREAU_Y_LIN_MV',\n",
    "                           'UTILITY_WOE',\n",
    "                           'DEVICE_LIN_MV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted model\n",
    "\n",
    "Logitstic regression model using weights from `col_weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_simple_w = LogisticRegression(penalty = 'l2', C = 1000, solver = 'liblinear')\n",
    "logreg_simple_w.fit(X=data[train_mask][cols_pred_transformed_mv],\n",
    "                    y=data[train_mask][col_target],\n",
    "                    sample_weight=data[train_mask][col_weight])\n",
    "data['score_weighted'] = logreg_simple_w.predict_proba(X=data[cols_pred_transformed_mv])[:,1]\n",
    "\n",
    "for pred, coef in zip(['Intercept'] + cols_pred_transformed_mv, np.concatenate((logreg_simple_w.intercept_, logreg_simple_w.coef_[0]))):\n",
    "    print (pred, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unweighted model\n",
    "\n",
    "Logistic regression model not using weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_simple = LogisticRegression(penalty = 'l2', C = 1000, solver = 'liblinear')\n",
    "logreg_simple.fit(X=data[train_mask][cols_pred_transformed_mv],\n",
    "                  y=data[train_mask][col_target])\n",
    "data['score_unweighted'] = logreg_simple.predict_proba(X=data[cols_pred_transformed_mv])[:,1]\n",
    "\n",
    "for pred, coef in zip(['Intercept'] + cols_pred_transformed_mv, np.concatenate((logreg_simple.intercept_, logreg_simple.coef_[0]))):\n",
    "    print (pred, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of both approaches\n",
    "\n",
    "Gini cross-check: comparing Gini of weighted model on weighted and unweighted sample and Gini of unweighted model on these samples to see whether the weights are or are not changing our model and its performance significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.metrics import gini\n",
    "approach_ginis = []\n",
    "approach_ginis.append({'gini':gini(data[valid_mask][col_target], data[valid_mask]['score_weighted']),\n",
    "                       'score':'weighted','measurement':'unweighted'})\n",
    "approach_ginis.append({'gini':gini(data[valid_mask][col_target], data[valid_mask]['score_weighted'], data[valid_mask][col_weight]),\n",
    "                       'score':'weighted','measurement':'weighted'})\n",
    "approach_ginis.append({'gini':gini(data[valid_mask][col_target], data[valid_mask]['score_unweighted']),\n",
    "                       'score':'unweighted','measurement':'unweighted'})\n",
    "approach_ginis.append({'gini':gini(data[valid_mask][col_target], data[valid_mask]['score_unweighted'], data[valid_mask][col_weight]),\n",
    "                       'score':'unweighted','measurement':'weighted'})\n",
    "display(pd.DataFrame(approach_ginis).pivot(index='score', columns='measurement', values='gini'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model using CrossValidatedGeneralModel class\n",
    "\n",
    "Class `CrossValidatedGeneralModel` is able to run most of the previous steps automatically, essentially it is able to:\n",
    "- transform variables which are in PD (probability, expit) form to logit form\n",
    "- interactively define the missing value imputation logic\n",
    "- run cross-validation or basic train/validation model fitting which will do the following:\n",
    "    - impute missing values of variables using chosen algorithm or given constants\n",
    "    - train logistic regression using the variables\n",
    "- measure Gini of the model either in a deterministic way or using bootstrappings\n",
    "- calculate marginal contribution analysis for\n",
    "    - each predictor which is already in the model\n",
    "    - additional predictors given by the user\n",
    "- return tabular outputs with all the values of coefficients, imputation values, gini values etc. from the previous steps\n",
    "- transform the data - calculate the score for each observation\n",
    "- generate python transformation code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of predictors\n",
    "\n",
    "List of variables we want to use in the model. These variables can be in form of probability of default (expit), logit or WOE. They don't have to be imputed as the imputation logic is implemented inside CrossValidatedGeneralModel class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_pred_final = ['INTERNAL',\n",
    "                   'TELCO_A',\n",
    "                   'TELCO_B',\n",
    "                   'BUREAU_X_WOE',\n",
    "                   'BUREAU_Y',\n",
    "                   'UTILITY_WOE',\n",
    "                   'DEVICE',\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability scores to be transformed\n",
    "\n",
    "List of predictors which are in the probability of default (expit) form. Should be subset of the list defined above. Can be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_pd = ['TELCO_A',\n",
    "           'BUREAU_Y',\n",
    "           'DEVICE',\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation metadata\n",
    "\n",
    "Logic for imputing empty values. In the following list of dictionaries, we define:\n",
    "- `fill_variable` - name of variable that should be imputed, i.e. *what*\n",
    "- `nohit_condition` - pandas query defining which rows the variable should be imputed in, i.e. *where*\n",
    "- `manual_value` - value the variable should be imputed by, i.e. *how* - if `None`, this value is computed automatically using the following\n",
    "- `hit_condition` - pandas query defining which rows should be used to calculate the relationship between score and default rate and so the imputation value, i.e. *how*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_imputation_meta = [\n",
    "    {\n",
    "        'fill_variable': 'TELCO_A_LIN',\n",
    "        'hit_condition': 'TELCO_A_LIN.notnull()',\n",
    "        'nohit_condition': 'TELCO_A_LIN.isnull()',\n",
    "        'manual_value': -2.2,\n",
    "    },\n",
    "    {\n",
    "        'fill_variable': 'TELCO_A',\n",
    "        'hit_condition': 'TELCO_A.notnull()',\n",
    "        'nohit_condition': 'TELCO_A.isnull()',\n",
    "        'manual_value': None,\n",
    "    },\n",
    "    {\n",
    "        'fill_variable': 'TELCO_B_LIN',\n",
    "        'hit_condition': 'TELCO_B_LIN.notnull()',\n",
    "        'nohit_condition': 'TELCO_B_LIN.isnull()',\n",
    "        'manual_value': -2.2,\n",
    "    },\n",
    "    {\n",
    "        'fill_variable': 'TELCO_B',\n",
    "        'hit_condition': 'TELCO_B.notnull()',\n",
    "        'nohit_condition': 'TELCO_B.isnull()',\n",
    "        'manual_value': None,\n",
    "    },\n",
    "    {\n",
    "        'fill_variable': 'DEVICE',\n",
    "        'hit_condition': 'DEVICE.notnull()',\n",
    "        'nohit_condition': 'DEVICE.isnull()',\n",
    "        'manual_value': None,\n",
    "    },\n",
    "    {\n",
    "        'fill_variable': 'BUREAU_Y',\n",
    "        'hit_condition': 'BUREAU_Y_STATUS==\"HIT\"',\n",
    "        'nohit_condition': 'BUREAU_Y_STATUS==\"NO-HIT\"',\n",
    "        'manual_value': None,\n",
    "    },\n",
    "    {\n",
    "        'fill_variable': 'BUREAU_Y',\n",
    "        'hit_condition': 'BUREAU_Y_STATUS==\"HIT\"',\n",
    "        'nohit_condition': 'BUREAU_Y_STATUS==\"NO-ASK\"',\n",
    "        'manual_value': None,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting basic parameters\n",
    "\n",
    "Creating new instance of `CrossValidatedGeneralModel`. The parameters are:\n",
    "- `predictors` - list of predictors we want to use in the model\n",
    "- `predictors_pd_form` - list of predictors which are in the probability of default (expit) form\n",
    "- `imputation_dicts` - imputation metadata as defined above\n",
    "- `cv` - boolean whether to use cross validation\n",
    "- `cv_folds` - number of folds for cross validation\n",
    "- `cv_seed` - random seed for cross validation\n",
    "- `imputation_type` - *quantile* to use `quantile_imputer()` or *linear* to use `missing_value()` imputer\n",
    "- `bootstrapped_gini` - boolean whether Gini should be calculated using bootstrap algorithm\n",
    "- `bootstrap_seed` - random seed for Gini bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(scoring)\n",
    "# importlib.reload(scoring.general_model)\n",
    "\n",
    "from scoring.general_model import CrossValidatedGeneralModel\n",
    "\n",
    "cvgm = CrossValidatedGeneralModel(\n",
    "    predictors = cols_pred_final,\n",
    "    predictors_pd_form = cols_pd,\n",
    "    imputation_dicts = mv_imputation_meta,\n",
    "    cv = True,\n",
    "    cv_folds = 5,\n",
    "    cv_seed = 1111,\n",
    "    imputation_type = 'quantile',\n",
    "    bootstrapped_gini = True,\n",
    "    bootstrap_seed = 2222,\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing metadata in a interactive way\n",
    "\n",
    "Interactive tools using `qgrid` to change the metadata loaded when the instance was created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilty scores to be trasformed\n",
    "\n",
    "From the `predictors` list, we can choose which of them are in probability form by ticking them in the *ispd* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvgm.set_colspd_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation metadata\n",
    "\n",
    "Change the imputation metadata. You can also add a new row (i.e. new dictionary for missing value imputation) by clicking the *New empty row* button on remove row by deleting the content of its *fill_variable* field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvgm.set_imputations_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "\n",
    "`.fit()` method is used to train the model. The arguments are:\n",
    "- `X` - training dataframe with predictors\n",
    "- `y` - series with targets corresponding to training dataframe\n",
    "- `w` - series with weights corresponding to training dataframe (not mandatory)\n",
    "- `X_valid` - validation dataframe with predictors (not mandatory)\n",
    "- `y_valid` - series with targets corresponding to validation dataframe (not mandatory)\n",
    "- `w_valid` - series with weights corresponding to validation dataframe (not mandatory)\n",
    "- `predictors` - list of predictors from X to be used. Overwrites the list defined in initialization. (not mandatory)\n",
    "\n",
    "In case we don't use cross validation and `X_valid` is defined, the model is trained using `X` dataset and Gini is measured using `X_valid` dataset. If `X_valid` is not defined, Gini is also meaured using `X` dataset.\n",
    "\n",
    "In case we use cross validation and both `X` and `X_valid` are defined, they are first concatenated and then use as base dataset for cross validation data split. If `X_valid` is not defined, `X` is used as base dataset for cross validation data split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvgm.fit(X=data[train_mask],\n",
    "        y=data[train_mask][col_target],\n",
    "        w=data[train_mask][col_weight],\n",
    "        X_valid=data[valid_mask],\n",
    "        y_valid=data[valid_mask][col_target],\n",
    "        w_valid=data[valid_mask][col_weight],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_scorecard = cvgm.scorecard_table\n",
    "display(result_scorecard)\n",
    "result_scorecard.to_csv(output_folder+'/model/scorecard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_imputations = cvgm.imputation_table\n",
    "display(result_imputations)\n",
    "result_imputations.to_csv(output_folder+'/model/imputations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini in form of `(expected Gini, [5% Gini confidence interval border, 95% Gini confidence interval border])`\n",
    "\n",
    "In case we did not use bootatrapping for Gini evaluation, the interval borders are not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvgm.gini_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal contribution\n",
    "\n",
    "`.marginal_contribution()` method is used to measure marginal contribution of individual predictors:\n",
    "- each predictor which is already in the model - we calculate the Gini difference between the base model and model without this predictor\n",
    "- additional predictors given by the user - we calculate the Gini difference between model with a predictor added and the base model\n",
    "\n",
    "Its argument is `predictors_to_add` - list with additional predictors, their marginal contribution to the base model is calculated. Can be empty list. (not mandatory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = cvgm.marginal_contribution(predictors_to_add = ['TELCO_B_LIN'])\n",
    "display(mc)\n",
    "mc.to_csv(output_folder+'/model/marginal_contribution.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring code\n",
    "\n",
    "Python code that can be used for calculation of the score. It uses `pandas` dataframe as the source of the predictors and `expit` function for `scipy` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_code = cvgm.transformation_code(dataset_name='data')\n",
    "print(result_code)\n",
    "\n",
    "result_code_file = open(output_folder+'/model/scoring_code.py', 'w') \n",
    "print(result_code, file = result_code_file) \n",
    "result_code_file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add score column to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_score = 'GM_SCORE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[col_score] = cvgm.transform(data)\n",
    "print(data[col_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column with the temporary scores from cross-validation folds\n",
    "\n",
    "If we used cross validation to fit the scorecard, the final score was trained in the whole set, so the scorecard might be overfitted. \n",
    "\n",
    "For such cases, during each iteration of cross-validation process when a temporary scorecard is fitted, the scored testing data set from that iteration is saved. Then all these sets are concatenated to create dataset scored by non-overfitted temporary scorecards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_tmp_cv_score = 'GM_SCORE_TMP_CV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[col_tmp_cv_score] = cvgm.validation_prediction\n",
    "eval_mask = data[col_tmp_cv_score].notnull()\n",
    "data[col_tmp_cv_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subpopulation analyses\n",
    "\n",
    "As General Model is directly used for rejection of clients, we might be curious how it will behave for various subpopulations of clients, e.g. hit populations of some data sources or some special client groups.\n",
    "\n",
    "In the following part, we will measure Gini of the model using observations of these groups only; and we will estimate approval rate of these groups if we base the approval/rejection process just on this score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hit population Gini\n",
    "\n",
    "For each population of interest, we calculate Gini coefficient of our score.\n",
    "\n",
    "We use the temporary validation score in case cross validation was used to fit the scorecard, so our Gini estimation is not biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populations_of_interest = [\n",
    "    'TELCO_A.notnull()',\n",
    "    'TELCO_B.notnull()',\n",
    "    'BUREAU_X.notnull()',\n",
    "    'BUREAU_Y.notnull()',\n",
    "]\n",
    "\n",
    "analysis_gini_mask = eval_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.metrics import gini, bootstrap_gini\n",
    "\n",
    "population_gini = []\n",
    "for pop in populations_of_interest:\n",
    "    pop_mask = data.eval(pop, engine=\"python\")\n",
    "    g, g_std, g_ci = bootstrap_gini(data = data[analysis_gini_mask & pop_mask],\n",
    "                                   col_target=col_target,\n",
    "                                   col_score=col_tmp_cv_score,\n",
    "                                   col_weight=col_weight,\n",
    "                                   n_iter=100,\n",
    "                                   ci_range=5,)\n",
    "    population_gini.append({'Population':pop, 'Gini':g, 'Gini_5%':g_ci[0], 'Gini_95%':g_ci[1]})\n",
    "population_gini = pd.DataFrame(population_gini)\n",
    "population_gini.set_index('Population', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(population_gini)\n",
    "population_gini.to_csv(output_folder+'/analysis/subpopulation_gini.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approval rates\n",
    "\n",
    "For each population of interest we calculate theoretical approval rate. The estimation goes as follows:\n",
    "- We define a reference approval rate for the whole population of incoming customers\n",
    "- We calculate a cutoff value which corresponds to this targetted approval rate\n",
    "- We set the same cutoff for the subpopulation\n",
    "- We evaluate what would the approval rate on just this subpopulation be when the cutoff is applied.\n",
    "\n",
    "If the subpopulation approval rate is different to the reference approval rate, it ususally means that the subpopulation is shifted, i.e. the estimated probability of default of such customers is different from probability of default of a typical customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populations_of_interest_ar = [\n",
    "    'TELCO_A.notnull()',\n",
    "    'TELCO_B.notnull()',\n",
    "    'BUREAU_X.notnull()',\n",
    "    'BUREAU_Y.notnull()',\n",
    "]\n",
    "\n",
    "analysis_ar_mask = everything_mask\n",
    "reference_ar = 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.general_model import expected_ar\n",
    "\n",
    "population_ar = []\n",
    "for pop in populations_of_interest_ar:\n",
    "    ar = expected_ar(data = data[analysis_ar_mask],\n",
    "                     col_score = col_score,\n",
    "                     query_subset = pop,\n",
    "                     col_weight = col_weight,\n",
    "                     reference_ar = reference_ar,\n",
    "                     def_by_score_ascending = False)\n",
    "    population_ar.append({'Population':pop, 'AR':ar})\n",
    "population_ar = pd.DataFrame(population_ar)\n",
    "population_ar.set_index('Population', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(population_ar)\n",
    "population_ar.to_csv(output_folder+'/analysis/approval_rates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation matrix\n",
    "\n",
    "First we transform the data (including filling the missing values) using the outputs from CrossValidatedGeneralModel class and then draw the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = data[cvgm.predictors].copy()\n",
    "for col_pd in set(cvgm.predictors) & set(cvgm.predictors_pd_form):\n",
    "    corr_data.loc[:,col_pd] = logit(corr_data[col_pd])\n",
    "for imp in cvgm.imputation_values:\n",
    "    if imp['fill_variable'] in corr_data.columns:\n",
    "        corr_data.loc[data.eval(imp['nohit_condition'],engine=\"python\"),imp['fill_variable']] = imp['fill_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation.Correlations(data=corr_data,\n",
    "                           predictors=corr_data.columns,\n",
    "                           sample=\"All\",\n",
    "                           output_folder=output_folder+\"/analysis/\",\n",
    "                           filename=\"correlation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition matrices\n",
    "\n",
    "Matrices describing the relationship between deciles of two scores. They show the default rate in each decile-decile combination and shares of observations from one decile of \"old\" score in each decile of \"new\" score.\n",
    "\n",
    "In this example we compare the internal scorecard and the GM that was developed on top of that.\n",
    "\n",
    "The transition matrix is calculated on just the observable rows first (where we have the target) and on all rows of dataset then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import transmatrix\n",
    "col_comparison_score = 'INTERNAL'\n",
    "\n",
    "transmatrix(oldscore = data[observable_mask][col_comparison_score],\n",
    "            newscore = data[observable_mask][col_score],\n",
    "            target = data[observable_mask][col_target],\n",
    "            base = data[observable_mask][col_base],\n",
    "            obs = data[observable_mask][col_base],\n",
    "            draw_default_matrix=True,\n",
    "            draw_transition_matrix=True,\n",
    "            savepath=output_folder+'/analysis/devpop_',\n",
    "            quantiles_count = 10)\n",
    "\n",
    "from scoring.plot import transmatrix\n",
    "\n",
    "transmatrix(oldscore = data[col_comparison_score],\n",
    "            newscore = data[col_score],\n",
    "            target = data[col_target],\n",
    "            base = data[col_base],\n",
    "            obs = data[col_base],\n",
    "            draw_default_matrix=False,\n",
    "            draw_transition_matrix=True,\n",
    "            savepath=output_folder+'/analysis/allpop_',\n",
    "            quantiles_count = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini and lift curves\n",
    "\n",
    "We can choose for which samples (masks) we want to draw the Lift and Gini curves and calculate the performance metrics.\n",
    "\n",
    "If we used cross validation for development of the model, we want to use the temporary score from the validation part of the cross-validation folds to evaluate the performance which we previously saved to column `col_tmp_cv_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_masks = {\n",
    "    'eval' : eval_mask,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.metrics import eval_performance_wrapper\n",
    "from scoring.tools import curves_wrapper\n",
    "\n",
    "perf = eval_performance_wrapper(data=data,\n",
    "                                masks=eval_masks,\n",
    "                                col_target=col_target,\n",
    "                                col_score=col_tmp_cv_score,\n",
    "                                col_weight=col_weight,\n",
    "                                lift_perc=10)\n",
    "display(perf)\n",
    "perf.to_csv(output_folder+'/performance/performance.csv')\n",
    "\n",
    "curves_wrapper(data=data,\n",
    "               masks=eval_masks,\n",
    "               col_target=col_target,\n",
    "               col_score=col_tmp_cv_score,\n",
    "               col_weight=col_weight,\n",
    "               output_folder=output_folder+'/performance/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score distribution\n",
    "\n",
    "### Histogram of goods/bads\n",
    "\n",
    "Distribution (histogram) of final GM score in its PD and logit form, and distribution of goods and bads in each part of the historgram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import plot_score_dist\n",
    "\n",
    "plot_score_dist(data,\n",
    "                score_name = col_score,\n",
    "                target_name = col_target,\n",
    "                weight_name = col_weight,\n",
    "                n_bins = 40,\n",
    "                labels = ['good','bad'],\n",
    "                legend_loc = 'upper right',\n",
    "                savefile = output_folder+'/model/distr_pd.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[col_score+'_LIN'] = logit(data[col_score])\n",
    "\n",
    "plot_score_dist(data,\n",
    "                score_name = col_score+'_LIN',\n",
    "                target_name = col_target,\n",
    "                weight_name = col_weight,\n",
    "                n_bins = 40,\n",
    "                labels = ['good','bad'],\n",
    "                legend_loc = 'upper right',\n",
    "                savefile = output_folder+'/model/distr_linear.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of hits of certain datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[col_score+'_LIN'] = logit(data[col_score])\n",
    "\n",
    "for pred in cvgm.predictors:\n",
    "    \n",
    "    data['_hit'] = data[pred].notnull().astype(int)\n",
    "    \n",
    "    if 0<data['_hit'].mean()<1:\n",
    "\n",
    "        plot_score_dist(data,\n",
    "                score_name = col_score+'_LIN',\n",
    "                target_name = '_hit',\n",
    "                weight_name = col_weight,\n",
    "                n_bins = 40,\n",
    "                labels = [f'{pred} no-hit',f'{pred} hit'],\n",
    "                legend_loc = 'upper right',\n",
    "                savefile = output_folder+'/model/distr_linear.png')\n",
    "        \n",
    "    data.drop(['_hit'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(scoring)\n",
    "importlib.reload(scoring.plot)\n",
    "\n",
    "from scoring.plot import score_calibration\n",
    "\n",
    "score_calibration(\n",
    "    data=data,\n",
    "    score=col_score+'_LIN',\n",
    "    target=col_target,\n",
    "    weight=col_weight,\n",
    "    savefile=output_folder+'/model/calibration.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutoff analysis\n",
    "\n",
    "Simple analyses showing what would cutoff targeting to a given reference approval rate mean for the confusion matrix and bad rate of approved customers.\n",
    "\n",
    "Before we do this analysis, we impute the target of rejected (non-observable) rows using the new GM score. From each of these rows, we create two new rows, one with target value `1`, other one with target value `0`. Weight of these rows equals to weight of the original row times the probability of default and non-default respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(scoring)\n",
    "# importlib.reload(scoring.reject_inference)\n",
    "\n",
    "from scoring.reject_inference import TargetImputer\n",
    "col_reject = 'REJECTED'\n",
    "data[col_reject] = 1 - data['APPROVED']\n",
    "\n",
    "targimp = TargetImputer(imputation_type='weighted')\n",
    "targimp.fit(data = data,\n",
    "            col_probs = col_score,\n",
    "            col_reject = col_reject,\n",
    "            col_weight = col_weight,\n",
    "            prob_of = 1)\n",
    "data_imputed = targimp.transform(data = data,\n",
    "                                col_target = col_target,\n",
    "                                col_weight = col_weight,\n",
    "                                as_new_columns = True,\n",
    "                                reset_index = True)\n",
    "\n",
    "col_target_imputed = col_target + '_IMPUTED'\n",
    "col_weight_imputed = col_weight + '_IMPUTED'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`confusion_chart` plots the percentage of false rejects (number of good rejected clients divided by number of all good clients) and percentage of false approves (number of bad approved clients divided by number of all bad clients) in dependence on approval rate.\n",
    "\n",
    "`expected_default_rate` plots the bad rate of rejected clients and bad rate of approved clients in dependence on approval rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import confusion_chart, expected_default_rate\n",
    "reference_ar_cutoff = 0.70\n",
    "\n",
    "false_reject, false_approve = confusion_chart(\n",
    "    data=data_imputed,\n",
    "    col_score=col_score,\n",
    "    col_target=col_target_imputed,\n",
    "    col_weight=col_weight_imputed,\n",
    "    reference_ar=reference_ar_cutoff,\n",
    "    savefile=output_folder+'/analysis/distr_linear.png'\n",
    ")\n",
    "\n",
    "bad_rate_rejected, bad_rate_approved = expected_default_rate(\n",
    "    data=data_imputed,\n",
    "    col_score=col_score,\n",
    "    col_target=col_target_imputed,\n",
    "    col_weight=col_weight_imputed,\n",
    "    reference_ar=reference_ar_cutoff,\n",
    "    savefile=output_folder+'/analysis/distr_linear.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Nice to have todos\n",
    "- Cost analysis of source XY\n",
    "- Autoweighter which also reflects intersection values\n",
    "- WOE part of CVGM\n",
    "- Voting ensemble (avg of subscore weighted by their power), calibration first\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
