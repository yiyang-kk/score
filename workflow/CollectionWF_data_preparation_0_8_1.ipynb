{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:28pt;font-weight:bold\">Home Credit Python Scoring for Collections </font> <br><br>\n",
    "<span style=\"font-size:28pt;font-weight:bold\">    Data Preparation Workflow v.0.8.1</font>\n",
    "\n",
    "**Copyright:**\n",
    "\n",
    "© 2017-2020, Pavel Sůva, Marek Teller, Martin Kotek, Jan Zeller, Marek Mukenšnabl, Kirill Odintsov, Jan Hynek, Elena Kuchina, Lubor Pacák, Naďa Horká and Home Credit & Finance Bank Limited Liability Company, Moscow, Russia – all rights reserved\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the [License](http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "For list of contributors see [Gitlab page](https://git.homecredit.net/risk/python-scoring-workflow) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# # import cx_Oracle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os.path\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..') # path of scoring workflow folder \n",
    "import scoring\n",
    "from scoring import db\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set general technical parameters and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.close_figures=True\n",
    "from IPython.display import display, Markdown\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 15\n",
    "output_folder = 'documentation_preparation_demo'\n",
    "\n",
    "if not os.path.exists(output_folder): os.makedirs(output_folder)\n",
    "if not os.path.exists(output_folder+'/analysis'): os.makedirs(output_folder+'/analysis')\n",
    "if not os.path.exists(output_folder+'/datasets'): os.makedirs(output_folder+'/datasets')\n",
    "    \n",
    "scoring.check_version('0.9.0', list_versions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import\n",
    "\n",
    "Importing data from a CSV file. It is important to set the following parameters:\n",
    "\n",
    "encoding: usually 'utf-8' or windows-xxxx on Windows machines, where xxxx is 1250 for Central Europe, 1251 for Cyrilic etc. sep: separator of columns in the file decimal: decimal dot or coma index_col: which columns is used as index - should be the unique credit case identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = db.read_csv(r'coll_demo_data\\demo_dataset.csv',\n",
    "                      sep = ',', decimal = '.', optimize_types=True,\n",
    "                      encoding = 'utf-8', low_memory = False)\n",
    "\n",
    "print('Data loaded on',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print()\n",
    "print('Number of rows: {:15.0f}'.format(data.shape[0]))\n",
    "print('Number of columns: {:12.0f}'.format(data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning ID column, target column, time column, month and week column. The month and week columns doesn't have to exist in the dataset, it will be created later in this workflow. Creating a metadata.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THESE COLUMNS MUST BE INCLUDED IN THE DATA SET ###\n",
    "\n",
    "# name of your target column in your dataset\n",
    "col_target_orig = \"TARGET_DPD\"\n",
    "col_target_z = 'TARGET_Z'\n",
    "# name of the time column in your dataset\n",
    "col_time = \"STARTDATE\"\n",
    "col_diff_days = \"DAYS_DIFF\"\n",
    "# name of the workflow column - usually Low, High, Medium etc.\n",
    "col_workflow = 'PROCESS_NAME'\n",
    "col_treatment = 'HIGHER_TREATMENT'\n",
    "# name of the product column - e.g. CASH/CONSUMER\n",
    "col_product = 'TYPEOFCREDIT'\n",
    "\n",
    "\n",
    "### THESE COLUMNS DON'T HAVE TO BE INCLUDED IN THE DATA SET AND ARE CREATED AUTOMATICALLY LATER with this given name ###\n",
    "#name of the base column\n",
    "col_base = \"BASE\"\n",
    "# name of the year column\n",
    "col_year = \"YEAR\"\n",
    "# name of the month column\n",
    "col_month = \"MONTH\"\n",
    "# name of the day column\n",
    "col_day = \"DAY\"\n",
    "# name of the year and week column\n",
    "col_week = \"WEEK\"\n",
    "\n",
    "\n",
    "\n",
    "col_instalment = 'AMTINSTALMENT'\n",
    "col_receivable = 'AMT_RECEIVABLE' \n",
    "\n",
    "\n",
    "# #name of the weight column \n",
    "col_weight = 'WEIGHT'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECIDE WHICH TIME UNIT YOU WANT TO USE AS DEFAULT FOR THIS EXPLORATORY ANALYSIS - A WEEK OR A MONTH? \n",
    "\n",
    "time_unit = col_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the month and day column from the time column is doing the following\n",
    "- take the time column and tell in which format the time is saved in - **you need to specify this in variable *dtime_input_format*** (see https://docs.python.org/3/library/time.html#time.strftime for reference)\n",
    "- strip the format just to year, month, day string\n",
    "- convert the string to number\n",
    "- the new column will be added to the dataset as day\n",
    "- truncate this column to just year and month and add it to dataset as month\n",
    "- add the week to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtime_input_format = '%Y-%m-%d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,col_day] = pd.to_numeric(pd.to_datetime(data[col_time], format=dtime_input_format, cache=False).dt.strftime('%Y%m%d'))\n",
    "data[col_month] = data[col_day].apply(lambda x: math.trunc(x/100))\n",
    "data[col_year] = data[col_day].apply(lambda x: math.trunc(x/10000))\n",
    "\n",
    "data[col_time]=pd.to_datetime(data[col_time], format=dtime_input_format, cache=False)\n",
    "\n",
    "data[col_week] =data[col_year]*100 + data[col_time].dt.week\n",
    "\n",
    "print('Columns',col_day,',',col_month,'and',col_week,'added/modified. Number of columns:',data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Definition \n",
    "\n",
    "Use if you have the target not decided yet. The target is defined as 'delinquent in target-DPD'. \n",
    "\n",
    "To be able to use the script, your data have to contain\n",
    "- The date of the payment\n",
    "- The date difference between entry date and payment date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing he date difference between entry date and payment date\n",
    "data[[col_time,'PAIDDATE']] = data[[col_time,'PAIDDATE']].apply(pd.to_datetime, format=dtime_input_format, cache=False)\n",
    "data[col_diff_days] = (data['PAIDDATE'] - data[col_time]).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['PAIDDATE'].isna(), 'DAYS_DIFF'] = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data[col_time] > datetime.datetime.strptime('2020-03-10', '%Y-%m-%d')) & (data['PAIDDATE'].isna())].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete rows with target date in future (for the time of the dataset download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data[(data[col_time] > datetime.datetime.strptime('2020-03-10', '%Y-%m-%d')) & \\\n",
    "               (data['PAIDDATE'].isna())].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unfinished = data[data['PAIDDATE'].isna()] \n",
    "\n",
    "print((datetime.datetime.strptime('2020-03-13', '%Y-%m-%d') - data_unfinished[col_time]).dt.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['DAYS_DIFF'] > 30, 'DAYS_DIFF'] = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = pd.DataFrame(data[[col_diff_days, col_workflow, 'AMTBALANCEACTUALCONTRACT']].groupby([col_diff_days, col_workflow]).sum())\n",
    "days.sort_index(inplace=True)\n",
    "\n",
    "days_cum = days.groupby(col_workflow).cumsum()\n",
    "days_cum.reset_index(level = col_workflow, inplace=True)\n",
    "days_cum = days_cum.pivot(columns=col_workflow, values='AMTBALANCEACTUALCONTRACT')\n",
    "maxim = days_cum.max()\n",
    "days_pct = days_cum/maxim\n",
    "# display(days_pct)\n",
    "days_pct_diff = (days_pct['HIGH'] - days_pct['LOW'])*100\n",
    "# display(days_pct_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 10))\n",
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(days_pct)\n",
    "ax2.plot(days_pct_diff, 'xkcd:cloudy blue')\n",
    "\n",
    "plt.title('Cumulative Payment of Balance', fontsize=28)\n",
    "ax1.set_xlabel('DPD')\n",
    "ax1.set_ylabel('Cumulative % Paid')\n",
    "ax1.legend(days_pct.columns, loc='lower center')\n",
    "# ax2.legend('diff H-L')\n",
    "ax2.set_ylabel('pp diff H-L', color='b')\n",
    "\n",
    "filepath = os.path.join(output_folder, 'analysis', 'payment_curve.png')\n",
    "plt.savefig(filepath, format='png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision of the target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_target_orig = 'TARGET_10D'\n",
    "\n",
    "data[col_target_orig] = 1\n",
    "data.loc[(data[col_diff_days]<= 10), col_target_orig] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if col_base not in data:\n",
    "    data[col_base] = 0\n",
    "    data.loc[data[col_target_orig]==0,col_base] = 1\n",
    "    data.loc[data[col_target_orig]==1,col_base] = 1\n",
    "    print('Column',col_base,'added/modified. Number of columns:',data.shape[1])\n",
    "else:\n",
    "    print('Column',col_base,'already exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "The most important part of preparation of data is to carefully check the values (explore_df, explore_numerical and explore_categorical is here to help you) and decide which attributes or which rows you will or will not use. Some attributes are crucial for calculating the CAASB (amount receivable and amount of instalment) and thus you cannot calculate CAASB for rows with missing values for them.\n",
    "\n",
    "You can decide for cleaning the data - e.g. deleting the rows which miss important information or deleting the columns with predictor which does have not-null value in only 5 % of rows etc.\n",
    "\n",
    "You can refer to the Segmentation Cookbook https://wiki.homecredit.net/confluence/display/RSK/Segmentation+Cookbook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installment and Receivable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, only rows with null in amount of instalment are deleted from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "rownr_0 = data.shape[0]\n",
    "print('Original number of rows: ', rownr_0)\n",
    "\n",
    "data = data[data[col_instalment].notna()]\n",
    "\n",
    "rownr_1 = data.shape[0]\n",
    "print('New number of rows: ', rownr_1)\n",
    "print('Number of rows deleted: ', rownr_0 - rownr_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " here, only rows with null in amount of receivable are deleted from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rownr_0 = data.shape[0]\n",
    "print('Original number of rows: ', rownr_0)\n",
    "\n",
    "data = data[data[col_receivable].notna()]\n",
    "\n",
    "rownr_1 = data.shape[0]\n",
    "print('New number of rows: ', rownr_1)\n",
    "print('Number of rows deleted: ', rownr_0 - rownr_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Too Many Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.data_exploration import metadata_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_table = metadata_table(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_fill_percentage = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_perc = pd.DataFrame(columns=meta_table.columns)\n",
    "for i,j in np.array(meta_table[['name','fill pct']]):\n",
    "    if j < min_fill_percentage:\n",
    "        low_perc = low_perc.append(meta_table[meta_table['name'] == i])\n",
    "display(low_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset into Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the masks for different products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = data[col_product].unique()\n",
    "\n",
    "print(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring.plot import plot_dataset \n",
    "\n",
    "# time_unit = col_month # use if you want to see different time interval \n",
    "    \n",
    "plot_dataset(\n",
    "    data,\n",
    "    month_col=time_unit,\n",
    "    def_col=col_target_orig,\n",
    "    base_col=col_base,\n",
    "    segment_col=col_product,\n",
    "    output_folder=os.path.join(output_folder, \"analysis\"),\n",
    "    filename=\"bad_rate_plot.png\",\n",
    "#     weight_col=col_weight,\n",
    "    zero_ylim=True,\n",
    ")\n",
    "\n",
    "# time_unit = col_week # use if you want to set the interval back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset into Workflows\n",
    "\n",
    "Creating the masks for different workflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get to know your workflows - which do we have in the dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflows = data[col_workflow].unique()\n",
    "\n",
    "print(workflows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Higher treatment workflow definition**\n",
    "\n",
    "For obtaining the uplift graphs, we have to specify, which treatment is the 'more intensive treatment' and mark it as higher_treatment = 1. The less intensive is higher_treatment = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[col_treatment] = 0\n",
    "data.loc[data['PROCESS_NAME'] == 'HIGH', col_treatment] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default Rate in Time**\n",
    "\n",
    "Simple visualization of the counts and bad rates for each workflow defined in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scoring.plot import plot_dataset \n",
    "\n",
    "# time_unit = col_week # use if you want to see different time interval \n",
    "\n",
    "plot_dataset(\n",
    "    data,\n",
    "    month_col=time_unit,\n",
    "    def_col=col_target_orig,\n",
    "    base_col=col_base,\n",
    "    segment_col=col_workflow,\n",
    "    output_folder=os.path.join(output_folder, \"analysis\"),\n",
    "    filename=\"bad_rate_plot_wf.png\",\n",
    "    #     weight_col=col_weight,\n",
    "    zero_ylim=True,\n",
    "    )\n",
    "\n",
    "    \n",
    "# time_unit = col_month # use if you want to set the interval back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable Transformation\n",
    "\n",
    "The Jaroszewicz's transformation is creating a new Target Variable in a very simple manner. \n",
    "\n",
    "<br> <br>\n",
    "\n",
    "<center>\n",
    "Z = 1 if treatment = High and original target = 0 <br>\n",
    "Z = 1 if treatment = Low and original target = 1 <br>\n",
    "Z = 0 ... otherwise <br>\n",
    "    \n",
    "</center>\n",
    "\n",
    "If you are interested in the background, please check this [link](http://people.cs.pitt.edu/~milos/icml_clinicaldata_2012/Papers/Oral_Jaroszewitz_ICML_Clinical_2012.pdf)\n",
    "\n",
    "\n",
    "\n",
    "**Please be aware** that since our target is defined as 1 == client did not pay, 0 == client paid, some of the definitions from this paper had to be flipped to comply to our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TARGET TRANSFORMATION\n",
    "data[col_target_z] = 0\n",
    "data.loc[(data[col_target_orig]==0) & (data[col_treatment] == 1),col_target_z] = 1\n",
    "data.loc[(data[col_target_orig]==1) & (data[col_treatment] == 0),col_target_z] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scoring.plot import plot_dataset \n",
    "\n",
    "# time_unit = col_week # use if you want to see different time interval \n",
    "plot_dataset(\n",
    "    data,\n",
    "    month_col=time_unit,\n",
    "    def_col=col_target_z,\n",
    "    base_col=col_base,\n",
    "    segment_col=col_product,\n",
    "    output_folder=os.path.join(output_folder, \"analysis\"),\n",
    "    filename=\"bad_rate_product_z.png\",\n",
    "    #     weight_col=col_weight,\n",
    "    zero_ylim=True,\n",
    "    )\n",
    "    \n",
    "# time_unit = col_month # use if you want to set the interval back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights for the Transformed Target\n",
    "\n",
    "For the one model transformation, it is needed to have the ratio of classifiables in higher and lower treatment = 1:1, thus, the probability \n",
    "\n",
    "**P(group == higher treatment) = P(group == lower treatment) = 1/2**\n",
    "\n",
    "If not, we may reweight or resample the training dataset such that the assumption becomes valid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_balanced(df, balancing_var, weight_col, group_name=None, group=None):\n",
    "    \n",
    "        if group:\n",
    "            df = df[df[group_name] == group]\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        B = df[balancing_var].value_counts().values/df[balancing_var].value_counts().values.sum()\n",
    "        max_PD = B.max()\n",
    "        B = max_PD/B\n",
    "        A = df[balancing_var].value_counts().index\n",
    "\n",
    "        C = pd.DataFrame(B,A)\n",
    "        C = C.reset_index(drop = False)\n",
    "        C.rename( columns = {'index' : balancing_var,  0 : weight_col}, inplace = True)\n",
    "\n",
    "        df =  pd.merge(df, C , how = 'left', left_on=balancing_var, right_on=balancing_var) \n",
    "                                       \n",
    "        print(C)\n",
    "        return df     \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose which groups to balance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_group = 'all'  # col_product, col_name\n",
    "\n",
    "\n",
    "if balance_group == 'all':\n",
    "    if col_weight in (data.columns):\n",
    "        data.drop(columns=col_weight, inplace=True)\n",
    "        print('column ' + col_weight + ' dropped')\n",
    "    data = make_balanced(data, col_treatment, col_weight)\n",
    "    \n",
    "else:\n",
    "    balance_group_lst = data[balance_group].unique()\n",
    "    data_product = list()\n",
    "    for i in range(len(balance_group_lst)):\n",
    "        data_product.append(make_balanced(data, col_treatment, 'WEIGHT_BY_'+ balance_group, group_name=balance_group, group=balance_group_lst[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Datasets for the Main Python Scoring Workflow (PSW)\n",
    "\n",
    "Here, you can create various types of datasets ready to use in the PSW: \n",
    "- Distinct workflows' datasets for two-model segmentation\n",
    "- Distinct products' datasets for one-model transformed segmentation for distinct products (e.g. POS, CARDs, CLX, ...)\n",
    "- Combination of workflows and products for two-model segmentation for distinct products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Workflow for One-model with Transformed Target Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = os.path.join(output_folder,\"datasets\",'dataset_z.csv')\n",
    "data.to_csv(savepath, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinct Workflows' Datasets for Two-model Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(workflows)):\n",
    "    savepath = os.path.join(output_folder,\"datasets\",'dataset_' + workflows[i] + '.csv')\n",
    "    data[data[col_workflow] == workflows[i]].to_csv(savepath, encoding='utf-8', index=False)\n",
    "\n",
    "# data.to_csv('prep2_df_all.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinct Products' Datasets for One-model Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(products)):\n",
    "    savepath = os.path.join(output_folder,\"datasets\",'dataset_z_' + products[i] + '.csv')\n",
    "    data_product[i].to_csv(savepath, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(products)):\n",
    "    for j in range (0,len(workflows)):\n",
    "        savepath = os.path.join(output_folder,\"datasets\",'dataset_' + products[i] + '_' + workflows[j] + '.csv')\n",
    "        data[(data[col_product] == products[i]) & (data[col_workflow] == workflows[j])]\\\n",
    "        .to_csv(savepath, encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "370px",
    "width": "547px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
